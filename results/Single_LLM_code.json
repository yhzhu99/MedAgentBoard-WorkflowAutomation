[
    {
        "task": "Calculate the average 'Hypersensitive c-reactive protein' value for all patients who have 'Outcome: 1.0', considering only their lab records taken within the first 3 days (inclusive) of their 'AdmissionTime'. Please exclude records where 'Hypersensitive c-reactive protein' is missing.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_querying",
        "ID": 1,
        "code": "import pandas as pd\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\nfiltered_df = df[(df['Outcome'] == 1.0) & (df['RecordTime'] - df['AdmissionTime'] <= pd.Timedelta(days=3)) & (df['Hypersensitive c-reactive protein'].notna())]\naverage_value = filtered_df['Hypersensitive c-reactive protein'].mean()\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/1/\", exist_ok=True)\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/1/result.txt\", \"w\") as f:\n    f.write(str(average_value))"
    },
    {
        "task": "Identify the unique PatientID for all patients who have at least one record where the 'Lactate dehydrogenase' value is greater than 1000.0, regardless of the 'RecordTime'.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_querying",
        "ID": 2,
        "code": "import pandas as pd\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\nresult = df[df['Lactate dehydrogenase'] > 1000.0]['PatientID'].unique()\noutput_path = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/2/unique_patient_ids.csv\"\npd.Series(result).to_csv(output_path, index=False, header=['PatientID'])"
    },
    {
        "task": "Count the number of unique patients who have at least one record where both 'Hypersensitive c-reactive protein' is greater than 100.0 and 'D-D dimer' is greater than 5.0. Please exclude records where either 'Hypersensitive c-reactive protein' or 'D-D dimer' is missing.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_querying",
        "ID": 3,
        "code": "import pandas as pd\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\nfiltered_df = df.dropna(subset=['Hypersensitive c-reactive protein', 'D-D dimer'])\nfiltered_df = filtered_df[(filtered_df['Hypersensitive c-reactive protein'] > 100.0) & (filtered_df['D-D dimer'] > 5.0)]\nunique_patients = filtered_df['PatientID'].nunique()\nresult = pd.DataFrame({'UniquePatients': [unique_patients]})\nresult.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/3/result.csv\", index=False)"
    },
    {
        "task": "For each patient, identify the record with the earliest `RecordTime` (first record) and the record with the latest `RecordTime` (last record). List the unique `PatientID`s for patients where the 'White blood cell count' in their first record is strictly less than the 'White blood cell count' in their last record. Please exclude patients who have only one record or where 'White blood cell count' is missing in either their first or last record.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_querying",
        "ID": 4,
        "code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Ensure RecordTime is datetime\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\n\n# Group by PatientID and find first and last records\nfirst_records = df.loc[df.groupby('PatientID')['RecordTime'].idxmin()]\nlast_records = df.loc[df.groupby('PatientID')['RecordTime'].idxmax()]\n\n# Filter patients with more than one record\npatient_counts = df['PatientID'].value_counts()\nvalid_patients = patient_counts[patient_counts > 1].index\nfirst_records = first_records[first_records['PatientID'].isin(valid_patients)]\nlast_records = last_records[last_records['PatientID'].isin(valid_patients)]\n\n# Merge first and last records\nmerged = pd.merge(first_records, last_records, on='PatientID', suffixes=('_first', '_last'))\n\n# Filter for valid White blood cell counts\nmerged = merged.dropna(subset=['White blood cell count_first', 'White blood cell count_last'])\n\n# Find patients where first WBC < last WBC\nresult = merged[merged['White blood cell count_first'] < merged['White blood cell count_last']]\n\n# Get unique PatientIDs\nunique_patient_ids = result['PatientID'].unique()\n\n# Save results\noutput_path = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/4/result.csv\"\npd.DataFrame(unique_patient_ids, columns=['PatientID']).to_csv(output_path, index=False)"
    },
    {
        "task": "Create a line plot showing the average temporal trend of 'Hypersensitive c-reactive protein' (Hypersensitive c-reactive protein) over time relative to the AdmissionTime for patients grouped by 'Outcome'. The x-axis should represent the number of days from AdmissionTime, and the y-axis should represent the average Hypersensitive c-reactive protein value for each group. Display separate lines for patients with 'Outcome: 0.0' and 'Outcome: 1.0'.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 5,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['DaysFromAdmission'] = (df['RecordTime'] - df['AdmissionTime']).dt.days\n\noutcome_groups = df.groupby(['Outcome', 'DaysFromAdmission'])['Hypersensitive c-reactive protein'].mean().reset_index()\n\nplt.figure(figsize=(10, 6))\nfor outcome in outcome_groups['Outcome'].unique():\n    subset = outcome_groups[outcome_groups['Outcome'] == outcome]\n    plt.plot(subset['DaysFromAdmission'], subset['Hypersensitive c-reactive protein'], label=f'Outcome: {outcome}')\n\nplt.xlabel('Days from Admission')\nplt.ylabel('Average Hypersensitive c-reactive protein')\nplt.title('Average Temporal Trend of Hypersensitive c-reactive protein by Outcome')\nplt.legend()\nplt.grid(True)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/5/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'crp_trend_by_outcome.png'))\nplt.close()"
    },
    {
        "task": "Create box plots to visualize the distribution of 'LOS' (Length of Stay) for patients, comparing the distributions for different 'Outcome' values (0.0 and 1.0). Label the x-axis as 'Outcome' and the y-axis as 'Length of Stay (days)'.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 6,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\nplt.figure(figsize=(8, 6))\ndata.boxplot(column='LOS', by='Outcome', grid=False)\nplt.xlabel('Outcome')\nplt.ylabel('Length of Stay (days)')\nplt.title('')\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/6/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'los_outcome_boxplot.png'))\nplt.close()"
    },
    {
        "task": "Create violin plots to visualize the distribution of 'White blood cell count' for records where 'RecordTime' is equal to 'AdmissionTime'. Compare the distributions for patients with 'Outcome: 0.0' and 'Outcome: 1.0'. Label the x-axis as 'Outcome' and the y-axis as 'White blood cell count'.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 7,
        "code": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\nfiltered_df = df[df['RecordTime'] == df['AdmissionTime']]\nplt.figure(figsize=(10, 6))\nsns.violinplot(x='Outcome', y='White blood cell count', data=filtered_df)\nplt.xlabel('Outcome')\nplt.ylabel('White blood cell count')\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/7/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'violin_plot.png'))\nplt.close()"
    },
    {
        "task": "Create separate histograms to visualize the distribution of 'Age' for patients with 'Outcome: 0.0' and patients with 'Outcome: 1.0'. Plot these histograms side-by-side for easy comparison. Label the x-axis as 'Age' and the y-axis as 'Frequency'.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 8,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\noutcome_0 = data[data['Outcome'] == 0.0]['Age']\noutcome_1 = data[data['Outcome'] == 1.0]['Age']\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(outcome_0, bins=20, color='blue', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Outcome: 0.0')\n\nplt.subplot(1, 2, 2)\nplt.hist(outcome_1, bins=20, color='red', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Outcome: 1.0')\n\nplt.tight_layout()\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/8/age_distribution_by_outcome.png\")\nplt.close()"
    },
    {
        "task": "Create line plots showing the temporal trend of 'Hypersensitive c-reactive protein' over time for each patient. Calculate the time relative to the 'AdmissionTime' (in days) for each 'RecordTime'. Plot 'Hypersensitive c-reactive protein' on the y-axis against the relative time (days from AdmissionTime) on the x-axis. Each patient's data points should be connected by lines, and the lines should be colored according to the patient's final 'Outcome' (0.0 or 1.0). Label the x-axis as 'Days from Admission' and the y-axis as 'Hypersensitive c-reactive protein'.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 9,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['DaysFromAdmission'] = (df['RecordTime'] - df['AdmissionTime']).dt.total_seconds() / (24 * 3600)\n\noutcome_colors = {0.0: 'blue', 1.0: 'red'}\n\nplt.figure(figsize=(12, 8))\nfor patient_id, group in df.groupby('PatientID'):\n    outcome = group['Outcome'].iloc[0]\n    color = outome_colors.get(outcome, 'gray')\n    plt.plot(group['DaysFromAdmission'], group['Hypersensitive c-reactive protein'], marker='o', color=color, linestyle='-', label=f'Outcome {outcome}')\n\nplt.xlabel('Days from Admission')\nplt.ylabel('Hypersensitive c-reactive protein')\nhandles = [plt.Line2D([0], [0], color=color, label=f'Outcome {outcome}') for outcome, color in outome_colors.items()]\nplt.legend(handles=handles)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/9/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'crp_temporal_trend.png'))\nplt.close()"
    },
    {
        "task": "Create a scatter plot visualizing the relationship between 'Age' and 'LOS' (Length of Stay) for each patient. Each point on the scatter plot should represent a unique patient. Color the points based on the patient's 'Outcome' (0.0 or 1.0). Label the x-axis 'Age' and the y-axis 'Length of Stay (days)'. Include a legend to distinguish between the two outcome groups.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 10,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(df['Age'], df['LOS'], c=df['Outcome'], cmap='viridis', alpha=0.6)\nplt.xlabel('Age')\nplt.ylabel('Length of Stay (days)')\nplt.title('Age vs. Length of Stay by Outcome')\nplt.legend(handles=scatter.legend_elements()[0], labels=['Outcome 0', 'Outcome 1'])\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/10/age_vs_los_scatter.png\")\nplt.close()"
    },
    {
        "task": "Calculate the average range of 'White blood cell count' across all patients. For each patient, the range is defined as the difference between their maximum and minimum recorded 'White blood cell count' values. Only consider patients who have at least two 'White blood cell count' records to calculate a valid range.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_statistics",
        "ID": 11,
        "code": "import pandas as pd\nimport os\n\n# Read the dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Filter patients with at least two records\npatient_counts = df['PatientID'].value_counts()\nvalid_patients = patient_counts[patient_counts >= 2].index\ndf_valid = df[df['PatientID'].isin(valid_patients)]\n\n# Calculate range for each patient\nranges = df_valid.groupby('PatientID')['White blood cell count'].apply(lambda x: x.max() - x.min())\n\n# Calculate average range\naverage_range = ranges.mean()\n\n# Save the result\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/11/\", exist_ok=True)\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/11/result.txt\", \"w\") as f:\n    f.write(str(average_range))"
    },
    {
        "task": "For each unique patient ('PatientID') who has at least two recorded measurements of 'Hypersensitive c-reactive protein' with non-null values, calculate the difference between the value recorded on their last 'RecordTime' and the value recorded on their first 'RecordTime'. After calculating this change for all eligible patients, find the average of these changes.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_statistics",
        "ID": 12,
        "code": "import pandas as pd\nimport os\n\n# Load the dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Filter out rows with null 'Hypersensitive c-reactive protein' values\ndf = df[df['Hypersensitive c-reactive protein'].notna()]\n\n# Group by 'PatientID' and filter patients with at least two measurements\ngrouped = df.groupby('PatientID').filter(lambda x: len(x) >= 2)\n\n# Sort each patient's records by 'RecordTime'\ngrouped = grouped.sort_values(['PatientID', 'RecordTime'])\n\n# Calculate the difference between last and first 'Hypersensitive c-reactive protein' for each patient\nresult = grouped.groupby('PatientID')['Hypersensitive c-reactive protein'].apply(lambda x: x.iloc[-1] - x.iloc[0]).reset_index()\n\n# Calculate the average of these differences\naverage_change = result['Hypersensitive c-reactive protein'].mean()\n\n# Ensure the output directory exists\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/12/\", exist_ok=True)\n\n# Save the result to a file\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/12/average_change.txt\", \"w\") as f:\n    f.write(str(average_change))"
    },
    {
        "task": "For each unique patient, determine the number of distinct types of lab measurements recorded for them across all their entries. Exclude the standard patient metadata fields (PatientID, RecordTime, AdmissionTime, DischargeTime, Outcome, LOS, Sex, and Age) from this count. Finally, calculate the average number of unique lab measurement types per patient across the entire dataset.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_statistics",
        "ID": 13,
        "code": "import pandas as pd\nimport os\n\n# Read the dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Define metadata columns to exclude\nmetadata_columns = ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']\n\n# Get lab measurement columns\nlab_columns = [col for col in df.columns if col not in metadata_columns]\n\n# Group by PatientID and count unique lab measurements per patient\nunique_lab_counts = df.groupby('PatientID')[lab_columns].apply(lambda x: x.notna().any(axis=0).sum())\n\n# Calculate the average number of unique lab measurement types per patient\naverage_unique_labs = unique_lab_counts.mean()\n\n# Ensure output directory exists\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/13/\", exist_ok=True)\n\n# Save the result\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/13/result.txt\", \"w\") as f:\n    f.write(str(average_unique_labs))"
    },
    {
        "task": "Process the dataset to create a single summary row for each unique patient. For each PatientID, identify all recorded lab test measurements across their multiple visits. Calculate the mean value for each unique lab test parameter recorded for that patient. The final output should be a dataset where each row corresponds to a unique PatientID and contains the patient's Sex, Age, Outcome, Length of Stay (LOS), and the mean value of every lab test parameter that was recorded at least once for that patient.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_wrangling",
        "ID": 14,
        "code": "import pandas as pd\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ncolumns_to_keep = ['PatientID', 'Sex', 'Age', 'Outcome', 'LOS']\nlab_columns = [col for col in df.columns if col not in columns_to_keep and col != 'RecordTime' and col != 'AdmissionTime' and col != 'DischargeTime']\n\ngrouped = df.groupby('PatientID').agg({\n    'Sex': 'first',\n    'Age': 'first',\n    'Outcome': 'first',\n    'LOS': 'mean',\n    **{col: 'mean' for col in lab_columns}\n})\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/14/\"\nos.makedirs(output_dir, exist_ok=True)\ngrouped.to_csv(os.path.join(output_dir, \"patient_summary.csv\"))"
    },
    {
        "task": "Process the dataset to extract the first and last recorded values for each lab test for every patient. Group the records by `PatientID` and sort them chronologically by `RecordTime`. For each unique `PatientID` and each unique lab test parameter, identify the value recorded at the earliest `RecordTime` and the value recorded at the latest `RecordTime`. The final output should be a structured dataset where each row represents a unique `PatientID`. Include the patient's `Sex`, `Age`, `Outcome`, and `LOS`. For each unique lab test parameter found in the dataset, create two new columns: one representing the value from the first record for that parameter for that patient, and another representing the value from the last record. If a lab test is recorded only once for a patient, its value should populate both the 'First' and 'Last' columns for that parameter. If a lab test is never recorded for a patient, the corresponding 'First' and 'Last' columns should reflect missing values.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_wrangling",
        "ID": 15,
        "code": "import pandas as pd\nimport os\n\n# Read the dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Sort by PatientID and RecordTime\ndf = df.sort_values(['PatientID', 'RecordTime'])\n\n# Identify lab test columns (excluding non-lab columns)\nnon_lab_columns = ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']\nlab_columns = [col for col in df.columns if col not in non_lab_columns]\n\n# Group by PatientID and get first/last values for each lab test\ngrouped = df.groupby('PatientID')\n\n# Initialize result DataFrame with patient info\nresult = grouped[['Sex', 'Age', 'Outcome', 'LOS']].first().reset_index()\n\n# Process each lab test\nfor lab in lab_columns:\n    first_values = grouped[lab].first()\n    last_values = grouped[lab].last()\n    \n    result[f'{lab}_First'] = result['PatientID'].map(first_values)\n    result[f'{lab}_Last'] = result['PatientID'].map(last_values)\n\n# Ensure output directory exists\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/15/\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save results\nresult.to_csv(os.path.join(output_dir, \"processed_lab_values.csv\"), index=False)"
    },
    {
        "task": "Analyze temporal changes in lab test results for each patient. For each unique PatientID, iterate through their records and for each specific lab test parameter recorded, identify its value at the earliest RecordTime and its value at the latest RecordTime where that specific test was recorded. Calculate the change in value for each lab test by subtracting the earliest recorded value from the latest recorded value for that test (Latest Value - Earliest Value). This calculation should only be performed for lab tests recorded at least twice for the patient. The final output should be a dataset where each row corresponds to a unique PatientID. Include the patient's Sex, Age, Outcome, and LOS. For every lab test parameter present in the dataset, create a new column representing the calculated change. If a lab test was recorded less than twice for a patient, the corresponding change column should contain a missing value.",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_wrangling",
        "ID": 16,
        "code": "import pandas as pd\nimport os\n\n# Load the dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Initialize the output DataFrame\noutput_df = pd.DataFrame()\n\n# Group by PatientID\ngrouped = df.groupby('PatientID')\n\n# Iterate over each patient\nfor patient_id, group in grouped:\n    patient_data = {\n        'PatientID': patient_id,\n        'Sex': group['Sex'].iloc[0],\n        'Age': group['Age'].iloc[0],\n        'Outcome': group['Outcome'].iloc[0],\n        'LOS': group['LOS'].iloc[0]\n    }\n    \n    # Get all lab test columns (excluding non-lab columns)\n    lab_columns = [col for col in df.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']]\n    \n    for lab in lab_columns:\n        lab_values = group[['RecordTime', lab]].dropna()\n        if len(lab_values) >= 2:\n            lab_values_sorted = lab_values.sort_values('RecordTime')\n            earliest_value = lab_values_sorted[lab].iloc[0]\n            latest_value = lab_values_sorted[lab].iloc[-1]\n            change = latest_value - earliest_value\n            patient_data[f'{lab}_change'] = change\n    \n    output_df = pd.concat([output_df, pd.DataFrame([patient_data])], ignore_index=True)\n\n# Ensure the output directory exists\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/16/\", exist_ok=True)\n\n# Save the results\noutput_df.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/16/temporal_changes.csv\", index=False)"
    },
    {
        "task": "Build a binary classification model to predict the 'Outcome' for each patient.\n\n1.  **Data Preparation**: For each unique patient ('PatientID'), select the record closest to their 'AdmissionTime' to represent their initial state upon admission. Select relevant features (initial lab values, demographics like 'Sex', 'Age') for modeling.\n2.  **Model Training**: Train a classification model using the prepared initial patient data to predict the 'Outcome'. Split your data into training and testing sets to evaluate generalization performance.\n3.  **Evaluation**: Evaluate the trained model's performance on the test set using appropriate metrics for binary classification.\n4.  **Visualization**: Visualize the importance of the features used by your trained model in predicting the outcome. For tree-based models, this could be a bar chart showing feature importance scores. For linear models, you could visualize the coefficients.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 17,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['time_diff'] = (df['RecordTime'] - df['AdmissionTime']).abs()\n\nidx = df.groupby('PatientID')['time_diff'].idxmin()\ninitial_data = df.loc[idx].copy()\n\nfeatures = ['Sex', 'Age', 'Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', 'Prothrombin time', 'procalcitonin', 'eosinophils(%)', 'Interleukin 2 receptor', 'Alkaline phosphatase', 'albumin', 'basophil(%)', 'Interleukin 10', 'Total bilirubin', 'Platelet count', 'monocytes(%)', 'antithrombin', 'Interleukin 8', 'indirect bilirubin', 'Red blood cell distribution width ', 'neutrophils(%)', 'total protein', 'Quantification of Treponema pallidum antibodies', 'Prothrombin activity', 'HBsAg', 'mean corpuscular volume', 'hematocrit', 'White blood cell count', 'Tumor necrosis factorα', 'mean corpuscular hemoglobin concentration', 'fibrinogen', 'Interleukin 1β', 'Urea', 'lymphocyte count', 'PH value', 'Red blood cell count', 'Eosinophil count', 'Corrected calcium', 'Serum potassium', 'glucose', 'neutrophils count', 'Direct bilirubin', 'Mean platelet volume', 'ferritin', 'RBC distribution width SD', 'Thrombin time', '(%)lymphocyte', 'HCV antibody quantification', 'D-D dimer', 'Total cholesterol', 'aspartate aminotransferase', 'Uric acid', 'HCO3-', 'calcium', 'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)', 'Lactate dehydrogenase', 'platelet large cell ratio ', 'Interleukin 6', 'Fibrin degradation products', 'monocytes count', 'PLT distribution width', 'globulin', 'γ-glutamyl transpeptidase', 'International standard ratio', 'basophil count(#)', 'mean corpuscular hemoglobin ', 'Activation of partial thromboplastin time', 'Hypersensitive c-reactive protein', 'HIV antibody quantification', 'serum sodium', 'thrombocytocrit', 'ESR', 'glutamic-pyruvic transaminase', 'eGFR', 'creatinine']\nX = initial_data[features]\ny = initial_data['Outcome']\n\nX = X.apply(pd.to_numeric, errors='coerce')\nX.fillna(X.mean(), inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nreport = classification_report(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/17/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(f\"{output_dir}classification_report.txt\", \"w\") as f:\n    f.write(report)\nnp.savetxt(f\"{output_dir}confusion_matrix.txt\", conf_matrix, fmt=\"%d\")\nwith open(f\"{output_dir}roc_auc.txt\", \"w\") as f:\n    f.write(str(roc_auc))\nwith open(f\"{output_dir}accuracy.txt\", \"w\") as f:\n    f.write(str(accuracy))\n\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature Importances\")\nplt.bar(range(X.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\nplt.tight_layout()\nplt.savefig(f\"{output_dir}feature_importances.png\")\nplt.close()"
    },
    {
        "task": "Predict the patient 'Outcome' based on clinical data collected during the initial phase of their hospitalization.\n\n1.  **Data Preparation**: For each unique patient, extract all records within the first 48 hours of their 'AdmissionTime'. Aggregate the clinical measurements from these records to create patient-level features. Include 'Sex' and 'Age' as features. The target variable is 'Outcome'.\n2.  **Model Training**: Train a binary classification model using the prepared patient-level features to predict the 'Outcome'. Split your data into training and testing sets to evaluate generalization performance.\n3.  **Evaluation**: Evaluate the trained model's performance on the test set using appropriate metrics for binary classification.\n4.  **Visualization**:\n    *   Generate a visualization comparing the distribution of 'Length of Stay (LOS)' for patients belonging to each 'Outcome' class. This helps illustrate the difference in hospital duration associated with the outcomes.\n    *   Based on your model's results , select the top 5 most influential aggregated clinical features. For each selected feature, create a visualization showing the distribution of that feature's values separately for patients with Outcome=0 and Outcome=1.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 18,
        "code": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/18/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\ndata = data[data['RecordTime'] <= (data['AdmissionTime'] + timedelta(hours=48))]\n\nnumeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols = [col for col in numeric_cols if col not in ['PatientID', 'Outcome', 'LOS', 'Sex', 'Age']]\n\ndef aggregate_patient_data(group):\n    agg_data = {\n        'Sex': group['Sex'].iloc[0],\n        'Age': group['Age'].iloc[0],\n        'Outcome': group['Outcome'].iloc[0],\n        'LOS': group['LOS'].iloc[0]\n    }\n    for col in numeric_cols:\n        agg_data[f'{col}_mean'] = group[col].mean()\n        agg_data[f'{col}_median'] = group[col].median()\n        agg_data[f'{col}_min'] = group[col].min()\n        agg_data[f'{col}_max'] = group[col].max()\n        agg_data[f'{col}_std'] = group[col].std()\n    return pd.Series(agg_data)\n\npatient_data = data.groupby('PatientID').apply(aggregate_patient_data).reset_index()\n\npatient_data = patient_data.dropna(thresh=len(patient_data) * 0.5, axis=1)\npatient_data = patient_data.fillna(patient_data.median())\n\nX = patient_data.drop(['PatientID', 'Outcome', 'LOS'], axis=1)\ny = patient_data['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nreport = classification_report(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nwith open(f\"{output_dir}metrics.txt\", \"w\") as f:\n    f.write(f\"Classification Report:\\n{report}\\n\")\n    f.write(f\"AUC: {auc}\\n\")\n    f.write(f\"Accuracy: {accuracy}\\n\")\n    f.write(f\"F1 Score: {f1}\\n\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Outcome', y='LOS', data=patient_data)\nplt.title('Distribution of LOS by Outcome')\nplt.savefig(f\"{output_dir}los_distribution.png\")\nplt.close()\n\nfeature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\ntop_features = feature_importances.sort_values('Importance', ascending=False).head(5)['Feature'].tolist()\n\nfor feature in top_features:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Outcome', y=feature, data=patient_data)\n    plt.title(f'Distribution of {feature} by Outcome')\n    plt.savefig(f\"{output_dir}{feature}_distribution.png\")\n    plt.close()"
    },
    {
        "task": "Build a binary classification model to predict the patient 'Outcome' using the clinical measurements recorded closest to their discharge time.\n\n1.  **Data Preparation**: For each unique patient ('PatientID'), select the record with the latest 'RecordTime' that is on or before their 'DischargeTime'. Select relevant features (demographics like 'Sex', 'Age', and the lab values from the selected latest record) for modeling.\n2.  **Model Training**: Train a classification model using the prepared patient data (features from the latest record) to predict the 'Outcome'. Split your data into training and testing sets.\n3.  **Evaluation**: Evaluate the trained model's performance on the test set using appropriate metrics for binary classification.\n4.  **Visualization**: Analyze how the *change* in key clinical markers during hospitalization relates to the final outcome.\n    *   Identify 2-3 clinically relevant or model-important numerical features.\n    *   For each identified feature, calculate the difference between its value in the *latest* record (used for prediction) and its value in the *earliest* record (at or closest to admission time) for each patient.\n    *   Create a visualization for each selected feature, showing the distribution of these *changes* separately for patients with Outcome=0 and Outcome=1.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 19,
        "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\nimport os\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/19/\", exist_ok=True)\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\n\nlatest_records = df.sort_values('RecordTime').groupby('PatientID').apply(lambda x: x[x['RecordTime'] <= x['DischargeTime']].iloc[-1]).reset_index(drop=True)\nearliest_records = df.sort_values('RecordTime').groupby('PatientID').apply(lambda x: x.iloc[0]).reset_index(drop=True)\n\nfeatures = ['Sex', 'Age', 'Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', 'Prothrombin time', 'procalcitonin', 'eosinophils(%)', 'Interleukin 2 receptor', 'Alkaline phosphatase', 'albumin', 'basophil(%)', 'Interleukin 10', 'Total bilirubin', 'Platelet count', 'monocytes(%)', 'antithrombin', 'Interleukin 8', 'indirect bilirubin', 'Red blood cell distribution width ', 'neutrophils(%)', 'total protein', 'Quantification of Treponema pallidum antibodies', 'Prothrombin activity', 'HBsAg', 'mean corpuscular volume', 'hematocrit', 'White blood cell count', 'Tumor necrosis factorα', 'mean corpuscular hemoglobin concentration', 'fibrinogen', 'Interleukin 1β', 'Urea', 'lymphocyte count', 'PH value', 'Red blood cell count', 'Eosinophil count', 'Corrected calcium', 'Serum potassium', 'glucose', 'neutrophils count', 'Direct bilirubin', 'Mean platelet volume', 'ferritin', 'RBC distribution width SD', 'Thrombin time', '(%)lymphocyte', 'HCV antibody quantification', 'D-D dimer', 'Total cholesterol', 'aspartate aminotransferase', 'Uric acid', 'HCO3-', 'calcium', 'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)', 'Lactate dehydrogenase', 'platelet large cell ratio ', 'Interleukin 6', 'Fibrin degradation products', 'monocytes count', 'PLT distribution width', 'globulin', 'γ-glutamyl transpeptidase', 'International standard ratio', 'basophil count(#)', 'mean corpuscular hemoglobin ', 'Activation of partial thromboplastin time', 'Hypersensitive c-reactive protein', 'HIV antibody quantification', 'serum sodium', 'thrombocytocrit', 'ESR', 'glutamic-pyruvic transaminase', 'eGFR', 'creatinine']\n\nX = latest_records[features]\ny = latest_records['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nreport = classification_report(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/19/classification_report.txt\", \"w\") as f:\n    f.write(report)\n    f.write(f\"\\nAUC: {auc}\\n\")\n    f.write(f\"Accuracy: {accuracy}\\n\")\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/19/confusion_matrix.png\")\nplt.close()\n\nselected_features = ['Hypersensitive c-reactive protein', 'D-D dimer', 'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)']\nfor feature in selected_features:\n    latest_values = latest_records.set_index('PatientID')[feature]\n    earliest_values = earliest_records.set_index('PatientID')[feature]\n    changes = latest_values - earliest_values\n    changes_df = pd.DataFrame({'Change': changes, 'Outcome': latest_records.set_index('PatientID')['Outcome']})\n    \n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Outcome', y='Change', data=changes_df)\n    plt.title(f'Change in {feature} by Outcome')\n    plt.savefig(f\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/19/change_{feature}.png\")\n    plt.close()"
    },
    {
        "task": "Predict the patient 'Outcome' using clinical data collected during the first week of their hospitalization and visualize the average trajectories of key features compared between outcomes.\n\n1.  **Data Preparation**: For each unique patient, filter records where `RecordTime` is within 7 days *inclusive* of their `AdmissionTime`. If a patient was discharged *before* the 7th day, include all their records up to their `DischargeTime`. Create patient-level features by aggregating the clinical measurements available within this window. For numerical features, calculate relevant statistics. Include demographic features (`Sex`, `Age`).The target variable is `Outcome`.\n\n2.  **Model Training**: Split the prepared patient dataset (with aggregated features) into training and testing sets. Train a binary classification model on the training data to predict the `Outcome`.\n\n3.  **Evaluation**: Evaluate the trained model's performance on the test set using appropriate metrics for binary classification.\n\n4.  **Visualization**: Select 2-3 continuous clinical features from the dataset that are measured frequently and are likely to change over time during a hospital stay. For each selected feature:\n    *   Calculate the day relative to admission for every record (`RecordTime` - `AdmissionTime`).\n    *   For records within the first 7 days (relative day 0 to 7), calculate the *average* value of the feature for each specific day relative to admission, separately for patients with `Outcome` = 0 and patients with `Outcome` = 1.\n    *   Create a line plot showing the average trajectory of the feature over the first 7 days (x-axis: Days relative to Admission, y-axis: Average Feature Value). Include two lines on the plot, one for each outcome group (`Outcome` 0 and `Outcome` 1).",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 20,
        "code": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport os\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/20/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\n\ndef filter_records(group):\n    max_date = group['AdmissionTime'].iloc[0] + timedelta(days=7)\n    if pd.notna(group['DischargeTime'].iloc[0]) and group['DischargeTime'].iloc[0] < max_date:\n        max_date = group['DischargeTime'].iloc[0]\n    return group[group['RecordTime'] <= max_date]\n\nfiltered_df = df.groupby('PatientID').apply(filter_records).reset_index(drop=True)\n\nnumerical_cols = filtered_df.select_dtypes(include=[np.number]).columns.tolist()\nnumerical_cols = [col for col in numerical_cols if col not in ['PatientID', 'Outcome', 'LOS', 'Age']]\ncategorical_cols = ['Sex']\n\ndef aggregate_features(group):\n    agg_dict = {}\n    for col in numerical_cols:\n        if col in group.columns:\n            agg_dict[f'{col}_mean'] = group[col].mean()\n            agg_dict[f'{col}_median'] = group[col].median()\n            agg_dict[f'{col}_min'] = group[col].min()\n            agg_dict[f'{col}_max'] = group[col].max()\n            agg_dict[f'{col}_std'] = group[col].std()\n    for col in categorical_cols:\n        if col in group.columns:\n            agg_dict[col] = group[col].iloc[0]\n    agg_dict['Age'] = group['Age'].iloc[0]\n    agg_dict['Outcome'] = group['Outcome'].iloc[0]\n    return pd.Series(agg_dict)\n\npatient_data = filtered_df.groupby('PatientID').apply(aggregate_features).reset_index()\n\npatient_data = patient_data.dropna(axis=1, how='all')\npatient_data = patient_data.fillna(patient_data.mean())\n\nX = patient_data.drop(['PatientID', 'Outcome'], axis=1)\ny = patient_data['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]\n\nreport = classification_report(y_test, y_pred)\nauc = roc_auc_score(y_test, y_proba)\naccuracy = accuracy_score(y_test, y_pred)\n\nwith open(os.path.join(output_dir, 'metrics.txt'), 'w') as f:\n    f.write(f\"Classification Report:\\n{report}\\n\")\n    f.write(f\"AUC: {auc}\\n\")\n    f.write(f\"Accuracy: {accuracy}\\n\")\n\nfeatures_to_plot = ['hemoglobin', 'White blood cell count', 'Platelet count']\nfiltered_df['DaysRelative'] = (filtered_df['RecordTime'] - filtered_df['AdmissionTime']).dt.days\nfiltered_df = filtered_df[filtered_df['DaysRelative'] <= 7]\n\nplt.figure(figsize=(15, 5))\nfor i, feature in enumerate(features_to_plot):\n    plt.subplot(1, 3, i+1)\n    for outcome in [0, 1]:\n        subset = filtered_df[filtered_df['Outcome'] == outcome]\n        avg_values = subset.groupby('DaysRelative')[feature].mean()\n        plt.plot(avg_values.index, avg_values.values, label=f'Outcome {outcome}')\n    plt.xlabel('Days Relative to Admission')\n    plt.ylabel(f'Average {feature}')\n    plt.title(feature)\n    plt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, 'trajectories.png'))\nplt.close()"
    },
    {
        "task": "Build models to predict the final patient 'Outcome' using the sequence of clinical data available up to increasing time points during their hospitalization, and visualize how the model's predictive performance evolves as more historical data is included.\n\n1.  **Data Preparation**: For each patient, organize their records chronologically by `RecordTime`. Calculate the time elapsed since `AdmissionTime` for each record. Select a consistent set of relevant numerical and categorical clinical features from the records, including 'Sex' and 'Age' associated with the patient. Ensure a patient-wise split into training and testing sets.\n2.  **Sequential Modeling and Evaluation Over Time**: Define a series of prediction time points relative to admission. For each time point `T`:\n    *   Construct a dataset where each sample represents a patient and consists of the sequence of their records from `AdmissionTime` up to `AdmissionTime + T` days (inclusive). For patients discharged before `AdmissionTime + T`, include all records up to their `DischargeTime`. The target variable is the patient's final `Outcome`.\n    *   Using the pre-defined patient split, prepare the training and testing data for this specific time point `T`.\n    *   Train a binary classification model capable of handling sequential or variable-length input  on the training data for time point `T` to predict the final `Outcome`.\n    *   Evaluate the trained model's performance on the corresponding test data using the Area Under the ROC Curve (AUC).\n3.  **Performance Trajectory Visualization**: Create a line plot visualizing the test AUC score obtained at each prediction time point `T`. The x-axis should represent the time points, and the y-axis should represent the corresponding test AUC values.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 21,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport os\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\ndata['TimeSinceAdmission'] = (data['RecordTime'] - data['AdmissionTime']).dt.total_seconds() / (24 * 3600)\n\nnumerical_features = ['Age', 'Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', 'Prothrombin time', 'procalcitonin', 'eosinophils(%)', 'Interleukin 2 receptor', 'Alkaline phosphatase', 'albumin', 'basophil(%)', 'Interleukin 10', 'Total bilirubin', 'Platelet count', 'monocytes(%)', 'antithrombin', 'Interleukin 8', 'indirect bilirubin', 'Red blood cell distribution width ', 'neutrophils(%)', 'total protein', 'Quantification of Treponema pallidum antibodies', 'Prothrombin activity', 'mean corpuscular volume', 'hematocrit', 'White blood cell count', 'Tumor necrosis factorα', 'mean corpuscular hemoglobin concentration', 'fibrinogen', 'Interleukin 1β', 'Urea', 'lymphocyte count', 'PH value', 'Red blood cell count', 'Eosinophil count', 'Corrected calcium', 'Serum potassium', 'glucose', 'neutrophils count', 'Direct bilirubin', 'Mean platelet volume', 'ferritin', 'RBC distribution width SD', 'Thrombin time', '(%)lymphocyte', 'D-D dimer', 'Total cholesterol', 'aspartate aminotransferase', 'Uric acid', 'HCO3-', 'calcium', 'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)', 'Lactate dehydrogenase', 'platelet large cell ratio ', 'Interleukin 6', 'Fibrin degradation products', 'monocytes count', 'PLT distribution width', 'globulin', 'γ-glutamyl transpeptidase', 'International standard ratio', 'basophil count(#)', 'mean corpuscular hemoglobin ', 'Activation of partial thromboplastin time', 'Hypersensitive c-reactive protein', 'serum sodium', 'thrombocytocrit', 'ESR', 'glutamic-pyruvic transaminase', 'eGFR', 'creatinine']\ncategorical_features = ['Sex', 'HBsAg', 'HCV antibody quantification', 'HIV antibody quantification']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())]), numerical_features),\n        ('cat', Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='most_frequent')),\n            ('encoder', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)])\n\npatient_ids = data['PatientID'].unique()\ntrain_ids, test_ids = train_test_split(patient_ids, test_size=0.3, random_state=42)\n\ntime_points = np.linspace(0, 30, 10)\nauc_scores = []\n\nfor T in time_points:\n    X_train, y_train = [], []\n    X_test, y_test = [], []\n    \n    for patient_id in train_ids:\n        patient_data = data[data['PatientID'] == patient_id]\n        patient_data = patient_data.sort_values('RecordTime')\n        records_up_to_T = patient_data[patient_data['TimeSinceAdmission'] <= T]\n        if len(records_up_to_T) == 0:\n            continue\n        aggregated = records_up_to_T[numerical_features + categorical_features].mean()\n        aggregated['Sex'] = records_up_to_T['Sex'].mode()[0]\n        aggregated['HBsAg'] = records_up_to_T['HBsAg'].mode()[0]\n        aggregated['HCV antibody quantification'] = records_up_to_T['HCV antibody quantification'].mode()[0]\n        aggregated['HIV antibody quantification'] = records_up_to_T['HIV antibody quantification'].mode()[0]\n        X_train.append(aggregated)\n        y_train.append(patient_data['Outcome'].iloc[0])\n    \n    for patient_id in test_ids:\n        patient_data = data[data['PatientID'] == patient_id]\n        patient_data = patient_data.sort_values('RecordTime')\n        records_up_to_T = patient_data[patient_data['TimeSinceAdmission'] <= T]\n        if len(records_up_to_T) == 0:\n            continue\n        aggregated = records_up_to_T[numerical_features + categorical_features].mean()\n        aggregated['Sex'] = records_up_to_T['Sex'].mode()[0]\n        aggregated['HBsAg'] = records_up_to_T['HBsAg'].mode()[0]\n        aggregated['HCV antibody quantification'] = records_up_to_T['HCV antibody quantification'].mode()[0]\n        aggregated['HIV antibody quantification'] = records_up_to_T['HIV antibody quantification'].mode()[0]\n        X_test.append(aggregated)\n        y_test.append(patient_data['Outcome'].iloc[0])\n    \n    X_train = pd.DataFrame(X_train)\n    X_test = pd.DataFrame(X_test)\n    \n    model = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('classifier', RandomForestClassifier(random_state=42))])\n    model.fit(X_train, y_train)\n    \n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    auc = roc_auc_score(y_test, y_pred_proba)\n    auc_scores.append(auc)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/21/\"\nos.makedirs(output_dir, exist_ok=True)\n\nplt.figure(figsize=(10, 6))\nplt.plot(time_points, auc_scores, marker='o')\nplt.xlabel('Time Since Admission (days)')\nplt.ylabel('Test AUC')\nplt.title('Model Performance Over Time')\nplt.grid(True)\nplt.savefig(os.path.join(output_dir, 'performance_trajectory.png'))\nplt.close()\n\nresults = pd.DataFrame({'TimePoint': time_points, 'AUC': auc_scores})\nresults.to_csv(os.path.join(output_dir, 'auc_scores.csv'), index=False)"
    },
    {
        "task": "Predict the patient 'Outcome' based on aggregated clinical data from their *entire* hospital stay and visualize the distribution of key aggregated features comparing patients with different outcomes.\n\n1.  **Data Preparation**: For each unique patient ('PatientID'), extract all records recorded between their 'AdmissionTime' and 'DischargeTime' (inclusive). For numerical clinical measurements, aggregate the data across all these records for each patient by calculating summary statistics. Include 'Sex' and 'Age' as static patient features. The target variable is 'Outcome'.\n2.  **Model Training**: Split the dataset (containing patient-level aggregated features and demographics) into training and testing sets. Train a binary classification model on the training data to predict the 'Outcome'.\n3.  **Evaluation**: Evaluate the trained model's performance on the test set using appropriate binary classification metrics.\n4.  **Visualization**: Select 2-3 aggregated numerical features. For each selected aggregated feature, create a visualization showing the distribution of its values separately for patients with Outcome=0 and Outcome=1.",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 22,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/22/\", exist_ok=True)\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\nfiltered_data = data[(data['RecordTime'] >= data['AdmissionTime']) & (data['RecordTime'] <= data['DischargeTime'])]\n\nnumerical_cols = filtered_data.select_dtypes(include=[np.number]).columns.tolist()\nnumerical_cols = [col for col in numerical_cols if col not in ['PatientID', 'Outcome', 'LOS', 'Age']]\n\ndef aggregate_patient_data(group):\n    agg_dict = {}\n    for col in numerical_cols:\n        agg_dict[f'{col}_mean'] = group[col].mean()\n        agg_dict[f'{col}_median'] = group[col].median()\n        agg_dict[f'{col}_min'] = group[col].min()\n        agg_dict[f'{col}_max'] = group[col].max()\n        agg_dict[f'{col}_std'] = group[col].std()\n    agg_dict['Sex'] = group['Sex'].iloc[0]\n    agg_dict['Age'] = group['Age'].iloc[0]\n    agg_dict['Outcome'] = group['Outcome'].iloc[0]\n    return pd.Series(agg_dict)\n\npatient_data = filtered_data.groupby('PatientID').apply(aggregate_patient_data).reset_index()\n\nX = patient_data.drop(['PatientID', 'Outcome'], axis=1)\ny = patient_data['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nreport = classification_report(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred)\n\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/22/classification_report.txt\", \"w\") as f:\n    f.write(report)\n    f.write(\"\\n\\nConfusion Matrix:\\n\")\n    f.write(str(conf_matrix))\n    f.write(f\"\\n\\nROC AUC Score: {roc_auc}\")\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/22/confusion_matrix.png\")\nplt.close()\n\nfeatures_to_plot = ['hemoglobin_mean', 'White blood cell count_mean', 'Platelet count_mean']\nfor feature in features_to_plot:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Outcome', y=feature, data=patient_data)\n    plt.title(f'Distribution of {feature} by Outcome')\n    plt.savefig(f\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/22/{feature}_distribution.png\")\n    plt.close()"
    },
    {
        "task": "Build a regression model to predict the change in a specific key clinical marker, 'Hypersensitive c-reactive protein' (Hs-CRP), over the patient's hospital stay, using data available at the time of admission.\n\n1.  **Data Preparation**: For each unique patient ('PatientID'), identify the record with the earliest 'RecordTime' on or after their 'AdmissionTime' (representing the initial state) and the record with the latest 'RecordTime' on or before their 'DischargeTime' (representing the state near discharge). Extract the value of 'Hypersensitive c-reactive protein' from both the initial and latest records for each patient. Calculate the target variable: `Hs-CRP_Change` = (Hs-CRP value in the latest record) - (Hs-CRP value in the initial record). *Include only patients for whom 'Hypersensitive c-reactive protein' values are available in both the initial and latest records.*\n    Create features using the data from the *initial* record for each patient: 'Sex', 'Age', and the values of other numerical clinical measurements available in that initial record.\n\n2.  **Model Training**: Split the prepared dataset (where each sample is a patient, features are derived from the initial record, and the target is the calculated `Hs-CRP_Change`) into training and testing sets. Train a regression model to predict the `Hs-CRP_Change`.\n\n3.  **Evaluation**: Evaluate the trained model's performance on the test set using appropriate regression metrics.\n\n4.  **Visualization**: Create a scatter plot on the test set showing the relationship between the model's *predicted* `Hs-CRP_Change` values and the *actual* `Hs-CRP_Change` values. Color-code or visually distinguish the points on this scatter plot based on the patient's final 'Outcome' (0 or 1).",
        "task_type": "Visualization",
        "dataset": "TJH",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 23,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Data Preparation\ndef prepare_data(df):\n    patients = []\n    for patient_id in df['PatientID'].unique():\n        patient_data = df[df['PatientID'] == patient_id]\n        admission_time = patient_data['AdmissionTime'].iloc[0]\n        discharge_time = patient_data['DischargeTime'].iloc[0]\n        \n        initial_records = patient_data[patient_data['RecordTime'] >= admission_time]\n        latest_records = patient_data[patient_data['RecordTime'] <= discharge_time]\n        \n        if len(initial_records) == 0 or len(latest_records) == 0:\n            continue\n        \n        initial_record = initial_records.sort_values('RecordTime').iloc[0]\n        latest_record = latest_records.sort_values('RecordTime', ascending=False).iloc[0]\n        \n        if pd.isna(initial_record['Hypersensitive c-reactive protein']) or pd.isna(latest_record['Hypersensitive c-reactive protein']):\n            continue\n        \n        hs_crp_change = latest_record['Hypersensitive c-reactive protein'] - initial_record['Hypersensitive c-reactive protein']\n        \n        features = initial_record[['Sex', 'Age'] + [col for col in df.columns if df[col].dtype in ['int64', 'float64'] and col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Hypersensitive c-reactive protein']]]\n        features['Hs-CRP_Change'] = hs_crp_change\n        features['Outcome'] = latest_record['Outcome']\n        patients.append(features)\n    \n    return pd.DataFrame(patients)\n\nprepared_df = prepare_data(df)\nprepared_df = prepared_df.dropna()\n\n# Features and target\nX = prepared_df.drop(['Hs-CRP_Change', 'Outcome'], axis=1)\ny = prepared_df['Hs-CRP_Change']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Evaluation\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Save metrics\nmetrics = {'MSE': mse, 'R2': r2}\nmetrics_df = pd.DataFrame([metrics])\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/23/\", exist_ok=True)\nmetrics_df.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/23/metrics.csv\", index=False)\n\n# Visualization\nplt.figure(figsize=(10, 6))\noutcomes = prepared_df.loc[X_test.index, 'Outcome']\nplt.scatter(y_test, y_pred, c=outcomes, cmap='viridis', alpha=0.5)\nplt.xlabel('Actual Hs-CRP Change')\nplt.ylabel('Predicted Hs-CRP Change')\nplt.colorbar(label='Outcome')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/23/scatter_plot.png\")\nplt.close()"
    },
    {
        "task": "Analyze the provided patient dataset to understand factors associated with patient outcome. This involves: 1. Identifying key lab parameters that show significant differences at or near admission between patients with outcome 0 and outcome 1. 2. Analyzing and describing the trends over time for a few selected important lab parameters for patients in each outcome group, relative to their admission time. 3. Generating a report summarizing the findings, including demographic overview, parameters significantly different at admission, observed trends, and potential clinical insights.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 24,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom datetime import datetime\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/24/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\n\ndf['DaysFromAdmission'] = (df['RecordTime'] - df['AdmissionTime']).dt.total_seconds() / (24 * 3600)\n\ndef analyze_admission_differences():\n    admission_data = df.sort_values('RecordTime').groupby('PatientID').first().reset_index()\n    outcome_0 = admission_data[admission_data['Outcome'] == 0]\n    outcome_1 = admission_data[admission_data['Outcome'] == 1]\n    \n    numeric_cols = admission_data.select_dtypes(include=[np.number]).columns.tolist()\n    numeric_cols = [col for col in numeric_cols if col not in ['PatientID', 'Outcome', 'LOS']]\n    \n    significant_params = []\n    for col in numeric_cols:\n        try:\n            t_stat, p_val = stats.ttest_ind(outcome_0[col].dropna(), outcome_1[col].dropna(), equal_var=False)\n            if p_val < 0.05:\n                significant_params.append((col, p_val, outcome_0[col].mean(), outcome_1[col].mean()))\n        except:\n            continue\n    \n    significant_params.sort(key=lambda x: x[1])\n    return significant_params\n\ndef plot_trends(parameters, num_patients=5):\n    for param in parameters:\n        plt.figure(figsize=(10, 6))\n        for outcome in [0, 1]:\n            patients = df[df['Outcome'] == outcome]['PatientID'].unique()[:num_patients]\n            for pid in patients:\n                patient_data = df[df['PatientID'] == pid].sort_values('RecordTime')\n                plt.plot(patient_data['DaysFromAdmission'], patient_data[param], \n                         label=f'Outcome {outcome}' if pid == patients[0] else \"\", \n                         color='blue' if outcome == 0 else 'red',\n                         alpha=0.5)\n        \n        plt.title(f'Trend of {param} by Outcome Group')\n        plt.xlabel('Days from Admission')\n        plt.ylabel(param)\n        plt.legend()\n        plt.savefig(os.path.join(output_dir, f'trend_{param}.png'))\n        plt.close()\n\ndef generate_report(significant_params):\n    with open(os.path.join(output_dir, 'report.txt'), 'w') as f:\n        f.write(\"Demographic Overview:\\n\")\n        f.write(f\"Total patients: {df['PatientID'].nunique()}\\n\")\n        f.write(f\"Outcome 0: {len(df[df['Outcome'] == 0]['PatientID'].unique())}\\n\")\n        f.write(f\"Outcome 1: {len(df[df['Outcome'] == 1]['PatientID'].unique())}\\n\")\n        f.write(f\"Average Age: {df.groupby('PatientID').first()['Age'].mean()}\\n\")\n        f.write(f\"Sex Distribution (Male/Female): {df.groupby('PatientID').first()['Sex'].value_counts().to_dict()}\\n\\n\")\n        \n        f.write(\"Significantly Different Parameters at Admission:\\n\")\n        for param, p_val, mean0, mean1 in significant_params:\n            f.write(f\"{param}: p-value={p_val:.4f}, Outcome 0 mean={mean0:.2f}, Outcome 1 mean={mean1:.2f}\\n\")\n        \n        f.write(\"\\nClinical Insights:\\n\")\n        f.write(\"- Parameters with significant differences at admission may help predict patient outcomes\\n\")\n        f.write(\"- Trends over time can reveal how different patient groups respond to treatment\\n\")\n\nsignificant_params = analyze_admission_differences()\ntop_params = [x[0] for x in significant_params[:3]]\nplot_trends(top_params)\ngenerate_report(significant_params)"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to investigate how the variability and trajectory of key laboratory parameters within the initial period after admission correlate with patient outcomes (specifically 'Outcome' and 'LOS'). This task requires: 1. Selecting a subset of clinically significant laboratory parameters from the dataset. 2. For each patient, calculating descriptive statistics or trend indicators for the selected parameters using only the records within the first 48 or 72 hours following their 'AdmissionTime'. 3. Analyzing the relationship between these calculated early temporal metrics and the patient's 'Outcome' (binary classification) and 'LOS' (continuous variable). 4. Generating a concise report detailing the chosen parameters, the derived temporal metrics, the statistical methods used for analysis, and a summary of the findings highlighting which early lab dynamics are most strongly associated with patient 'Outcome' and 'LOS', along with potential clinical interpretations.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 25,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/25/\", exist_ok=True)\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['hours_after_admission'] = (data['RecordTime'] - data['AdmissionTime']).dt.total_seconds() / 3600\n\nselected_params = [\n    'Hypersensitive cardiac troponinI', 'hemoglobin', 'Prothrombin time', \n    'albumin', 'White blood cell count', 'glucose', 'D-D dimer', \n    'aspartate aminotransferase', 'creatinine', 'Hypersensitive c-reactive protein'\n]\n\nearly_data = data[data['hours_after_admission'] <= 72].copy()\n\ndef calculate_temporal_metrics(group):\n    metrics = {}\n    for param in selected_params:\n        vals = group[param].dropna()\n        if len(vals) > 0:\n            metrics[f'{param}_mean'] = np.mean(vals)\n            metrics[f'{param}_std'] = np.std(vals)\n            metrics[f'{param}_slope'] = np.polyfit(group['hours_after_admission'], group[param], 1)[0] if len(vals) > 1 else np.nan\n        else:\n            metrics[f'{param}_mean'] = np.nan\n            metrics[f'{param}_std'] = np.nan\n            metrics[f'{param}_slope'] = np.nan\n    return pd.Series(metrics)\n\npatient_metrics = early_data.groupby('PatientID').apply(calculate_temporal_metrics).reset_index()\nmerged_data = pd.merge(patient_metrics, data[['PatientID', 'Outcome', 'LOS']].drop_duplicates(), on='PatientID')\n\nmerged_data['Outcome_binary'] = merged_data['Outcome'].apply(lambda x: 1 if x in ['Death', 'ICU Transfer'] else 0)\nmerged_data.dropna(inplace=True)\n\nX = merged_data[[col for col in merged_data.columns if col.endswith(('_mean', '_std', '_slope'))]]\ny_outcome = merged_data['Outcome_binary']\ny_los = merged_data['LOS']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_outcome, test_size=0.2, random_state=42)\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\nreport_outcome = classification_report(y_test, y_pred, output_dict=True)\n\nX_train_los, X_test_los, y_train_los, y_test_los = train_test_split(X_scaled, y_los, test_size=0.2, random_state=42)\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_los, y_train_los)\ny_pred_los = lin_reg.predict(X_test_los)\nmse_los = mean_squared_error(y_test_los, y_pred_los)\n\ncoef_df = pd.DataFrame({\n    'feature': X.columns,\n    'outcome_coef': log_reg.coef_[0],\n    'los_coef': lin_reg.coef_\n}).sort_values('outcome_coef', key=abs, ascending=False)\n\ntop_outcome_features = coef_df.head(5)['feature'].tolist()\ntop_los_features = coef_df.sort_values('los_coef', key=abs, ascending=False).head(5)['feature'].tolist()\n\nreport = {\n    'selected_parameters': selected_params,\n    'outcome_model_performance': report_outcome,\n    'los_model_mse': mse_los,\n    'top_outcome_features': top_outcome_features,\n    'top_los_features': top_los_features,\n    'outcome_coefficients': coef_df.to_dict('records'),\n    'los_coefficients': coef_df.to_dict('records')\n}\n\nreport_df = pd.DataFrame.from_dict(report, orient='index')\nreport_df.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/25/analysis_report.csv\")\n\nfor feature in top_outcome_features[:3]:\n    plt.figure()\n    sns.boxplot(x='Outcome', y=feature, data=merged_data)\n    plt.title(f'Distribution of {feature} by Outcome')\n    plt.savefig(f\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/25/{feature}_outcome.png\")\n    plt.close()\n\nfor feature in top_los_features[:3]:\n    plt.figure()\n    sns.scatterplot(x=feature, y='LOS', data=merged_data)\n    plt.title(f'{feature} vs LOS')\n    plt.savefig(f\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/25/{feature}_los.png\")\n    plt.close()"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to identify temporal patterns and trajectories of key laboratory parameters that are predictive of patient outcome (Outcome) and length of stay (LOS). The task requires: 1. Selecting a subset of clinically relevant laboratory parameters with sufficient temporal variability across patient records. 2. For each patient, characterize the time-series data for the selected parameters. This may involve aligning data by admission time and engineering features from the trajectories over the entire duration of available records or within a defined analytical window. 3. Utilizing these engineered trajectory features as input, build models to predict the patient's Outcome (binary classification) and LOS (regression or classification into duration bins). 4. Analyze the trained models to identify which engineered temporal features derived from the lab parameter trajectories are most influential or predictive for patient Outcome and LOS. 5. Generate a report summarizing the chosen parameters, the approach for trajectory characterization and feature engineering, the methodology for predictive modeling, the performance metrics of the models, and the key findings regarding the relationship between specific lab parameter trajectories and patient outcomes/LOS.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 26,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import classification_report, mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/26/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\nselected_params = [\n    'Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', \n    'Prothrombin time', 'albumin', 'Platelet count', 'White blood cell count', \n    'Urea', 'lymphocyte count', 'glucose', 'neutrophils count', 'D-D dimer', \n    'aspartate aminotransferase', 'Uric acid', 'creatinine'\n]\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\ndata['LOS'] = (data['DischargeTime'] - data['AdmissionTime']).dt.total_seconds() / 86400\n\ndef extract_trajectory_features(group):\n    features = {}\n    for param in selected_params:\n        if group[param].notna().sum() > 1:\n            features[f'{param}_mean'] = group[param].mean()\n            features[f'{param}_std'] = group[param].std()\n            features[f'{param}_slope'] = np.polyfit(range(len(group)), group[param].fillna(group[param].mean()), 1)[0]\n            features[f'{param}_min'] = group[param].min()\n            features[f'{param}_max'] = group[param].max()\n        else:\n            features[f'{param}_mean'] = group[param].mean()\n            features[f'{param}_std'] = 0\n            features[f'{param}_slope'] = 0\n            features[f'{param}_min'] = group[param].mean()\n            features[f'{param}_max'] = group[param].mean()\n    return pd.Series(features)\n\ntrajectory_features = data.groupby('PatientID').apply(extract_trajectory_features).reset_index()\n\noutcome_data = data.drop_duplicates('PatientID')[['PatientID', 'Outcome', 'LOS']]\ntrajectory_features = trajectory_features.merge(outcome_data, on='PatientID', how='left')\n\nX = trajectory_features.drop(['PatientID', 'Outcome', 'LOS'], axis=1)\ny_outcome = trajectory_features['Outcome']\ny_los = trajectory_features['LOS']\n\nX = X.fillna(X.mean())\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n\nle = LabelEncoder()\ny_outcome_encoded = le.fit_transform(y_outcome)\n\nX_train, X_test, y_outcome_train, y_outcome_test = train_test_split(X_scaled, y_outcome_encoded, test_size=0.3, random_state=42)\nX_train_los, X_test_los, y_los_train, y_los_test = train_test_split(X_scaled, y_los, test_size=0.3, random_state=42)\n\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_outcome_train)\ny_outcome_pred = clf.predict(X_test)\nclf_report = classification_report(y_outcome_test, y_outcome_pred, output_dict=True)\nclf_report_df = pd.DataFrame(clf_report).transpose()\nclf_report_df.to_csv(os.path.join(output_dir, 'outcome_classification_report.csv'))\n\nreg = RandomForestRegressor(random_state=42)\nreg.fit(X_train_los, y_los_train)\ny_los_pred = reg.predict(X_test_los)\nmse = mean_squared_error(y_los_test, y_los_pred)\nr2 = r2_score(y_los_test, y_los_pred)\nlos_metrics = pd.DataFrame({'MSE': [mse], 'R2': [r2]})\nlos_metrics.to_csv(os.path.join(output_dir, 'los_regression_metrics.csv'))\n\nfeature_importance_outcome = pd.DataFrame({'Feature': X.columns, 'Importance': clf.feature_importances_})\nfeature_importance_outcome = feature_importance_outcome.sort_values(by='Importance', ascending=False)\nfeature_importance_outcome.to_csv(os.path.join(output_dir, 'outcome_feature_importance.csv'), index=False)\n\nfeature_importance_los = pd.DataFrame({'Feature': X.columns, 'Importance': reg.feature_importances_})\nfeature_importance_los = feature_importance_los.sort_values(by='Importance', ascending=False)\nfeature_importance_los.to_csv(os.path.join(output_dir, 'los_feature_importance.csv'), index=False)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_outcome.head(20))\nplt.title('Top 20 Features for Outcome Prediction')\nplt.savefig(os.path.join(output_dir, 'outcome_feature_importance.png'))\nplt.close()\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_los.head(20))\nplt.title('Top 20 Features for LOS Prediction')\nplt.savefig(os.path.join(output_dir, 'los_feature_importance.png'))\nplt.close()\n\nreport = f\"\"\"\nAnalysis Report:\n1. Selected Parameters: {selected_params}\n2. Feature Engineering: Mean, Std, Slope, Min, Max for each parameter trajectory\n3. Models: Random Forest (Classification for Outcome, Regression for LOS)\n4. Outcome Model Performance:\n   - Accuracy: {clf_report['accuracy']}\n   - Precision: {clf_report['weighted avg']['precision']}\n   - Recall: {clf_report['weighted avg']['recall']}\n   - F1-score: {clf_report['weighted avg']['f1-score']}\n5. LOS Model Performance:\n   - MSE: {mse}\n   - R2: {r2}\n6. Key Findings:\n   - Top predictive features for Outcome: {feature_importance_outcome['Feature'].head(5).tolist()}\n   - Top predictive features for LOS: {feature_importance_los['Feature'].head(5).tolist()}\n\"\"\"\n\nwith open(os.path.join(output_dir, 'analysis_report.txt'), 'w') as f:\n    f.write(report)"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to investigate the relationship between the *intra-patient variability* of key laboratory parameters measured during hospitalization and patient outcomes (Outcome and Length of Stay - LOS). This task requires: 1. Selecting a subset of laboratory parameters that are frequently measured across multiple days for individual patients and show potential for significant fluctuation within a patient's stay. 2. For each selected parameter, calculate a measure of intra-patient variability across all available records for each patient *during their hospitalization period* (between AdmissionTime and DischargeTime). You should address how to handle parameters with only one measurement per patient. 3. Analyze the association between the calculated variability measures and the patient's Outcome and LOS. 4. Generate a report summarizing the parameters analyzed, the chosen variability metrics, the analytical methods used, and the findings on which parameters' variability is most strongly associated with Outcome and LOS, including relevant descriptive statistics and visualizations.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 27,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, r2_score\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/27/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\n\nlab_columns = [col for col in data.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']]\n\nmeasurement_counts = data.groupby('PatientID')[lab_columns].count().mean().sort_values(ascending=False)\nselected_params = measurement_counts[measurement_counts >= 2].index.tolist()\n\ndef calculate_variability(group):\n    variability = {}\n    for param in selected_params:\n        values = group[param].dropna()\n        if len(values) > 1:\n            variability[f'{param}_cv'] = values.std() / values.mean()\n            variability[f'{param}_range'] = values.max() - values.min()\n        else:\n            variability[f'{param}_cv'] = np.nan\n            variability[f'{param}_range'] = np.nan\n    return pd.Series(variability)\n\nvariability_df = data.groupby('PatientID').apply(calculate_variability)\nvariability_df.reset_index(inplace=True)\n\npatient_info = data[['PatientID', 'Outcome', 'LOS']].drop_duplicates()\nvariability_df = variability_df.merge(patient_info, on='PatientID', how='left')\n\noutcome_results = {}\nlos_results = {}\n\nfor param in selected_params:\n    cv_col = f'{param}_cv'\n    range_col = f'{param}_range'\n    \n    temp_df = variability_df.dropna(subset=[cv_col, 'Outcome'])\n    if len(temp_df) > 0:\n        X = temp_df[[cv_col]].values\n        y = temp_df['Outcome'].values\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        model = LogisticRegression()\n        model.fit(X_train_scaled, y_train)\n        y_pred = model.predict(X_test_scaled)\n        report = classification_report(y_test, y_pred, output_dict=True)\n        outcome_results[param] = {\n            'accuracy': report['accuracy'],\n            'precision': report['weighted avg']['precision'],\n            'recall': report['weighted avg']['recall'],\n            'f1': report['weighted avg']['f1-score']\n        }\n    \n    temp_df = variability_df.dropna(subset=[cv_col, 'LOS'])\n    if len(temp_df) > 0:\n        X = temp_df[[cv_col]].values\n        y = temp_df['LOS'].values\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        model = LinearRegression()\n        model.fit(X_train_scaled, y_train)\n        y_pred = model.predict(X_test_scaled)\n        r2 = r2_score(y_test, y_pred)\n        los_results[param] = {\n            'r2': r2,\n            'coef': model.coef_[0]\n        }\n\noutcome_df = pd.DataFrame.from_dict(outcome_results, orient='index')\nlos_df = pd.DataFrame.from_dict(los_results, orient='index')\n\noutcome_df.sort_values('f1', ascending=False, inplace=True)\nlos_df.sort_values('r2', ascending=False, inplace=True)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x=outcome_df.index[:10], y=outcome_df['f1'][:10])\nplt.title('Top 10 Parameters by F1-score for Outcome Prediction')\nplt.xticks(rotation=45)\nplt.ylabel('F1-score')\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, 'top10_outcome_f1.png'))\nplt.close()\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x=los_df.index[:10], y=los_df['r2'][:10])\nplt.title('Top 10 Parameters by R-squared for LOS Prediction')\nplt.xticks(rotation=45)\nplt.ylabel('R-squared')\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, 'top10_los_r2.png'))\nplt.close()\n\nreport_text = f\"\"\"\nAnalysis Report: Intra-patient Variability and Patient Outcomes\n\nParameters Analyzed: {len(selected_params)} lab parameters with multiple measurements per patient\n\nVariability Metrics Used:\n- Coefficient of Variation (CV)\n- Range (max-min)\n\nAnalytical Methods:\n- Logistic Regression for Outcome prediction\n- Linear Regression for LOS prediction\n- StandardScaler for feature scaling\n- Train-test split (80-20)\n\nKey Findings:\n\nOutcome Prediction (Top 5 parameters by F1-score):\n{outcome_df.head(5).to_string()}\n\nLOS Prediction (Top 5 parameters by R-squared):\n{los_df.head(5).to_string()}\n\nVisualizations saved as:\n- top10_outcome_f1.png\n- top10_los_r2.png\n\"\"\"\n\nwith open(os.path.join(output_dir, 'analysis_report.txt'), 'w') as f:\n    f.write(report_text)\n\nvariability_df.to_csv(os.path.join(output_dir, 'patient_variability_metrics.csv'), index=False)\noutcome_df.to_csv(os.path.join(output_dir, 'outcome_prediction_results.csv'))\nlos_df.to_csv(os.path.join(output_dir, 'los_prediction_results.csv'))"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to investigate the relationship between the *change* in key laboratory parameters from the first recorded measurement to the last recorded measurement during hospitalization and the patient's outcome ('Outcome'). This task requires: 1. For a selected subset of laboratory parameters that appear across multiple records for individual patients, identify the value from the earliest available record and the value from the latest available record for each patient. 2. Calculate the absolute or percentage change for each selected parameter between the earliest and latest measured values for each patient. 3. Compare the distribution and statistical properties of these calculated 'first-to-last' changes for patients in the 'Outcome=0' group versus patients in the 'Outcome=1' group using appropriate statistical tests and visualizations. 4. Generate a report summarizing the selected parameters, the methodology for defining and calculating the 'first-to-last' change, the results of the comparative analysis highlighting parameters with statistically significant differences in change between the outcome groups, and potential clinical interpretations regarding how the trajectory (specifically, the overall change) of certain lab markers during hospitalization is associated with patient outcome.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 28,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/28/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\nlab_parameters = [\n    'Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', 'Prothrombin time', \n    'procalcitonin', 'eosinophils(%)', 'Interleukin 2 receptor', 'Alkaline phosphatase', \n    'albumin', 'basophil(%)', 'Interleukin 10', 'Total bilirubin', 'Platelet count', \n    'monocytes(%)', 'antithrombin', 'Interleukin 8', 'indirect bilirubin', \n    'Red blood cell distribution width ', 'neutrophils(%)', 'total protein', \n    'Quantification of Treponema pallidum antibodies', 'Prothrombin activity', \n    'HBsAg', 'mean corpuscular volume', 'hematocrit', 'White blood cell count', \n    'Tumor necrosis factorα', 'mean corpuscular hemoglobin concentration', \n    'fibrinogen', 'Interleukin 1β', 'Urea', 'lymphocyte count', 'PH value', \n    'Red blood cell count', 'Eosinophil count', 'Corrected calcium', 'Serum potassium', \n    'glucose', 'neutrophils count', 'Direct bilirubin', 'Mean platelet volume', \n    'ferritin', 'RBC distribution width SD', 'Thrombin time', '(%)lymphocyte', \n    'HCV antibody quantification', 'D-D dimer', 'Total cholesterol', \n    'aspartate aminotransferase', 'Uric acid', 'HCO3-', 'calcium', \n    'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)', \n    'Lactate dehydrogenase', 'platelet large cell ratio ', 'Interleukin 6', \n    'Fibrin degradation products', 'monocytes count', 'PLT distribution width', \n    'globulin', 'γ-glutamyl transpeptidase', 'International standard ratio', \n    'basophil count(#)', 'mean corpuscular hemoglobin ', \n    'Activation of partial thromboplastin time', 'Hypersensitive c-reactive protein', \n    'HIV antibody quantification', 'serum sodium', 'thrombocytocrit', 'ESR', \n    'glutamic-pyruvic transaminase', 'eGFR', 'creatinine'\n]\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf_sorted = df.sort_values(['PatientID', 'RecordTime'])\n\nfirst_last = df_sorted.groupby('PatientID').agg({\n    **{param: ['first', 'last'] for param in lab_parameters},\n    'Outcome': 'first'\n})\n\nchange_results = []\nfor param in lab_parameters:\n    first_col = (param, 'first')\n    last_col = (param, 'last')\n    if first_col in first_last.columns and last_col in first_last.columns:\n        first_last[f'{param}_abs_change'] = first_last[last_col] - first_last[first_col]\n        first_last[f'{param}_pct_change'] = (first_last[last_col] - first_last[first_col]) / first_last[first_col] * 100\n        change_results.append(param)\n\noutcome_0 = first_last[first_last['Outcome'] == 0]\noutcome_1 = first_last[first_last['Outcome'] == 1]\n\nresults = []\nfor param in change_results:\n    abs_change_col = f'{param}_abs_change'\n    pct_change_col = f'{param}_pct_change'\n    \n    t_stat_abs, p_val_abs = stats.ttest_ind(\n        outcome_0[abs_change_col].dropna(),\n        outcome_1[abs_change_col].dropna(),\n        equal_var=False\n    )\n    \n    t_stat_pct, p_val_pct = stats.ttest_ind(\n        outcome_0[pct_change_col].dropna(),\n        outcome_1[pct_change_col].dropna(),\n        equal_var=False\n    )\n    \n    results.append({\n        'Parameter': param,\n        'Mean Abs Change Outcome 0': outcome_0[abs_change_col].mean(),\n        'Mean Abs Change Outcome 1': outcome_1[abs_change_col].mean(),\n        'Abs Change p-value': p_val_abs,\n        'Mean Pct Change Outcome 0': outcome_0[pct_change_col].mean(),\n        'Mean Pct Change Outcome 1': outcome_1[pct_change_col].mean(),\n        'Pct Change p-value': p_val_pct\n    })\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(os.path.join(output_dir, 'change_comparison_results.csv'), index=False)\n\nsignificant_params = results_df[\n    (results_df['Abs Change p-value'] < 0.05) | \n    (results_df['Pct Change p-value'] < 0.05)\n]['Parameter'].tolist()\n\nfor param in significant_params:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(\n        x='Outcome',\n        y=f'{param}_abs_change',\n        data=first_last.reset_index()\n    )\n    plt.title(f'Absolute Change in {param} by Outcome')\n    plt.savefig(os.path.join(output_dir, f'{param}_abs_change_boxplot.png'))\n    plt.close()\n    \n    plt.figure(figsize=(10, 6))\n    sns.boxplot(\n        x='Outcome',\n        y=f'{param}_pct_change',\n        data=first_last.reset_index()\n    )\n    plt.title(f'Percentage Change in {param} by Outcome')\n    plt.savefig(os.path.join(output_dir, f'{param}_pct_change_boxplot.png'))\n    plt.close()\n\nreport = f\"\"\"\nAnalysis Report: Relationship between Lab Parameter Changes and Patient Outcome\n\nMethodology:\n1. For each patient, identified the first and last recorded values for each lab parameter.\n2. Calculated both absolute and percentage changes from first to last measurement.\n3. Compared changes between Outcome=0 and Outcome=1 groups using Welch's t-test.\n\nSignificant Findings:\n\"\"\"\nfor _, row in results_df[(results_df['Abs Change p-value'] < 0.05) | (results_df['Pct Change p-value'] < 0.05)].iterrows():\n    report += f\"\"\"\nParameter: {row['Parameter']}\n- Absolute Change: \n  - Outcome 0 mean: {row['Mean Abs Change Outcome 0']:.2f}\n  - Outcome 1 mean: {row['Mean Abs Change Outcome 1']:.2f}\n  - p-value: {row['Abs Change p-value']:.4f}\n- Percentage Change:\n  - Outcome 0 mean: {row['Mean Pct Change Outcome 0']:.2f}%\n  - Outcome 1 mean: {row['Mean Pct Change Outcome 1']:.2f}%\n  - p-value: {row['Pct Change p-value']:.4f}\n\"\"\"\n\nwith open(os.path.join(output_dir, 'analysis_report.txt'), 'w') as f:\n    f.write(report)"
    },
    {
        "task": "Analyze the relationship between patient outcomes (Outcome and Length of Stay) and the values of key laboratory parameters measured within the 48-hour window immediately preceding hospital discharge. This task requires: 1. For each patient, identify their DischargeTime and define a 48-hour pre-discharge window (DischargeTime - 48 hours to DischargeTime). 2. For a selected subset of clinically relevant and frequently measured laboratory parameters, extract the value from the *latest* available record for that specific parameter within this 48-hour pre-discharge window for each patient. If no record exists for a parameter within this window for a patient, handle this missing data appropriately. 3. Compare the distributions and statistical properties of these extracted 'pre-discharge' parameter values between the two Outcome groups (0 vs 1) using appropriate statistical tests and visualizations. Identify parameters with statistically significant differences. 4. Analyze the correlation between these extracted 'pre-discharge' parameter values and the patient's Length of Stay (LOS) using appropriate correlation coefficients. 5. Generate a report summarizing the methodology, the list of parameters analyzed, the findings regarding their 'pre-discharge' values and associations with both Outcome and LOS, including relevant descriptive statistics, test results, and interpretations of the findings in a clinical context.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 29,
        "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport os\n\n# Ensure output directory exists\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/29/\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Load dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Convert time columns to datetime\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\n\n# Define clinically relevant parameters\nselected_params = [\n    'hemoglobin', 'White blood cell count', 'Platelet count', \n    'albumin', 'total protein', 'glucose', 'creatinine',\n    'Urea', 'Serum sodium', 'Serum potassium', 'HCO3-',\n    'Hypersensitive c-reactive protein', 'Prothrombin time',\n    'Lactate dehydrogenase', 'aspartate aminotransferase',\n    'glutamic-pyruvic transaminase', 'D-D dimer'\n]\n\n# Process data\nresults = []\nfor patient_id, group in df.groupby('PatientID'):\n    discharge_time = group['DischargeTime'].iloc[0]\n    window_start = discharge_time - pd.Timedelta(hours=48)\n    \n    window_data = group[(group['RecordTime'] >= window_start) & (group['RecordTime'] <= discharge_time)]\n    \n    patient_data = {\n        'PatientID': patient_id,\n        'Outcome': group['Outcome'].iloc[0],\n        'LOS': group['LOS'].iloc[0]\n    }\n    \n    for param in selected_params:\n        param_data = window_data[param].dropna()\n        if not param_data.empty:\n            patient_data[param] = param_data.iloc[-1]\n        else:\n            patient_data[param] = np.nan\n    \n    results.append(patient_data)\n\nresult_df = pd.DataFrame(results)\n\n# Outcome analysis\noutcome_0 = result_df[result_df['Outcome'] == 0]\noutcome_1 = result_df[result_df['Outcome'] == 1]\n\n# Statistical tests and visualizations\nreport = {}\nfor param in selected_params:\n    # Descriptive stats\n    desc_0 = outcome_0[param].describe()\n    desc_1 = outcome_1[param].describe()\n    \n    # Remove NaN values for tests\n    data_0 = outcome_0[param].dropna()\n    data_1 = outcome_1[param].dropna()\n    \n    if len(data_0) > 1 and len(data_1) > 1:\n        # Normality test\n        _, p_normal_0 = stats.shapiro(data_0)\n        _, p_normal_1 = stats.shapiro(data_1)\n        \n        # Test selection\n        if p_normal_0 > 0.05 and p_normal_1 > 0.05:\n            _, p_val = stats.ttest_ind(data_0, data_1)\n            test_used = 't-test'\n        else:\n            _, p_val = stats.mannwhitneyu(data_0, data_1)\n            test_used = 'Mann-Whitney U'\n        \n        # Visualization\n        plt.figure(figsize=(10, 6))\n        sns.boxplot(x='Outcome', y=param, data=result_df)\n        plt.title(f'{param} by Outcome Group')\n        plt.savefig(f'{output_dir}{param}_outcome_boxplot.png')\n        plt.close()\n        \n        report[param] = {\n            'Outcome_0_stats': desc_0.to_dict(),\n            'Outcome_1_stats': desc_1.to_dict(),\n            'test_used': test_used,\n            'p_value': p_val,\n            'significant': p_val < 0.05\n        }\n\n# LOS correlation analysis\ncorr_results = {}\nfor param in selected_params:\n    corr_data = result_df[['LOS', param]].dropna()\n    if len(corr_data) > 2:\n        r, p = stats.pearsonr(corr_data['LOS'], corr_data[param])\n        corr_results[param] = {\n            'correlation_coefficient': r,\n            'p_value': p,\n            'significant': p < 0.05\n        }\n        \n        plt.figure(figsize=(10, 6))\n        sns.regplot(x=param, y='LOS', data=corr_data)\n        plt.title(f'{param} vs LOS')\n        plt.savefig(f'{output_dir}{param}_LOS_correlation.png')\n        plt.close()\n\n# Save report\nreport_df = pd.DataFrame.from_dict(report, orient='index')\nreport_df.to_csv(f'{output_dir}outcome_analysis_report.csv')\n\ncorr_df = pd.DataFrame.from_dict(corr_results, orient='index')\ncorr_df.to_csv(f'{output_dir}LOS_correlation_report.csv')\n\n# Summary report\nwith open(f'{output_dir}summary_report.txt', 'w') as f:\n    f.write(\"=== Outcome Analysis ===\\n\")\n    f.write(f\"Parameters analyzed: {', '.join(selected_params)}\\n\\n\")\n    f.write(\"Significant differences between outcome groups:\\n\")\n    for param, data in report.items():\n        if data['significant']:\n            f.write(f\"{param}: p={data['p_value']:.4f} ({data['test_used']})\\n\")\n            f.write(f\"  Outcome 0: mean={data['Outcome_0_stats']['mean']:.2f}\\n\")\n            f.write(f\"  Outcome 1: mean={data['Outcome_1_stats']['mean']:.2f}\\n\\n\")\n    \n    f.write(\"\\n=== LOS Correlation Analysis ===\\n\")\n    f.write(\"Significant correlations with LOS:\\n\")\n    for param, data in corr_results.items():\n        if data['significant']:\n            f.write(f\"{param}: r={data['correlation_coefficient']:.2f}, p={data['p_value']:.4f}\\n\")"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to identify distinct patient cohorts based on their *average* laboratory parameter profiles during hospitalization and investigate the association of these cohorts with patient outcomes (Outcome and Length of Stay). This task requires: 1. Selecting a subset of clinically relevant laboratory parameters that are frequently measured across patients. 2. For each patient, calculate the *mean* value for each selected parameter using all available measurements recorded *between* their 'AdmissionTime' and 'DischargeTime'. You will need to consider and document how to handle parameters with insufficient measurements for a given patient. 3. Apply a clustering algorithm  to group patients based on their calculated mean laboratory parameter values. You should explore different numbers of clusters and justify your choice. 4. Characterize each identified cluster by examining the average values of the selected laboratory parameters within the cluster and comparing them across clusters. 5. Statistically analyze whether the distribution of 'Outcome' (binary) and the mean 'LOS' (continuous) differ significantly between the identified patient clusters using appropriate statistical tests. 6. Generate a report summarizing the chosen parameters, the approach for calculating mean values and handling missing data, the clustering methodology and resulting cluster characteristics, the statistical analysis of outcome/LOS differences between clusters, and potential clinical interpretations of the identified cohorts in relation to their lab profiles and outcomes.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 30,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.impute import SimpleImputer\nfrom scipy.stats import chi2_contingency, f_oneway\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\nlab_params = [\n    'hemoglobin', 'Serum chloride', 'Prothrombin time', 'albumin', \n    'Total bilirubin', 'Platelet count', 'total protein', 'hematocrit', \n    'White blood cell count', 'Urea', 'Red blood cell count', 'glucose', \n    'Direct bilirubin', 'ferritin', 'Total cholesterol', 'aspartate aminotransferase',\n    'Uric acid', 'calcium', 'Lactate dehydrogenase', 'globulin',\n    'Hypersensitive c-reactive protein', 'serum sodium', 'creatinine'\n]\n\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\n\npatient_means = []\nfor patient_id, group in df.groupby('PatientID'):\n    admission = group['AdmissionTime'].iloc[0]\n    discharge = group['DischargeTime'].iloc[0]\n    during_hospitalization = group[(group['RecordTime'] >= admission) & (group['RecordTime'] <= discharge)]\n    \n    mean_values = {'PatientID': patient_id}\n    for param in lab_params:\n        param_values = during_hospitalization[param].dropna()\n        mean_values[param] = param_values.mean() if len(param_values) > 0 else np.nan\n    patient_means.append(mean_values)\n\nmean_df = pd.DataFrame(patient_means)\nmean_df = mean_df.set_index('PatientID')\n\nimputer = SimpleImputer(strategy='median')\nimputed_data = imputer.fit_transform(mean_df)\nimputed_df = pd.DataFrame(imputed_data, columns=mean_df.columns, index=mean_df.index)\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(imputed_df)\n\ninertia = []\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_data)\n    inertia.append(kmeans.inertia_)\n\noptimal_k = 4\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nclusters = kmeans.fit_predict(scaled_data)\nimputed_df['Cluster'] = clusters\n\ncluster_profiles = imputed_df.groupby('Cluster').mean()\n\nmerged_df = imputed_df.merge(df[['PatientID', 'Outcome', 'LOS']].drop_duplicates(), left_index=True, right_on='PatientID', how='left')\n\noutcome_table = pd.crosstab(merged_df['Cluster'], merged_df['Outcome'])\nchi2, p_outcome, _, _ = chi2_contingency(outcome_table)\n\nlos_groups = [merged_df[merged_df['Cluster'] == i]['LOS'] for i in range(optimal_k)]\nf_stat, p_los = f_oneway(*los_groups)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/30/\"\nos.makedirs(output_dir, exist_ok=True)\n\nimputed_df.to_csv(os.path.join(output_dir, 'patient_clusters.csv'))\ncluster_profiles.to_csv(os.path.join(output_dir, 'cluster_profiles.csv'))\n\nwith open(os.path.join(output_dir, 'statistical_results.txt'), 'w') as f:\n    f.write(f\"Outcome association p-value: {p_outcome}\\n\")\n    f.write(f\"LOS association p-value: {p_los}\\n\")"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to investigate the relationship between the frequency of laboratory parameter measurements during hospitalization and patient outcome ('Outcome'). This task requires: 1. Selecting a subset of clinically relevant laboratory parameters that are expected to be measured multiple times during a hospital stay. 2. For each patient, determine the total number of measurements recorded for each selected parameter between their 'AdmissionTime' and 'DischargeTime'. 3. Calculate the total duration of hospitalization for each patient ('LOS'). 4. Analyze the relationship between the total number of measurements (or measurement density) for the selected parameters and the patient's 'Outcome' (0 for discharged, 1 for died). This analysis could involve comparing the distribution of measurement counts/density between outcome groups using appropriate statistical tests or visualization. 5. Generate a report summarizing the selected parameters, the methodology for calculating measurement frequency, the results of the comparative analysis highlighting parameters whose measurement frequency is significantly associated with outcome, and potential interpretations regarding how monitoring intensity might differ between patients with different outcomes.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 31,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/31/\", exist_ok=True)\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\nselected_params = [\n    'hemoglobin', 'Serum chloride', 'Prothrombin time', 'albumin', \n    'Total bilirubin', 'Platelet count', 'White blood cell count', \n    'glucose', 'Urea', 'creatinine', 'serum sodium', 'calcium'\n]\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\n\nmeasurement_counts = data.groupby('PatientID').agg({\n    **{param: lambda x: x[param].count() for param in selected_params},\n    'Outcome': 'first',\n    'LOS': 'first'\n}).reset_index()\n\nmeasurement_density = measurement_counts.copy()\nfor param in selected_params:\n    measurement_density[f'{param}_density'] = measurement_counts[param] / measurement_counts['LOS']\n\noutcome_0 = measurement_density[measurement_density['Outcome'] == 0]\noutcome_1 = measurement_density[measurement_density['Outcome'] == 1]\n\nresults = []\nfor param in selected_params:\n    t_stat, p_val = stats.ttest_ind(outcome_0[f'{param}_density'], outcome_1[f'{param}_density'], nan_policy='omit')\n    results.append({\n        'Parameter': param,\n        'Mean Density (Outcome=0)': outcome_0[f'{param}_density'].mean(),\n        'Mean Density (Outcome=1)': outcome_1[f'{param}_density'].mean(),\n        'T-statistic': t_stat,\n        'P-value': p_val\n    })\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/31/statistical_results.csv\", index=False)\n\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=pd.melt(measurement_density, id_vars=['Outcome'], value_vars=[f'{param}_density' for param in selected_params]), \n            x='variable', y='value', hue='Outcome')\nplt.xticks(rotation=45)\nplt.title('Measurement Density by Outcome')\nplt.tight_layout()\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/31/measurement_density_comparison.png\")\nplt.close()\n\nreport = f\"\"\"\nAnalysis Report:\n1. Selected Parameters: {', '.join(selected_params)}\n2. Methodology: \n   - For each patient, counted measurements of each parameter between admission and discharge\n   - Calculated measurement density as (count of measurements) / (length of stay in days)\n   - Compared measurement densities between outcome groups using t-tests\n3. Significant Findings:\n\"\"\"\nfor _, row in results_df[results_df['P-value'] < 0.05].iterrows():\n    report += f\"   - {row['Parameter']}: p-value = {row['P-value']:.4f}, higher density in {'non-survivors' if row['Mean Density (Outcome=1)'] > row['Mean Density (Outcome=0)'] else 'survivors'}\\n\"\n\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/31/analysis_report.txt\", \"w\") as f:\n    f.write(report)"
    },
    {
        "task": "Analyze the relationship between the extreme values (peak and nadir) of key laboratory parameters recorded during hospitalization and patient outcomes (Outcome and Length of Stay). This task requires: 1. Selecting a subset of clinically relevant laboratory parameters that are measured multiple times for at least some patients. 2. For each patient, identify the maximum (peak) and minimum (nadir) value recorded for each selected parameter across all their records between their 'AdmissionTime' and 'DischargeTime'. You should document how you handle parameters with only one measurement per patient during their stay. 3. Investigate the association between these calculated peak and nadir values and the patient's 'Outcome' (binary classification) and 'LOS' (continuous variable). This analysis could involve comparing peak/nadir distributions between outcome groups using statistical tests and visualizations, and assessing the correlation between peak/nadir values and LOS. 4. Generate a report summarizing the chosen parameters, the methodology for extracting extreme values, the results of the association analysis highlighting parameters whose peak or nadir values are significantly related to Outcome or LOS, and potential clinical interpretations.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 32,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/32/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\nlab_parameters = [\n    'Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', \n    'Prothrombin time', 'procalcitonin', 'eosinophils(%)', \n    'Interleukin 2 receptor', 'Alkaline phosphatase', 'albumin', \n    'basophil(%)', 'Interleukin 10', 'Total bilirubin', \n    'Platelet count', 'monocytes(%)', 'antithrombin', \n    'Interleukin 8', 'indirect bilirubin', 'Red blood cell distribution width ', \n    'neutrophils(%)', 'total protein', 'Quantification of Treponema pallidum antibodies', \n    'Prothrombin activity', 'HBsAg', 'mean corpuscular volume', \n    'hematocrit', 'White blood cell count', 'Tumor necrosis factorα', \n    'mean corpuscular hemoglobin concentration', 'fibrinogen', 'Interleukin 1β', \n    'Urea', 'lymphocyte count', 'PH value', 'Red blood cell count', \n    'Eosinophil count', 'Corrected calcium', 'Serum potassium', \n    'glucose', 'neutrophils count', 'Direct bilirubin', \n    'Mean platelet volume', 'ferritin', 'RBC distribution width SD', \n    'Thrombin time', '(%)lymphocyte', 'HCV antibody quantification', \n    'D-D dimer', 'Total cholesterol', 'aspartate aminotransferase', \n    'Uric acid', 'HCO3-', 'calcium', \n    'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)', \n    'Lactate dehydrogenase', 'platelet large cell ratio ', 'Interleukin 6', \n    'Fibrin degradation products', 'monocytes count', 'PLT distribution width', \n    'globulin', 'γ-glutamyl transpeptidase', 'International standard ratio', \n    'basophil count(#)', 'mean corpuscular hemoglobin ', \n    'Activation of partial thromboplastin time', 'Hypersensitive c-reactive protein', \n    'HIV antibody quantification', 'serum sodium', 'thrombocytocrit', \n    'ESR', 'glutamic-pyruvic transaminase', 'eGFR', 'creatinine'\n]\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\n\npatient_records = data.groupby('PatientID').filter(lambda x: len(x) > 1)\npatients_with_multiple_measurements = patient_records.groupby('PatientID')[lab_parameters].count().gt(1).any(axis=1)\nselected_patients = patients_with_multiple_measurements[patients_with_multiple_measurements].index.tolist()\nfiltered_data = patient_records[patient_records['PatientID'].isin(selected_patients)]\n\ndef get_extreme_values(group):\n    extreme_values = {}\n    for param in lab_parameters:\n        if param in group.columns:\n            param_data = group[param].dropna()\n            if len(param_data) > 1:\n                extreme_values[f'{param}_peak'] = param_data.max()\n                extreme_values[f'{param}_nadir'] = param_data.min()\n            else:\n                extreme_values[f'{param}_peak'] = np.nan\n                extreme_values[f'{param}_nadir'] = np.nan\n    return pd.Series(extreme_values)\n\nextreme_values_df = filtered_data.groupby('PatientID').apply(get_extreme_values).reset_index()\nmerged_df = pd.merge(extreme_values_df, data[['PatientID', 'Outcome', 'LOS']].drop_duplicates(), on='PatientID', how='left')\n\noutcome_analysis_results = []\nlos_correlation_results = []\n\nfor param in lab_parameters:\n    peak_col = f'{param}_peak'\n    nadir_col = f'{param}_nadir'\n    \n    if peak_col in merged_df.columns:\n        merged_df_clean = merged_df[[peak_col, nadir_col, 'Outcome', 'LOS']].dropna()\n        \n        if len(merged_df_clean) > 0:\n            if merged_df_clean['Outcome'].nunique() > 1:\n                peak_outcome_p = stats.mannwhitneyu(\n                    merged_df_clean[merged_df_clean['Outcome'] == 0][peak_col],\n                    merged_df_clean[merged_df_clean['Outcome'] == 1][peak_col]\n                ).pvalue\n                nadir_outcome_p = stats.mannwhitneyu(\n                    merged_df_clean[merged_df_clean['Outcome'] == 0][nadir_col],\n                    merged_df_clean[merged_df_clean['Outcome'] == 1][nadir_col]\n                ).pvalue\n                \n                outcome_analysis_results.append({\n                    'Parameter': param,\n                    'Peak_p_value': peak_outcome_p,\n                    'Nadir_p_value': nadir_outcome_p\n                })\n            \n            peak_los_corr, peak_los_p = stats.pearsonr(merged_df_clean[peak_col], merged_df_clean['LOS'])\n            nadir_los_corr, nadir_los_p = stats.pearsonr(merged_df_clean[nadir_col], merged_df_clean['LOS'])\n            \n            los_correlation_results.append({\n                'Parameter': param,\n                'Peak_correlation': peak_los_corr,\n                'Peak_p_value': peak_los_p,\n                'Nadir_correlation': nadir_los_corr,\n                'Nadir_p_value': nadir_los_p\n            })\n\noutcome_results_df = pd.DataFrame(outcome_analysis_results)\nlos_results_df = pd.DataFrame(los_correlation_results)\n\nsignificant_outcome_params = outcome_results_df[\n    (outcome_results_df['Peak_p_value'] < 0.05) | \n    (outcome_results_df['Nadir_p_value'] < 0.05)\n]\n\nsignificant_los_params = los_results_df[\n    (los_results_df['Peak_p_value'] < 0.05) | \n    (los_results_df['Nadir_p_value'] < 0.05)\n]\n\nplt.figure(figsize=(12, 8))\nsns.boxplot(x='Outcome', y='Hypersensitive cardiac troponinI_peak', data=merged_df)\nplt.title('Peak Hypersensitive cardiac troponinI by Outcome')\nplt.savefig(os.path.join(output_dir, 'troponin_outcome.png'))\nplt.close()\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='Hypersensitive cardiac troponinI_peak', y='LOS', data=merged_df)\nplt.title('Peak Hypersensitive cardiac troponinI vs LOS')\nplt.savefig(os.path.join(output_dir, 'troponin_los.png'))\nplt.close()\n\nreport = f\"\"\"\nAnalysis Report:\n\n1. Selected Parameters: {len(lab_parameters)} clinically relevant laboratory parameters\n\n2. Extreme Value Extraction Methodology:\n   - For each patient, identified peak (maximum) and nadir (minimum) values for each parameter\n   - Only considered patients with multiple measurements for at least one parameter\n   - Parameters with only one measurement were excluded from analysis\n\n3. Significant Associations with Outcome:\n{significant_outcome_params.to_string()}\n\n4. Significant Associations with Length of Stay:\n{significant_los_params.to_string()}\n\n5. Clinical Interpretation:\n   - Parameters showing significant associations may be important biomarkers for patient outcomes\n   - Peak values often more strongly associated with outcomes than nadir values\n   - Inflammatory markers and cardiac biomarkers frequently show strong associations\n\"\"\"\n\nwith open(os.path.join(output_dir, 'analysis_report.txt'), 'w') as f:\n    f.write(report)\n\nsignificant_outcome_params.to_csv(os.path.join(output_dir, 'significant_outcome_params.csv'))\nsignificant_los_params.to_csv(os.path.join(output_dir, 'significant_los_params.csv'))\nmerged_df.to_csv(os.path.join(output_dir, 'patient_extreme_values.csv'))"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to investigate the relationship between the *proportion of records with values outside statistically defined 'typical' ranges* for key laboratory parameters during hospitalization and patient outcomes (Outcome and Length of Stay). This task requires: 1. Selecting a subset of clinically relevant laboratory parameters that are measured multiple times for at least some patients. 2. For each selected parameter, define 'typical' value ranges statistically. Clearly state the statistical method used for defining these ranges. 3. For each patient, using only records with a RecordTime between their AdmissionTime and DischargeTime, count the number of records where the value for a selected parameter falls outside the defined 'typical' range. 4. Calculate the total number of records for that parameter within the hospitalization period for each patient. 5. For each patient and selected parameter, calculate the *proportion* of records with values outside the 'typical' range (count of atypical records / total count of records for that parameter during hospitalization). Address patients with insufficient measurements for a given parameter. 6. Analyze the association between these calculated 'proportion of atypical records' metrics and the patient's 'Outcome' (binary) and 'LOS' (continuous). This involves comparing the distribution of the 'proportion atypical' metric between Outcome groups using statistical tests and visualizations, and assessing correlation with LOS. 7. Generate a report summarizing the chosen parameters, the methodology for defining 'typical' ranges and calculating the 'proportion atypical', the results of the association analysis with Outcome and LOS highlighting parameters with significant findings, including relevant descriptive statistics, test results, and clinical interpretations.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 33,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Ensure output directory exists\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/33/\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert time columns to datetime\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\n\n# Select clinically relevant lab parameters measured multiple times for some patients\nlab_params = [\n    'hemoglobin', 'Serum chloride', 'Prothrombin time', 'procalcitonin', \n    'eosinophils(%)', 'Alkaline phosphatase', 'albumin', 'basophil(%)', \n    'Total bilirubin', 'Platelet count', 'monocytes(%)', 'indirect bilirubin', \n    'Red blood cell distribution width', 'neutrophils(%)', 'total protein', \n    'mean corpuscular volume', 'hematocrit', 'White blood cell count', \n    'mean corpuscular hemoglobin concentration', 'Urea', 'lymphocyte count', \n    'Red blood cell count', 'Corrected calcium', 'Serum potassium', 'glucose', \n    'neutrophils count', 'Direct bilirubin', 'ferritin', 'D-D dimer', \n    'Total cholesterol', 'aspartate aminotransferase', 'Uric acid', 'HCO3-', \n    'calcium', 'Lactate dehydrogenase', 'globulin', 'γ-glutamyl transpeptidase', \n    'Hypersensitive c-reactive protein', 'serum sodium', 'ESR', \n    'glutamic-pyruvic transaminase', 'eGFR', 'creatinine'\n]\n\n# Filter parameters that exist in dataset and have multiple measurements\nlab_params = [p for p in lab_params if p in df.columns]\nparam_counts = df[['PatientID'] + lab_params].groupby('PatientID').count().mean()\nselected_params = param_counts[param_counts > 1].index.tolist()\n\n# Define typical ranges using IQR method (Q1-1.5*IQR to Q3+1.5*IQR)\ntypical_ranges = {}\nfor param in selected_params:\n    values = df[param].dropna()\n    q1 = np.percentile(values, 25)\n    q3 = np.percentile(values, 75)\n    iqr = q3 - q1\n    lower = q1 - 1.5 * iqr\n    upper = q3 + 1.5 * iqr\n    typical_ranges[param] = (lower, upper)\n\n# Calculate proportion of atypical records for each patient and parameter\nresults = []\nfor patient_id, patient_data in df.groupby('PatientID'):\n    patient_data = patient_data[\n        (patient_data['RecordTime'] >= patient_data['AdmissionTime'].iloc[0]) & \n        (patient_data['RecordTime'] <= patient_data['DischargeTime'].iloc[0])\n    \n    for param in selected_params:\n        param_data = patient_data[param].dropna()\n        total_records = len(param_data)\n        if total_records == 0:\n            continue\n        \n        lower, upper = typical_ranges[param]\n        atypical_count = ((param_data < lower) | (param_data > upper)).sum()\n        prop_atypical = atypical_count / total_records\n        \n        results.append({\n            'PatientID': patient_id,\n            'Parameter': param,\n            'TotalRecords': total_records,\n            'AtypicalCount': atypical_count,\n            'ProportionAtypical': prop_atypical,\n            'Outcome': patient_data['Outcome'].iloc[0],\n            'LOS': patient_data['LOS'].iloc[0]\n        })\n\nresults_df = pd.DataFrame(results)\n\n# Save intermediate results\nresults_df.to_csv(os.path.join(output_dir, 'proportion_atypical_results.csv'), index=False)\n\n# Analyze association with Outcome (binary)\noutcome_analysis = []\nfor param in selected_params:\n    param_data = results_df[results_df['Parameter'] == param]\n    if len(param_data) < 2:\n        continue\n    \n    group0 = param_data[param_data['Outcome'] == 0]['ProportionAtypical']\n    group1 = param_data[param_data['Outcome'] == 1]['ProportionAtypical']\n    \n    if len(group0) > 1 and len(group1) > 1:\n        _, p_val = stats.mannwhitneyu(group0, group1, alternative='two-sided')\n        outcome_analysis.append({\n            'Parameter': param,\n            'Median_Proportion_Outcome0': group0.median(),\n            'Median_Proportion_Outcome1': group1.median(),\n            'MannWhitneyU_pvalue': p_val\n        })\n\noutcome_df = pd.DataFrame(outcome_analysis)\noutcome_df.to_csv(os.path.join(output_dir, 'outcome_analysis.csv'), index=False)\n\n# Analyze association with LOS (continuous)\nlos_analysis = []\nfor param in selected_params:\n    param_data = results_df[results_df['Parameter'] == param]\n    if len(param_data) < 2:\n        continue\n    \n    r, p_val = stats.pearsonr(param_data['ProportionAtypical'], param_data['LOS'])\n    los_analysis.append({\n        'Parameter': param,\n        'Pearson_r': r,\n        'Pearson_pvalue': p_val\n    })\n\nlos_df = pd.DataFrame(los_analysis)\nlos_df.to_csv(os.path.join(output_dir, 'los_analysis.csv'), index=False)\n\n# Generate visualizations\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=results_df, x='Parameter', y='ProportionAtypical', hue='Outcome')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, 'proportion_by_outcome.png'))\nplt.close()\n\nplt.figure(figsize=(12, 6))\nfor param in selected_params[:10]:  # Plot first 10 for clarity\n    param_data = results_df[results_df['Parameter'] == param]\n    sns.regplot(data=param_data, x='ProportionAtypical', y='LOS', label=param)\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, 'proportion_vs_los.png'))\nplt.close()\n\n# Generate report\nwith open(os.path.join(output_dir, 'analysis_report.txt'), 'w') as f:\n    f.write(\"Laboratory Parameter Analysis Report\\n\")\n    f.write(\"===================================\\n\\n\")\n    \n    f.write(\"Selected Parameters:\\n\")\n    f.write(\", \".join(selected_params) + \"\\n\\n\")\n    \n    f.write(\"Typical Range Definition:\\n\")\n    f.write(\"Used IQR method (Q1-1.5*IQR to Q3+1.5*IQR) for each parameter\\n\\n\")\n    \n    f.write(\"Significant Associations with Outcome (p < 0.05):\\n\")\n    sig_outcome = outcome_df[outcome_df['MannWhitneyU_pvalue'] < 0.05]\n    if len(sig_outcome) > 0:\n        f.write(sig_outcome.to_string(index=False))\n    else:\n        f.write(\"No significant associations found\\n\")\n    f.write(\"\\n\\n\")\n    \n    f.write(\"Significant Correlations with LOS (p < 0.05):\\n\")\n    sig_los = los_df[los_df['Pearson_pvalue'] < 0.05]\n    if len(sig_los) > 0:\n        f.write(sig_los.to_string(index=False))\n    else:\n        f.write(\"No significant correlations found\\n\")"
    },
    {
        "task": "Analyze the relationship between patient outcomes ('Outcome') and their laboratory parameter values recorded around a specific time point during hospitalization. The task requires: 1. Select a target day relative to admission. 2. For each patient, identify the record whose 'RecordTime' is closest to this target day. Define a reasonable time window around the target day within which to search for the closest record. 3. For a selected subset of clinically relevant laboratory parameters, extract the values from the identified 'closest' record for each patient. Address missing values for parameters that were not measured on the identified record. 4. Compare the distributions and statistical properties of these extracted parameter values on the target day between the two Outcome groups (0 vs 1) using appropriate statistical tests and visualizations. Identify parameters with statistically significant differences at a chosen significance level. 5. Generate a report summarizing the chosen target day and search window, the list of parameters analyzed, the methodology for selecting records and handling missing data, the results of the comparative analysis highlighting parameters whose values around the target day are significantly associated with outcome, including relevant descriptive statistics, test results, and interpretations of the findings in a clinical context.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 34,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom pathlib import Path\n\n# Load dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Convert time columns to datetime\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\n\n# Define target day and window\ntarget_day = 3\nwindow_days = 2\n\n# Calculate target time window for each patient\ndf['TargetStart'] = df['AdmissionTime'] + timedelta(days=target_day - window_days)\ndf['TargetEnd'] = df['AdmissionTime'] + timedelta(days=target_day + window_days)\n\n# Filter records within the window\nwindow_records = df[(df['RecordTime'] >= df['TargetStart']) & (df['RecordTime'] <= df['TargetEnd'])]\n\n# Find closest record to target day for each patient\nclosest_records = window_records.groupby('PatientID').apply(\n    lambda x: x.iloc[(x['RecordTime'] - (x['AdmissionTime'] + timedelta(days=target_day))).abs().argsort()[0]]\n).reset_index(drop=True)\n\n# Select clinically relevant parameters\nclinical_params = [\n    'hemoglobin', 'White blood cell count', 'Platelet count', 'albumin', \n    'creatinine', 'glucose', 'Total bilirubin', 'aspartate aminotransferase',\n    'Hypersensitive c-reactive protein', 'D-D dimer'\n]\n\n# Extract parameter values from closest records\nresult_df = closest_records[['PatientID', 'Outcome'] + clinical_params].copy()\n\n# Handle missing values\nfor param in clinical_params:\n    result_df[param] = pd.to_numeric(result_df[param], errors='coerce')\n\n# Create output directory\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/34/\"\nPath(output_dir).mkdir(parents=True, exist_ok=True)\n\n# Compare distributions between outcome groups\nreport_lines = []\nreport_lines.append(f\"Analysis Report for Day {target_day} ± {window_days} days\")\nreport_lines.append(f\"Parameters analyzed: {', '.join(clinical_params)}\")\nreport_lines.append(\"\\nMissing value handling: Missing values were set to NaN\")\n\n# Descriptive statistics\noutcome_0_stats = result_df[result_df['Outcome'] == 0][clinical_params].describe()\noutcome_1_stats = result_df[result_df['Outcome'] == 1][clinical_params].describe()\n\noutcome_0_stats.to_csv(os.path.join(output_dir, 'outcome_0_stats.csv'))\noutcome_1_stats.to_csv(os.path.join(output_dir, 'outcome_1_stats.csv'))\n\n# Statistical tests and visualizations\nsignificant_params = []\nalpha = 0.05\n\nfor param in clinical_params:\n    # Remove missing values\n    group0 = result_df[result_df['Outcome'] == 0][param].dropna()\n    group1 = result_df[result_df['Outcome'] == 1][param].dropna()\n    \n    if len(group0) > 0 and len(group1) > 0:\n        # Normality test\n        _, p0 = stats.shapiro(group0)\n        _, p1 = stats.shapiro(group1)\n        \n        if p0 > alpha and p1 > alpha:\n            # T-test for normally distributed data\n            _, p_val = stats.ttest_ind(group0, group1)\n            test_used = \"Student's t-test\"\n        else:\n            # Mann-Whitney U test for non-normal data\n            _, p_val = stats.mannwhitneyu(group0, group1)\n            test_used = \"Mann-Whitney U test\"\n        \n        # Save results\n        if p_val < alpha:\n            significant_params.append(param)\n        \n        # Create visualization\n        plt.figure(figsize=(8, 6))\n        sns.boxplot(x='Outcome', y=param, data=result_df)\n        plt.title(f'Distribution of {param} by Outcome Group')\n        plt.savefig(os.path.join(output_dir, f'{param}_distribution.png'))\n        plt.close()\n        \n        # Add to report\n        report_lines.append(f\"\\nParameter: {param}\")\n        report_lines.append(f\"Test used: {test_used}\")\n        report_lines.append(f\"p-value: {p_val:.4f}\")\n        report_lines.append(f\"Significant at alpha={alpha}: {'Yes' if p_val < alpha else 'No'}\")\n\n# Summary of significant findings\nreport_lines.append(\"\\n\\nSummary of Significant Findings:\")\nif significant_params:\n    report_lines.append(f\"The following parameters showed statistically significant differences between outcome groups on day {target_day}:\")\n    for param in significant_params:\n        report_lines.append(f\"- {param}\")\nelse:\n    report_lines.append(f\"No parameters showed statistically significant differences between outcome groups on day {target_day}\")\n\n# Save report\nwith open(os.path.join(output_dir, 'analysis_report.txt'), 'w') as f:\n    f.write(\"\\n\".join(report_lines))"
    },
    {
        "task": "Analyze the provided longitudinal patient dataset to identify frequently co-occurring laboratory tests within individual patient records and investigate if the prevalence of these co-occurrence patterns differs significantly between patients with different outcomes ('Outcome' 0 vs. 1). The task requires: 1. Grouping the data by 'PatientID' and 'RecordTime' to identify the set of laboratory parameters measured at each specific point in time for each patient. 2. Identify frequent sets of laboratory parameters that are measured together across all records in the dataset, using techniques such as frequent itemset mining with a defined minimum support threshold. 3. For the most frequent co-occurring sets of tests, calculate the proportion of records where this set appears, separately for records belonging to patients with 'Outcome' 0 and records belonging to patients with 'Outcome' 1. 4. Perform a statistical test to determine if the difference in proportions for these frequent co-occurring sets between the two outcome groups is statistically significant. 5. Generate a report summarizing the identified frequent co-occurrence patterns, the methods used for frequency analysis and statistical comparison, the results highlighting patterns with statistically significant differences in prevalence between outcome groups, and discuss potential clinical reasons or implications for these findings.",
        "task_type": "Reporting",
        "dataset": "TJH",
        "metadata": "report",
        "ID": 35,
        "code": "import pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori\nfrom scipy.stats import chi2_contingency\nimport os\n\ndef analyze_co_occurrence():\n    df = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n    lab_columns = [col for col in df.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']]\n    \n    grouped = df.groupby(['PatientID', 'RecordTime'])\n    transactions = []\n    for name, group in grouped:\n        measured_tests = [test for test in lab_columns if not pd.isna(group[test].iloc[0])]\n        transactions.append(measured_tests)\n    \n    te = TransactionEncoder()\n    te_ary = te.fit(transactions).transform(transactions)\n    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n    \n    frequent_itemsets = apriori(df_encoded, min_support=0.1, use_colnames=True)\n    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n    frequent_itemsets = frequent_itemsets[frequent_itemsets['length'] >= 2].sort_values(by='support', ascending=False)\n    \n    outcome_0 = df[df['Outcome'] == 0]\n    outcome_1 = df[df['Outcome'] == 1]\n    \n    results = []\n    for _, row in frequent_itemsets.head(10).iterrows():\n        itemset = list(row['itemsets'])\n        support_0 = len(outcome_0[outcome_0[list(itemset)].notna().all(axis=1)]) / len(outcome_0)\n        support_1 = len(outcome_1[outcome_1[list(itemset)].notna().all(axis=1)]) / len(outcome_1)\n        \n        contingency_table = [\n            [len(outcome_0[outcome_0[list(itemset)].notna().all(axis=1)]), len(outcome_0) - len(outcome_0[outcome_0[list(itemset)].notna().all(axis=1)])],\n            [len(outcome_1[outcome_1[list(itemset)].notna().all(axis=1)]), len(outcome_1) - len(outcome_1[outcome_1[list(itemset)].notna().all(axis=1)])]\n        ]\n        chi2, p, _, _ = chi2_contingency(contingency_table)\n        \n        results.append({\n            'itemset': itemset,\n            'support_overall': row['support'],\n            'support_outcome_0': support_0,\n            'support_outcome_1': support_1,\n            'p_value': p\n        })\n    \n    results_df = pd.DataFrame(results)\n    \n    output_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/35/\"\n    os.makedirs(output_dir, exist_ok=True)\n    results_df.to_csv(os.path.join(output_dir, 'co_occurrence_results.csv'), index=False)\n    frequent_itemsets.to_csv(os.path.join(output_dir, 'frequent_itemsets.csv'), index=False)\n\nanalyze_co_occurrence()"
    },
    {
        "task": "Build a predictive model to determine the final outcome ('Outcome') for each patient. Your model should utilize the patient's static attributes (Sex, Age) and process the sequential clinical measurements recorded at different time points ('RecordTime') during their hospitalization ('AdmissionTime' to 'DischargeTime'). The task is a binary classification problem to predict whether 'Outcome' is 0 or 1 based on the available data up to a certain point in time.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 36,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\nimport joblib\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\n\ndef preprocess_data(df):\n    df = df.copy()\n    df['Sex'] = df['Sex'].map({'M': 1, 'F': 0})\n    df = df.drop(['PatientID', 'AdmissionTime', 'DischargeTime', 'RecordTime'], axis=1)\n    df = df.fillna(df.mean())\n    return df\n\ndata_processed = preprocess_data(data)\n\nX = data_processed.drop('Outcome', axis=1)\ny = data_processed['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/36/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"accuracy.txt\"), \"w\") as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n\nwith open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n    f.write(report)\n\njoblib.dump(model, os.path.join(output_dir, \"model.joblib\"))"
    },
    {
        "task": "Predict the total Length of Stay ('LOS') for each patient upon their admission. Your model should utilize the patient's static attributes (Sex, Age) and the clinical measurements recorded on their admission day ('RecordTime' == 'AdmissionTime'). If multiple records exist on the admission day, you can use the first record or an aggregate of records from that day. This task is a regression problem to predict the continuous value of 'LOS'.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 37,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\n\n# Load dataset\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Filter records where RecordTime == AdmissionTime\nadmission_data = data[data['RecordTime'] == data['AdmissionTime']]\n\n# If multiple records exist on admission day, use the first record\nadmission_data = admission_data.groupby('PatientID').first().reset_index()\n\n# Select features and target\nfeatures = ['Sex', 'Age'] + [col for col in data.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS']]\nX = admission_data[features]\ny = admission_data['LOS']\n\n# Convert categorical 'Sex' to numerical\nX['Sex'] = X['Sex'].map({'M': 0, 'F': 1})\n\n# Handle missing values\nX = X.fillna(X.mean())\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = model.predict(X_test)\n\n# Calculate metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Save results\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/37/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"metrics.txt\"), \"w\") as f:\n    f.write(f\"MSE: {mse}\\nR2: {r2}\")\n\n# Save predictions\npredictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\npredictions.to_csv(os.path.join(output_dir, \"predictions.csv\"), index=False)"
    },
    {
        "task": "Build a time-series regression model to predict the value of 'Hypersensitive c-reactive protein' for a patient at their next recorded time point ('RecordTime'). Your model should utilize the patient's historical clinical measurements recorded at previous time points, along with their static attributes ('Sex', 'Age'). The task is to predict the continuous value of 'Hypersensitive c-reactive protein' based on the available sequence of data for that patient up to the prediction point.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 38,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport os\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata = data.sort_values(['PatientID', 'RecordTime'])\n\nstatic_features = ['Sex', 'Age']\ndynamic_features = [col for col in data.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS'] + static_features]\ndynamic_features.remove('Hypersensitive c-reactive protein')\n\ndef create_sequences(data, patient_ids, n_steps=3):\n    X, y = [], []\n    for pid in patient_ids:\n        patient_data = data[data['PatientID'] == pid].sort_values('RecordTime')\n        patient_dynamic = patient_data[dynamic_features].values\n        patient_static = patient_data[static_features].iloc[0].values\n        target = patient_data['Hypersensitive c-reactive protein'].values\n        for i in range(n_steps, len(patient_data)):\n            X_seq = patient_dynamic[i-n_steps:i]\n            X_static = patient_static\n            X.append((X_seq, X_static))\n            y.append(target[i])\n    return np.array(X, dtype=object), np.array(y)\n\npatient_ids = data['PatientID'].unique()\nX, y = create_sequences(data, patient_ids)\n\nX_dynamic = np.array([x[0] for x in X])\nX_static = np.array([x[1] for x in X])\n\nscaler_dynamic = StandardScaler()\nscaler_dynamic.fit(X_dynamic.reshape(-1, X_dynamic.shape[-1]))\nX_dynamic_scaled = scaler_dynamic.transform(X_dynamic.reshape(-1, X_dynamic.shape[-1])).reshape(X_dynamic.shape)\n\nscaler_static = StandardScaler()\nX_static_scaled = scaler_static.fit_transform(X_static)\n\nscaler_target = StandardScaler()\ny_scaled = scaler_target.fit_transform(y.reshape(-1, 1))\n\nX_train_d, X_test_d, X_train_s, X_test_s, y_train, y_test = train_test_split(\n    X_dynamic_scaled, X_static_scaled, y_scaled, test_size=0.2, random_state=42)\n\ndef create_model(dynamic_input_shape, static_input_shape):\n    model = Sequential()\n    model.add(LSTM(64, return_sequences=True, input_shape=(dynamic_input_shape[1], dynamic_input_shape[2])))\n    model.add(Dropout(0.2))\n    model.add(LSTM(32))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    \n    static_input = Input(shape=(static_input_shape[1],))\n    merged = concatenate([model.output, static_input])\n    \n    dense = Dense(16, activation='relu')(merged)\n    output = Dense(1)(dense)\n    \n    final_model = Model(inputs=[model.input, static_input], outputs=output)\n    final_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return final_model\n\nmodel = create_model(X_train_d.shape, X_train_s.shape)\nhistory = model.fit([X_train_d, X_train_s], y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\ny_pred = model.predict([X_test_d, X_test_s])\ny_pred_inverse = scaler_target.inverse_transform(y_pred)\ny_test_inverse = scaler_target.inverse_transform(y_test)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/38/\"\nos.makedirs(output_dir, exist_ok=True)\n\nnp.save(os.path.join(output_dir, 'y_pred.npy'), y_pred_inverse)\nnp.save(os.path.join(output_dir, 'y_test.npy'), y_test_inverse)\nmodel.save(os.path.join(output_dir, 'model.h5'))"
    },
    {
        "task": "Predict the likelihood of a patient having at least one critical laboratory value outside its normal clinical range at their next recorded time point. The model should utilize the patient's static attributes (Sex, Age) and their sequence of clinical measurements recorded up to the current time ('RecordTime'). This is a binary classification problem where the target is 1 if any of a predefined set of critical lab values falls outside its established normal clinical range at the patient's subsequent 'RecordTime', and 0 otherwise. The specific critical lab values and their corresponding normal ranges should be defined based on clinical standards or dataset characteristics.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 39,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ncritical_labs = {\n    'Hypersensitive cardiac troponinI': (0, 0.04),\n    'hemoglobin': (13.5, 17.5),\n    'Serum chloride': (96, 106),\n    'Prothrombin time': (11, 13.5),\n    'procalcitonin': (0, 0.05),\n    'Interleukin 2 receptor': (0, 1033),\n    'Alkaline phosphatase': (44, 147),\n    'albumin': (3.4, 5.4),\n    'Interleukin 10': (0, 9.1),\n    'Total bilirubin': (0.1, 1.2),\n    'Platelet count': (150, 450),\n    'Interleukin 8': (0, 62),\n    'Red blood cell distribution width ': (11.6, 14.6),\n    'total protein': (6, 8.3),\n    'mean corpuscular volume': (80, 100),\n    'hematocrit': (38.3, 48.6),\n    'White blood cell count': (4.5, 11),\n    'Tumor necrosis factorα': (0, 8.1),\n    'mean corpuscular hemoglobin concentration': (32, 36),\n    'fibrinogen': (2, 4),\n    'Interleukin 1β': (0, 5),\n    'Urea': (2.5, 7.1),\n    'Corrected calcium': (8.5, 10.2),\n    'Serum potassium': (3.5, 5.1),\n    'glucose': (70, 100),\n    'Direct bilirubin': (0, 0.3),\n    'ferritin': (20, 500),\n    'D-D dimer': (0, 0.5),\n    'Total cholesterol': (0, 200),\n    'aspartate aminotransferase': (0, 40),\n    'Uric acid': (3.4, 7),\n    'HCO3-': (22, 29),\n    'calcium': (8.5, 10.2),\n    'Lactate dehydrogenase': (140, 280),\n    'Interleukin 6': (0, 7),\n    'globulin': (2, 3.5),\n    'γ-glutamyl transpeptidase': (8, 61),\n    'Hypersensitive c-reactive protein': (0, 10),\n    'serum sodium': (135, 145),\n    'ESR': (0, 20),\n    'glutamic-pyruvic transaminase': (0, 55),\n    'eGFR': (90, 120),\n    'creatinine': (0.6, 1.2)\n}\n\ndef is_critical(row):\n    for lab, (low, high) in critical_labs.items():\n        if lab in row and not pd.isna(row[lab]) and (row[lab] < low or row[lab] > high):\n            return 1\n    return 0\n\ndef prepare_data(df):\n    df['RecordTime'] = pd.to_datetime(df['RecordTime'])\n    df = df.sort_values(['PatientID', 'RecordTime'])\n    df['target'] = df.groupby('PatientID').apply(lambda x: x.shift(-1)).apply(is_critical, axis=1)\n    df = df.dropna(subset=['target'])\n    df['target'] = df['target'].astype(int)\n    static_features = ['Sex', 'Age']\n    dynamic_features = [col for col in df.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'target'] + static_features]\n    df_agg = df.groupby('PatientID').agg({\n        **{feat: ['mean', 'std', 'last'] for feat in dynamic_features},\n        **{feat: 'last' for feat in static_features},\n        'target': 'last'\n    })\n    df_agg.columns = ['_'.join(col).strip() for col in df_agg.columns.values]\n    return df_agg\n\ndef main():\n    df = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n    df_prepared = prepare_data(df)\n    X = df_prepared.drop('target_last', axis=1)\n    y = df_prepared['target_last']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n    output_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/39/\"\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, \"results.txt\"), \"w\") as f:\n        f.write(f\"Accuracy: {accuracy}\\n\")\n        f.write(report)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
        "task": "Build a regression model to predict the remaining Length of Stay ('DischargeTime' - 'RecordTime' in days) for a patient at any given record time ('RecordTime'). Your model should leverage the patient's static attributes (Sex, Age) and the sequence of clinical measurements recorded up to that specific 'RecordTime'.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 40,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\ndf['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\ndf['LOS'] = (df['DischargeTime'] - df['RecordTime']).dt.total_seconds() / (24 * 3600)\n\nstatic_features = ['Sex', 'Age']\ndynamic_features = [col for col in df.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), dynamic_features),\n        ('cat', OneHotEncoder(), static_features)\n    ])\n\nX = df[static_features + dynamic_features]\ny = df['LOS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', RandomForestRegressor(random_state=42))\n])\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/40/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"metrics.txt\"), \"w\") as f:\n    f.write(f\"MAE: {mae}\\n\")\n    f.write(f\"MSE: {mse}\\n\")\n    f.write(f\"RMSE: {rmse}\\n\")\n\nfeature_importances = model.named_steps['regressor'].feature_importances_\nfeature_names = model.named_steps['preprocessor'].get_feature_names_out()\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\nimportance_df.to_csv(os.path.join(output_dir, \"feature_importances.csv\"), index=False)"
    },
    {
        "task": "Build a regression model to predict the maximum value of 'Hypersensitive c-reactive protein' recorded for a patient throughout their entire hospital stay. Your model should utilize the patient's static attributes (Sex, Age) and all available clinical measurements recorded during the initial 72 hours from their 'AdmissionTime'. Handle patients without 'Hypersensitive c-reactive protein' measurements within the first 72 hours appropriately.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 41,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\n\ndata['TimeDiff'] = (data['RecordTime'] - data['AdmissionTime']).dt.total_seconds() / 3600\nfirst_72_hours = data[data['TimeDiff'] <= 72]\n\nstatic_features = ['PatientID', 'Sex', 'Age']\nclinical_features = [col for col in data.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age', 'Hypersensitive c-reactive protein', 'TimeDiff']]\nall_features = static_features + clinical_features\n\nmax_crp_per_patient = first_72_hours.groupby('PatientID')['Hypersensitive c-reactive protein'].max().reset_index()\nmerged_data = pd.merge(max_crp_per_patient, first_72_hours[all_features].groupby('PatientID').first().reset_index(), on='PatientID', how='left')\n\nmerged_data = merged_data.dropna(subset=['Hypersensitive c-reactive protein'])\nmerged_data = pd.get_dummies(merged_data, columns=['Sex'])\n\nX = merged_data.drop(['PatientID', 'Hypersensitive c-reactive protein'], axis=1, errors='ignore')\ny = merged_data['Hypersensitive c-reactive protein']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/41/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"results.txt\"), \"w\") as f:\n    f.write(f\"MSE: {mse}\\n\")\n    f.write(f\"R2 Score: {r2}\\n\")\n\nfeature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\nfeature_importances.to_csv(os.path.join(output_dir, \"feature_importances.csv\"), index=False)"
    },
    {
        "task": "Build a binary classification model to predict, for patients whose hospital stay ('LOS') is greater than 7 days, whether their 'White blood cell count' will decrease below 10.0 at any time point after the first 7 days of hospitalization. Your model should utilize the patient's static attributes (Sex, Age) and the time-series of all available clinical measurements recorded within the first 7 days from their 'AdmissionTime'.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 42,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\nimport json\n\n# Load dataset\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Convert time columns to datetime\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\n# Filter patients with LOS > 7 days\ndata['LOS'] = (data['DischargeTime'] - data['AdmissionTime']).dt.total_seconds() / (24 * 3600)\nfiltered_data = data[data['LOS'] > 7].copy()\n\n# Process each patient\npatients = []\nfor pid in filtered_data['PatientID'].unique():\n    patient_data = filtered_data[filtered_data['PatientID'] == pid].copy()\n    first_7_days = patient_data[patient_data['RecordTime'] <= (patient_data['AdmissionTime'].iloc[0] + pd.Timedelta(days=7))]\n    after_7_days = patient_data[patient_data['RecordTime'] > (patient_data['AdmissionTime'].iloc[0] + pd.Timedelta(days=7))]\n    \n    # Check if WBC count drops below 10.0 after first 7 days\n    target = 1 if any(after_7_days['White blood cell count'] < 10.0) else 0\n    \n    # Get static features\n    static_features = patient_data[['Sex', 'Age']].iloc[0].to_dict()\n    \n    # Get time-series features (mean of first 7 days)\n    ts_features = first_7_days.select_dtypes(include=[np.number]).mean().to_dict()\n    \n    # Combine features\n    features = {**static_features, **ts_features}\n    features['PatientID'] = pid\n    features['target'] = target\n    \n    patients.append(features)\n\n# Create DataFrame\nprocessed_data = pd.DataFrame(patients)\n\n# Handle missing values\nprocessed_data.fillna(processed_data.mean(), inplace=True)\n\n# Prepare features and target\nX = processed_data.drop(['PatientID', 'target'], axis=1)\ny = processed_data['target']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Evaluate\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred, output_dict=True)\n\n# Save results\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/42/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"accuracy.json\"), \"w\") as f:\n    json.dump({\"accuracy\": accuracy}, f)\n\nwith open(os.path.join(output_dir, \"classification_report.json\"), \"w\") as f:\n    json.dump(report, f)"
    },
    {
        "task": "Build a multiclass classification model to predict the trajectory pattern of 'Platelet count' over the remainder of a patient's hospital stay. The model should use the patient's static attributes (Sex, Age) and the sequence of clinical measurements recorded up to the current 'RecordTime'. The trajectory patterns ('Increasing', 'Decreasing', 'Stable') should be defined based on the relative change in 'Platelet count' from the measurement at the current 'RecordTime' to the final measurement recorded before 'DischargeTime'.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 43,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport os\n\ndef load_data(filepath):\n    return pd.read_csv(filepath)\n\ndef preprocess_data(df):\n    df['AdmissionTime'] = pd.to_datetime(df['AdmissionTime'])\n    df['DischargeTime'] = pd.to_datetime(df['DischargeTime'])\n    df['RecordTime'] = pd.to_datetime(df['RecordTime'])\n    df['Sex'] = df['Sex'].map({'M': 1, 'F': 0})\n    df = df.sort_values(['PatientID', 'RecordTime'])\n    return df\n\ndef determine_trajectory(group):\n    if len(group) < 2:\n        return None\n    current_platelet = group.iloc[0]['Platelet count']\n    final_platelet = group.iloc[-1]['Platelet count']\n    change = (final_platelet - current_platelet) / current_platelet * 100\n    if change > 10:\n        return 'Increasing'\n    elif change < -10:\n        return 'Decreasing'\n    else:\n        return 'Stable'\n\ndef prepare_dataset(df):\n    trajectories = df.groupby('PatientID').apply(lambda x: determine_trajectory(x)).reset_index()\n    trajectories.columns = ['PatientID', 'Trajectory']\n    df = df.merge(trajectories, on='PatientID', how='left')\n    df = df.dropna(subset=['Trajectory'])\n    features = ['Sex', 'Age', 'Platelet count', 'hemoglobin', 'White blood cell count', 'Red blood cell count']\n    df = df.dropna(subset=features)\n    X = df[features]\n    y = df['Trajectory']\n    return X, y\n\ndef train_model(X_train, y_train):\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train_scaled, y_train)\n    return model, scaler\n\ndef evaluate_model(model, scaler, X_test, y_test):\n    X_test_scaled = scaler.transform(X_test)\n    y_pred = model.predict(X_test_scaled)\n    report = classification_report(y_test, y_pred)\n    return report\n\ndef save_results(report, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n        f.write(report)\n\ndef main():\n    filepath = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\"\n    output_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/43/\"\n    df = load_data(filepath)\n    df = preprocess_data(df)\n    X, y = prepare_dataset(df)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model, scaler = train_model(X_train, y_train)\n    report = evaluate_model(model, scaler, X_test, y_test)\n    save_results(report, output_dir)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
        "task": "Build a binary classification model to predict, for a patient at any given record time ('RecordTime') prior to their discharge ('RecordTime' < 'DischargeTime'), whether they will be discharged within the next 7 days from that 'RecordTime'. The model should utilize the patient's static attributes (Sex, Age) and the sequence of all clinical measurements recorded for that patient up to and including that specific 'RecordTime'.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 44,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\nimport joblib\n\n# Load dataset\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Preprocess data\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\n# Filter records before discharge\ndata = data[data['RecordTime'] < data['DischargeTime']]\n\n# Calculate target: whether discharge happens within next 7 days\ndata['DaysToDischarge'] = (data['DischargeTime'] - data['RecordTime']).dt.days\ndata['Target'] = (data['DaysToDischarge'] <= 7).astype(int)\n\n# Group by PatientID and RecordTime to get all measurements up to that time\ngrouped = data.groupby(['PatientID', 'RecordTime']).last().reset_index()\n\n# Select features\nstatic_features = ['Sex', 'Age']\ndynamic_features = [col for col in data.columns if col not in ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'DaysToDischarge', 'Target']]\nfeatures = static_features + dynamic_features\n\n# Handle missing values\ngrouped[features] = grouped[features].fillna(grouped[features].median())\n\n# Split data\nX = grouped[features]\ny = grouped['Target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Evaluate\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\n# Save results\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/44/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"accuracy.txt\"), \"w\") as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n\nwith open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n    f.write(report)\n\njoblib.dump(model, os.path.join(output_dir, \"model.pkl\"))"
    },
    {
        "task": "Build a regression model to predict the maximum absolute difference between consecutive 'White blood cell count' measurements for a patient during their entire hospitalization ('AdmissionTime' to 'DischargeTime'). Your model should utilize the patient's static attributes (Sex, Age) and all available clinical measurements recorded within the first 48 hours from their 'AdmissionTime'. For patients with no 'White blood cell count' measurements or only one measurement across their entire stay, exclude them from the training/testing set. For patients with 'White blood cell count' measurements but none or only one within the first 48 hours, handle the missing early data appropriately.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 45,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport os\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\ndef process_patient(group):\n    wbc_values = group['White blood cell count'].dropna()\n    if len(wbc_values) < 2:\n        return None\n    max_diff = np.max(np.abs(np.diff(wbc_values)))\n    \n    first_48_hours = group[group['RecordTime'] <= (group['AdmissionTime'].iloc[0] + pd.Timedelta(hours=48))]\n    features = {\n        'PatientID': group['PatientID'].iloc[0],\n        'Sex': group['Sex'].iloc[0],\n        'Age': group['Age'].iloc[0],\n        'max_diff': max_diff\n    }\n    \n    numeric_cols = group.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if col not in ['PatientID', 'Sex', 'Age', 'White blood cell count', 'max_diff']:\n            col_values = first_48_hours[col].dropna()\n            if len(col_values) > 0:\n                features[f'{col}_mean'] = col_values.mean()\n                features[f'{col}_median'] = col_values.median()\n                features[f'{col}_std'] = col_values.std()\n            else:\n                features[f'{col}_mean'] = np.nan\n                features[f'{col}_median'] = np.nan\n                features[f'{col}_std'] = np.nan\n    \n    return pd.Series(features)\n\nprocessed_data = data.groupby('PatientID').apply(process_patient).reset_index(drop=True)\nprocessed_data = processed_data.dropna(subset=['max_diff'])\n\nX = processed_data.drop(columns=['PatientID', 'max_diff'])\ny = processed_data['max_diff']\n\nX = pd.get_dummies(X, columns=['Sex'])\nX = X.fillna(X.median())\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/45/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"results.txt\"), \"w\") as f:\n    f.write(f\"Mean Squared Error: {mse}\\n\")\n    f.write(\"Feature Importances:\\n\")\n    for feature, importance in zip(X.columns, model.feature_importances_):\n        f.write(f\"{feature}: {importance}\\n\")"
    },
    {
        "task": "Build a regression model to predict the value of 'Hypersensitive c-reactive protein' for a patient on the day of their discharge ('DischargeTime'). Your model should utilize the patient's static attributes (Sex, Age) and the sequence of all clinical measurements recorded for that patient up to their last available record time ('RecordTime'). Patients who do not have 'Hypersensitive c-reactive protein' recorded at their 'DischargeTime' will require the model to infer or predict this value. Patients without any 'Hypersensitive c-reactive protein' measurements throughout their stay could be handled separately.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 46,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\n\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\ndef prepare_data(df):\n    features = []\n    targets = []\n    for patient_id, group in df.groupby('PatientID'):\n        group = group.sort_values('RecordTime')\n        last_record_time = group['RecordTime'].max()\n        discharge_time = group['DischargeTime'].iloc[0]\n        if last_record_time > discharge_time:\n            continue\n        static_features = group[['Sex', 'Age']].iloc[0].values\n        dynamic_features = group.drop(columns=['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age', 'Hypersensitive c-reactive protein']).values\n        flattened_features = np.concatenate([static_features, dynamic_features.flatten()])\n        target = group[group['RecordTime'] == discharge_time]['Hypersensitive c-reactive protein'].values\n        if len(target) == 0:\n            continue\n        features.append(flattened_features)\n        targets.append(target[0])\n    return np.array(features), np.array(targets)\n\nX, y = prepare_data(data)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/46/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"results.txt\"), \"w\") as f:\n    f.write(f\"MSE: {mse}\\n\")\n    f.write(f\"R2 Score: {r2}\\n\")\n\npd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).to_csv(os.path.join(output_dir, \"predictions.csv\"), index=False)"
    },
    {
        "task": "Build a regression model to predict the rate of change of 'Creatinine' for a patient during the first 48 hours of their hospitalization. The rate of change should be calculated as the slope of a linear fit to all 'Creatinine' measurements recorded between `AdmissionTime` and `AdmissionTime` + 48 hours (inclusive). Your model should utilize the patient's static attributes (Sex, Age) and all available clinical measurements recorded within the first 24 hours from their `AdmissionTime` (inclusive). Exclude patients who do not have at least two 'Creatinine' measurements recorded within the first 48 hours.",
        "task_type": "Modeling",
        "dataset": "TJH",
        "metadata": "modeling",
        "ID": 47,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\nfrom scipy import stats\n\n# Load dataset\ndata = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Convert time columns to datetime\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['AdmissionTime'] = pd.to_datetime(data['AdmissionTime'])\ndata['DischargeTime'] = pd.to_datetime(data['DischargeTime'])\n\n# Initialize output directory\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/47/\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Process each patient\nresults = []\nfor patient_id, patient_data in data.groupby('PatientID'):\n    # Filter creatinine measurements within first 48 hours\n    mask = (patient_data['RecordTime'] >= patient_data['AdmissionTime'].iloc[0]) & \\\n           (patient_data['RecordTime'] <= patient_data['AdmissionTime'].iloc[0] + pd.Timedelta(hours=48))\n    creatinine_data = patient_data[mask]['creatinine'].dropna()\n    \n    # Skip if less than 2 measurements\n    if len(creatinine_data) < 2:\n        continue\n    \n    # Calculate rate of change (slope)\n    times = (patient_data[mask]['RecordTime'] - patient_data['AdmissionTime'].iloc[0]).dt.total_seconds().values / 3600\n    times = times[~patient_data[mask]['creatinine'].isna()]\n    slope, _, _, _, _ = stats.linregress(times, creatinine_data.values)\n    \n    # Get static features (Sex, Age)\n    static_features = patient_data[['Sex', 'Age']].iloc[0].to_dict()\n    \n    # Get dynamic features from first 24 hours\n    mask_24h = (patient_data['RecordTime'] >= patient_data['AdmissionTime'].iloc[0]) & \\\n               (patient_data['RecordTime'] <= patient_data['AdmissionTime'].iloc[0] + pd.Timedelta(hours=24))\n    dynamic_data = patient_data[mask_24h].drop(columns=['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'creatinine'])\n    dynamic_features = dynamic_data.select_dtypes(include=[np.number]).mean().to_dict()\n    \n    # Combine all features\n    features = {**static_features, **dynamic_features}\n    features['PatientID'] = patient_id\n    features['Slope'] = slope\n    results.append(features)\n\n# Create dataframe\nresult_df = pd.DataFrame(results).dropna()\n\n# Prepare data for modeling\nX = result_df.drop(columns=['PatientID', 'Slope'])\ny = result_df['Slope']\n\n# Convert categorical variables\nX = pd.get_dummies(X, columns=['Sex'], drop_first=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Save results\nmetrics = pd.DataFrame({'MSE': [mse], 'R2': [r2]})\nmetrics.to_csv(os.path.join(output_dir, 'metrics.csv'), index=False)\nresult_df.to_csv(os.path.join(output_dir, 'processed_data.csv'), index=False)"
    },
    {
        "task": "Load the provided patient record data. Structure the data into a single table where each row represents a patient's record at a specific time point. Ensure that all unique clinical measurement types (`hemoglobin`, `Serum chloride`, etc.) across all records become columns in the final table. Handle missing values that arise from patients not having certain measurements at certain times, as well as potentially missing patient-specific attributes like `Sex` or `Age` in some records. ",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_preprocessing",
        "ID": 48,
        "code": "import pandas as pd\nimport os\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/48/\"\nos.makedirs(output_dir, exist_ok=True)\n\nmeasurement_columns = [\n    'Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', 'Prothrombin time', \n    'procalcitonin', 'eosinophils(%)', 'Interleukin 2 receptor', 'Alkaline phosphatase', \n    'albumin', 'basophil(%)', 'Interleukin 10', 'Total bilirubin', 'Platelet count', \n    'monocytes(%)', 'antithrombin', 'Interleukin 8', 'indirect bilirubin', \n    'Red blood cell distribution width ', 'neutrophils(%)', 'total protein', \n    'Quantification of Treponema pallidum antibodies', 'Prothrombin activity', 'HBsAg', \n    'mean corpuscular volume', 'hematocrit', 'White blood cell count', 'Tumor necrosis factorα', \n    'mean corpuscular hemoglobin concentration', 'fibrinogen', 'Interleukin 1β', 'Urea', \n    'lymphocyte count', 'PH value', 'Red blood cell count', 'Eosinophil count', 'Corrected calcium', \n    'Serum potassium', 'glucose', 'neutrophils count', 'Direct bilirubin', 'Mean platelet volume', \n    'ferritin', 'RBC distribution width SD', 'Thrombin time', '(%)lymphocyte', \n    'HCV antibody quantification', 'D-D dimer', 'Total cholesterol', 'aspartate aminotransferase', \n    'Uric acid', 'HCO3-', 'calcium', 'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)', \n    'Lactate dehydrogenase', 'platelet large cell ratio ', 'Interleukin 6', 'Fibrin degradation products', \n    'monocytes count', 'PLT distribution width', 'globulin', 'γ-glutamyl transpeptidase', \n    'International standard ratio', 'basophil count(#)', 'mean corpuscular hemoglobin ', \n    'Activation of partial thromboplastin time', 'Hypersensitive c-reactive protein', \n    'HIV antibody quantification', 'serum sodium', 'thrombocytocrit', 'ESR', \n    'glutamic-pyruvic transaminase', 'eGFR', 'creatinine'\n]\n\npatient_attributes = ['PatientID', 'Sex', 'Age']\n\ndf_final = df[patient_attributes + ['RecordTime'] + measurement_columns]\n\ndf_final.to_csv(os.path.join(output_dir, \"structured_patient_records.csv\"), index=False)"
    },
    {
        "task": "Transform the longitudinal patient records into a single patient-level representation by aggregating time-varying clinical measurements. For each unique patient, retain static attributes like Sex, Age, Outcome, and Admission/Discharge times. For dynamic clinical measurements that appear across multiple records for a patient, calculate summary statistics. The final output should be a structured dataset where each row corresponds to a unique patient, containing their static features and the calculated aggregate values for all relevant clinical parameters. Handle potential missing values during aggregation by using appropriate strategies. ",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_preprocessing",
        "ID": 49,
        "code": "import pandas as pd\nimport numpy as np\nimport os\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/49/\", exist_ok=True)\n\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\nstatic_columns = ['PatientID', 'Sex', 'Age', 'Outcome', 'AdmissionTime', 'DischargeTime']\ndynamic_columns = [col for col in df.columns if col not in static_columns and col != 'RecordTime']\n\ndef aggregate_dynamic_metrics(group):\n    agg_dict = {}\n    for col in dynamic_columns:\n        if pd.api.types.is_numeric_dtype(group[col]):\n            agg_dict[f'{col}_mean'] = group[col].mean()\n            agg_dict[f'{col}_median'] = group[col].median()\n            agg_dict[f'{col}_min'] = group[col].min()\n            agg_dict[f'{col}_max'] = group[col].max()\n            agg_dict[f'{col}_std'] = group[col].std()\n            agg_dict[f'{col}_count'] = group[col].count()\n        else:\n            agg_dict[f'{col}_first'] = group[col].iloc[0] if not group[col].empty else np.nan\n    return pd.Series(agg_dict)\n\nstatic_df = df[static_columns].drop_duplicates('PatientID').set_index('PatientID')\ndynamic_agg = df.groupby('PatientID').apply(aggregate_dynamic_metrics)\n\nresult = pd.concat([static_df, dynamic_agg], axis=1).reset_index()\nresult.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/49/aggregated_patient_data.csv\", index=False)"
    },
    {
        "task": "Structure the patient records into longitudinal sequences for time-series analysis. Group the records by `PatientID` and order them chronologically using the `RecordTime`. Identify the complete set of unique clinical measurement features that appear across all patient records in the dataset (excluding static patient attributes like `PatientID`, `AdmissionTime`, `DischargeTime`, `Outcome`, `LOS`, `Sex`, and `Age`). For each patient, transform their ordered sequence of records into a sequence of feature vectors. Each vector in the sequence corresponds to a single time point (record) and contains values for all identified unique clinical measurement features. If a clinical feature was not measured at a specific `RecordTime` for a patient, represent its value in the corresponding feature vector using a placeholder for missing data. ",
        "task_type": "Data",
        "dataset": "TJH",
        "metadata": "data_preprocessing",
        "ID": 50,
        "code": "import pandas as pd\nimport os\n\n# Load the dataset\ndf = pd.read_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/tjh_dataset_formatted.csv\")\n\n# Define static attributes to exclude\nstatic_attributes = ['PatientID', 'AdmissionTime', 'DischargeTime', 'Outcome', 'LOS', 'Sex', 'Age']\n\n# Identify unique clinical measurement features\nclinical_features = [col for col in df.columns if col not in static_attributes and col != 'RecordTime']\n\n# Group by PatientID and sort by RecordTime\ngrouped = df.groupby('PatientID').apply(lambda x: x.sort_values('RecordTime')).reset_index(drop=True)\n\n# Initialize a dictionary to hold patient sequences\npatient_sequences = {}\n\nfor patient_id, group in grouped.groupby('PatientID'):\n    # Initialize list to hold feature vectors for this patient\n    sequence = []\n    for _, record in group.iterrows():\n        # Initialize feature vector with NaN for all features\n        feature_vector = {feature: None for feature in clinical_features}\n        # Update with available measurements\n        for feature in clinical_features:\n            if pd.notna(record[feature]):\n                feature_vector[feature] = record[feature]\n        sequence.append(feature_vector)\n    patient_sequences[patient_id] = sequence\n\n# Save the results\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/50/\"\nos.makedirs(output_dir, exist_ok=True)\n\nimport json\nwith open(os.path.join(output_dir, 'patient_sequences.json'), 'w') as f:\n    json.dump(patient_sequences, f)"
    },
    {
        "task": "Count the number of unique PatientIDs that have at least one record where the 'Glascow coma scale total' is less than 10.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_querying",
        "ID": 51,
        "code": "import pandas as pd\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nfiltered_patients = df[df['Glascow coma scale total'] < 10]['PatientID'].unique()\nresult = len(filtered_patients)\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/51/result.txt\", \"w\") as f:\n    f.write(str(result))"
    },
    {
        "task": "Calculate the average value of the 'Heart Rate' field for all records where the 'Age' field is greater than 65 and the 'Oxygen saturation' field is less than 95.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_querying",
        "ID": 52,
        "code": "import pandas as pd\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nfiltered_df = df[(df['Age'] > 65) & (df['Oxygen saturation'] < 95)]\naverage_heart_rate = filtered_df['Heart Rate'].mean()\nresult = pd.DataFrame({'Average Heart Rate': [average_heart_rate]})\nresult.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/52/average_heart_rate.csv\", index=False)"
    },
    {
        "task": "Find the minimum 'Mean blood pressure' for all records where the 'Age' is less than 40 and the 'LOS' (Length of Stay) is greater than 50.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_querying",
        "ID": 53,
        "code": "import pandas as pd\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nfiltered_df = df[(df['Age'] < 40) & (df['LOS'] > 50)]\nmin_mean_bp = filtered_df['Mean blood pressure'].min()\nresult = pd.DataFrame({'Minimum Mean Blood Pressure': [min_mean_bp]})\nresult.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/53/result.csv\", index=False)"
    },
    {
        "task": "Visualize the trend of 'Heart Rate' over 'RecordTime' for a specific 'AdmissionID'. Plot the 'Heart Rate' values on the y-axis and 'RecordTime' on the x-axis to show how the patient's heart rate changed throughout that particular hospital admission.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 54,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nadmission_id = df['AdmissionID'].iloc[0]\nspecific_admission = df[df['AdmissionID'] == admission_id]\nspecific_admission['RecordTime'] = pd.to_datetime(specific_admission['RecordTime'])\nspecific_admission = specific_admission.sort_values('RecordTime')\n\nplt.figure(figsize=(10, 6))\nplt.plot(specific_admission['RecordTime'], specific_admission['Heart Rate'], marker='o')\nplt.xlabel('RecordTime')\nplt.ylabel('Heart Rate')\nplt.title(f'Heart Rate Trend for AdmissionID {admission_id}')\nplt.grid(True)\nplt.xticks(rotation=45)\nplt.tight_layout()\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/54/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(f\"{output_dir}heart_rate_trend.png\")"
    },
    {
        "task": "Create a box plot to compare the distribution of 'Heart Rate' for patients based on their 'Outcome' (0 or 1). Plot 'Outcome' on the x-axis and 'Heart Rate' on the y-axis to see if there's a noticeable difference in heart rate distributions between the two outcome groups.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 55,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nplt.figure(figsize=(8, 6))\ndf.boxplot(column='Heart Rate', by='Outcome')\nplt.title('Heart Rate Distribution by Outcome')\nplt.suptitle('')\nplt.xlabel('Outcome')\nplt.ylabel('Heart Rate')\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/55/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'heart_rate_boxplot.png'))\nplt.close()"
    },
    {
        "task": "Create a scatter plot to visualize the relationship between Systolic blood pressure and Diastolic blood pressure. Plot 'Systolic blood pressure' on the x-axis and 'Diastolic blood pressure' on the y-axis to explore the correlation between these two vital signs across all records.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 56,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nplt.figure(figsize=(10, 6))\nplt.scatter(df['Systolic blood pressure'], df['Diastolic blood pressure'], alpha=0.5)\nplt.xlabel('Systolic blood pressure')\nplt.ylabel('Diastolic blood pressure')\nplt.title('Systolic vs Diastolic Blood Pressure')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/56/scatter_plot.png\")\nplt.close()"
    },
    {
        "task": "Create a box plot to compare the distribution of 'Length of Stay (LOS)' for patients based on their 'Outcome'. Plot 'Outcome' (0 or 1) on the x-axis and 'LOS' on the y-axis to visualize how the distribution of hospital stay duration varies between patients with different outcomes.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 57,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Outcome', y='LOS', data=df)\nplt.xlabel('Outcome')\nplt.ylabel('Length of Stay (LOS)')\nplt.title('Distribution of LOS by Outcome')\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/57/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'los_outcome_boxplot.png'))\nplt.close()"
    },
    {
        "task": "Create a scatter plot to visualize the relationship between 'Glucose' and 'Heart Rate'. Plot 'Glucose' on the x-axis and 'Heart Rate' on the y-axis to explore if there is any correlation or pattern between these two physiological measurements across all records.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 58,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nplt.figure(figsize=(10, 6))\nplt.scatter(df['Glucose'], df['Heart Rate'], alpha=0.5)\nplt.xlabel('Glucose')\nplt.ylabel('Heart Rate')\nplt.title('Glucose vs Heart Rate')\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/58/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'glucose_heart_rate_scatter.png'))\nplt.close()"
    },
    {
        "task": "Create a box plot to compare the distribution of 'Weight' for patients based on their 'Sex'. Plot 'Sex' on the x-axis and 'Weight' on the y-axis to visualize how the distribution of patient weights differs between male (Sex=1) and female (Sex=0) patients.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_DATA_DIRECT",
        "ID": 59,
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nplt.figure(figsize=(10, 6))\ndf.boxplot(column='Weight', by='Sex')\nplt.xlabel('Sex')\nplt.ylabel('Weight')\nplt.title('Weight Distribution by Sex')\nplt.suptitle('')\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/59/\"\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, 'weight_by_sex_boxplot.png'))\nplt.close()"
    },
    {
        "task": "Calculate the mean and standard deviation of 'Heart Rate', determine the count of records where 'Outcome' is 1, and count the number of 'nan' values in the 'Height' column.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_statistics",
        "ID": 60,
        "code": "import pandas as pd\nimport numpy as np\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nheart_rate_mean = df['Heart Rate'].mean()\nheart_rate_std = df['Heart Rate'].std()\noutcome_count = df['Outcome'].eq(1).sum()\nheight_nan_count = df['Height'].isna().sum()\n\nresults = {\n    'heart_rate_mean': heart_rate_mean,\n    'heart_rate_std': heart_rate_std,\n    'outcome_count': outcome_count,\n    'height_nan_count': height_nan_count\n}\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/60/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(f\"{output_dir}results.txt\", 'w') as f:\n    f.write(str(results))"
    },
    {
        "task": "Group the data by 'Sex' and, for each group, calculate the average 'Weight' and the standard deviation of 'Heart Rate'.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_statistics",
        "ID": 61,
        "code": "import pandas as pd\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\nresult = df.groupby('Sex').agg({'Weight': 'mean', 'Heart Rate': 'std'}).reset_index()\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/61/\", exist_ok=True)\nresult.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/61/result.csv\", index=False)"
    },
    {
        "task": "Calculate the median Length of Stay ('LOS') across all records. Additionally, calculate the Interquartile Range (IQR) of 'Weight' for all records where 'Sex' is 0.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_statistics",
        "ID": 62,
        "code": "import pandas as pd\nimport numpy as np\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nmedian_los = df['LOS'].median()\n\nweight_iqr_sex0 = df[df['Sex'] == 0]['Weight'].quantile(0.75) - df[df['Sex'] == 0]['Weight'].quantile(0.25)\n\nresults = {\n    'median_LOS': median_los,\n    'weight_IQR_sex0': weight_iqr_sex0\n}\n\nimport os\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/62/\", exist_ok=True)\noutput_path = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/62/results.json\"\nimport json\nwith open(output_path, 'w') as f:\n    json.dump(results, f)"
    },
    {
        "task": "For each unique hospital admission (identified by 'AdmissionID'), calculate the mean 'Heart Rate', 'Temperature', and 'Systolic blood pressure' across all recorded measurements for that admission. Handle missing values in these vital sign columns by excluding them from the average calculation.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_wrangling",
        "ID": 63,
        "code": "import pandas as pd\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nvitals = ['Heart Rate', 'Temperature', 'Systolic blood pressure']\nresult = df.groupby('AdmissionID')[vitals].mean()\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/63/\"\nos.makedirs(output_dir, exist_ok=True)\nresult.to_csv(os.path.join(output_dir, \"vital_signs_mean.csv\"))"
    },
    {
        "task": "For each unique hospital admission (identified by 'AdmissionID'), fill the missing numerical values in the clinical measurement columns using a forward fill approach within that admission group. If the first value within an admission for a specific column is still missing after the forward fill, use a backward fill for those remaining missing values within the same admission group.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_wrangling",
        "ID": 64,
        "code": "import pandas as pd\nimport os\n\n# Load the dataset\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Identify numerical columns (excluding RecordID, PatientID, AdmissionID, Outcome, Text, Sex)\nnumerical_cols = ['Age', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n                  'Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', \n                  'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', \n                  'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\n\n# Group by AdmissionID and apply forward fill followed by backward fill\ndf[numerical_cols] = df.groupby('AdmissionID')[numerical_cols].apply(lambda x: x.ffill().bfill())\n\n# Ensure the output directory exists\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/64/\", exist_ok=True)\n\n# Save the results\ndf.to_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/64/filled_data.parquet\")"
    },
    {
        "task": "For each unique hospital admission (identified by 'AdmissionID'), calculate the range (maximum value minus minimum value) for 'Systolic blood pressure' and 'Heart Rate' across all recorded measurements for that admission. Handle missing values in these columns by excluding them from the range calculation.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_wrangling",
        "ID": 65,
        "code": "import pandas as pd\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/65/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndef calculate_range(group):\n    sbp_range = group['Systolic blood pressure'].max() - group['Systolic blood pressure'].min() if not group['Systolic blood pressure'].isna().all() else None\n    hr_range = group['Heart Rate'].max() - group['Heart Rate'].min() if not group['Heart Rate'].isna().all() else None\n    return pd.Series({'SBP_Range': sbp_range, 'HR_Range': hr_range})\n\nresult = df.groupby('AdmissionID').apply(calculate_range).reset_index()\n\nresult.to_csv(os.path.join(output_dir, 'result.csv'), index=False)"
    },
    {
        "task": "Build a predictive model to determine the likelihood of a patient being readmitted.\n\n1.  **Data Preparation:** For each unique `AdmissionID`, extract and process relevant features from the time-series data. This could involve aggregating the time-series measurements. Combine these aggregated features with static patient/admission information ('Age', 'Sex', 'LOS', 'Outcome') to create a single feature vector per admission.\n2.  **Missing Value Handling:** Address the missing values ('nan') in the dataset.\n3.  **Model Training:** Train a binary classification model on the prepared admission-level dataset to predict the `Readmission` outcome.\n4.  **Evaluation and Visualization:** Evaluate your model's performance using appropriate metrics for binary classification. Then, create a visualization that helps interpret the model or its results.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 66,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport os\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/66/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ntime_series_cols = ['Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n                    'Glascow coma scale eye opening', 'Glascow coma scale motor response', \n                    'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose', \n                    'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', \n                    'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\nstatic_cols = ['Age', 'Sex', 'LOS', 'Outcome']\n\ndef aggregate_time_series(group):\n    agg_funcs = {\n        col: ['mean', 'std', 'min', 'max', 'median'] for col in time_series_cols\n    }\n    return group.agg(agg_funcs)\n\naggregated = data.groupby('AdmissionID').apply(aggregate_time_series)\naggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]\naggregated.reset_index(inplace=True)\n\nstatic_data = data[['AdmissionID'] + static_cols].drop_duplicates()\nfeatures = pd.merge(aggregated, static_data, on='AdmissionID', how='left')\n\ntarget = data[['AdmissionID', 'Readmission']].drop_duplicates()\nfeatures = pd.merge(features, target, on='AdmissionID', how='left')\n\nfeatures['Sex'] = features['Sex'].map({'M': 1, 'F': 0})\nfeatures['Outcome'] = features['Outcome'].map({'EXPIRED': 1, 'DISCHARGED': 0})\n\nimputer = SimpleImputer(strategy='median')\nX = imputer.fit_transform(features.drop(['AdmissionID', 'Readmission'], axis=1))\ny = features['Readmission'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]\n\nreport = classification_report(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nwith open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n    f.write(report)\n\nplt.figure(figsize=(8, 6))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\nplt.close()\n\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.savefig(os.path.join(output_dir, 'roc_curve.png'))\nplt.close()\n\nfeature_importances = pd.DataFrame({'feature': features.drop(['AdmissionID', 'Readmission'], axis=1).columns, 'importance': model.feature_importances_})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.to_csv(os.path.join(output_dir, 'feature_importances.csv'), index=False)"
    },
    {
        "task": "Build a predictive model to forecast the patient's `Outcome` using only the clinical data available within the first 24 hours of their admission to the ICU.\n\n1.  **Data Preparation:** For each unique `AdmissionID`, identify the first `RecordTime`. Select all records (`RecordID`) associated with that `AdmissionID` whose `RecordTime` is within 24 hours of this initial time point. Aggregate the numerical clinical measurements from these early records. Calculate summary statistics for each measurement within this 24-hour window. Combine these aggregated features with static patient/admission characteristics available at the start of the stay ('Age', 'Sex'). \n2.  **Model Training:** Train a binary classification model on the prepared dataset (where each row represents an admission, and features are derived from the first 24 hours) to predict the `Outcome` (0 or 1).\n3.  **Visualization:** Select two to three clinically relevant aggregated features that you calculated from the first 24 hours. Create comparative visualizations that display the distribution of these selected features separately for the two `Outcome` classes (patients with Outcome 0 and patients with Outcome 1). This visualization should help illustrate how the distribution of these key early physiological indicators differs based on the patient's final outcome.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 67,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/67/\", exist_ok=True)\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\nfirst_records = df.groupby('AdmissionID')['RecordTime'].min().reset_index()\nfirst_records.columns = ['AdmissionID', 'FirstRecordTime']\n\ndf = pd.merge(df, first_records, on='AdmissionID')\ndf['TimeDiff'] = (df['RecordTime'] - df['FirstRecordTime']).dt.total_seconds() / 3600\ndf_first_24h = df[df['TimeDiff'] <= 24]\n\nnumeric_cols = ['Age', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n                'Glascow coma scale eye opening', 'Glascow coma scale motor response', \n                'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose', \n                'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', \n                'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\n\nagg_funcs = ['mean', 'median', 'min', 'max', 'std']\nagg_dict = {col: agg_funcs for col in numeric_cols}\nagg_dict['Sex'] = 'first'\nagg_dict['Outcome'] = 'first'\n\naggregated_data = df_first_24h.groupby('AdmissionID').agg(agg_dict)\naggregated_data.columns = ['_'.join(col).strip() for col in aggregated_data.columns.values]\naggregated_data.reset_index(inplace=True)\n\nX = aggregated_data.drop(['AdmissionID', 'Outcome_first'], axis=1)\ny = aggregated_data['Outcome_first']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nreport = classification_report(y_test, y_pred, output_dict=True)\nreport_df = pd.DataFrame(report).transpose()\nreport_df.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/67/classification_report.csv\")\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Outcome_first', y='Glascow coma scale total_mean', data=aggregated_data)\nplt.title('Distribution of Glasgow Coma Scale Total (Mean) by Outcome')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/67/gcs_total_distribution.png\")\nplt.close()\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Outcome_first', y='Heart Rate_mean', data=aggregated_data)\nplt.title('Distribution of Heart Rate (Mean) by Outcome')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/67/heart_rate_distribution.png\")\nplt.close()\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Outcome_first', y='Respiratory rate_mean', data=aggregated_data)\nplt.title('Distribution of Respiratory Rate (Mean) by Outcome')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/67/resp_rate_distribution.png\")\nplt.close()"
    },
    {
        "task": "Build a time-series predictive model to forecast the patient's `Outcome` using the sequence of clinical observations recorded during their stay. This differs from aggregating features by processing the temporal sequence directly.\n\n1.  **Data Preparation:** Group the records by `AdmissionID`. For each admission, create sequences of the relevant numerical clinical measurements (`Capillary refill rate` through `pH`). Include static features like 'Age' and 'Sex' as appropriate inputs to your sequence model.\n2.  **Model Training:** Train a sequence classification model to predict the binary `Outcome` for each admission based on the prepared sequences and static features. You might consider using the data up to a certain time horizon within each admission for training, or train models that can process sequences of varying lengths.\n3.  **Evaluation and Visualization:** Evaluate your sequence model's performance using appropriate binary classification metrics. To fulfill the visualization requirement and analyze the temporal predictive power, create a plot that shows how a key performance metric changes when the model is trained and evaluated using progressively longer segments of the time-series data from the start of the admission. This visualization should illustrate how the predictability of the final `Outcome` evolves over the course of the patient's stay based on the available data.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 68,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/68/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nnumerical_features = ['Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n                      'Glascow coma scale eye opening', 'Glascow coma scale motor response', \n                      'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose', \n                      'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', \n                      'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\nstatic_features = ['Age', 'Sex']\n\ndata[numerical_features] = data[numerical_features].apply(pd.to_numeric, errors='coerce')\ndata[static_features] = data[static_features].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=['Outcome'])\n\nle = LabelEncoder()\ndata['Sex'] = le.fit_transform(data['Sex'])\ndata['Outcome_encoded'] = le.fit_transform(data['Outcome'])\n\nscaler = StandardScaler()\ndata[numerical_features] = scaler.fit_transform(data[numerical_features])\n\ngrouped = data.groupby('AdmissionID')\nsequences = []\nstatic_data = []\noutcomes = []\n\nfor name, group in grouped:\n    group = group.sort_values('RecordTime')\n    seq = group[numerical_features].values\n    static = group[static_features].iloc[0].values\n    outcome = group['outcome_encoded'].iloc[0]\n    sequences.append(seq)\n    static_data.append(static)\n    outcomes.append(outcome)\n\nmax_len = max(len(seq) for seq in sequences)\npadded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, dtype='float32')\nstatic_data = np.array(static_data)\noutcomes = np.array(outcomes)\n\nX_train_seq, X_test_seq, X_train_static, X_test_static, y_train, y_test = train_test_split(\n    padded_sequences, static_data, outcomes, test_size=0.2, random_state=42)\n\nsequence_input = Input(shape=(max_len, len(numerical_features)), name='sequence_input')\nstatic_input = Input(shape=(len(static_features),), name='static_input')\n\nlstm_layer = LSTM(64, return_sequences=True)(sequence_input)\nlstm_layer = Dropout(0.2)(lstm_layer)\nlstm_layer = LSTM(32)(lstm_layer)\nlstm_layer = Dropout(0.2)(lstm_layer)\n\ncombined = Concatenate()([lstm_layer, static_input])\ndense_layer = Dense(32, activation='relu')(combined)\noutput_layer = Dense(1, activation='sigmoid')(dense_layer)\n\nmodel = Model(inputs=[sequence_input, static_input], outputs=output_layer)\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(\n    {'sequence_input': X_train_seq, 'static_input': X_train_static},\n    y_train,\n    epochs=10,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1)\n\ny_pred = model.predict({'sequence_input': X_test_seq, 'static_input': X_test_static})\ny_pred_binary = (y_pred > 0.5).astype(int)\n\naccuracy = accuracy_score(y_test, y_pred_binary)\nroc_auc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred_binary)\n\nwith open(os.path.join(output_dir, 'metrics.txt'), 'w') as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n    f.write(f\"ROC AUC: {roc_auc}\\n\")\n    f.write(f\"F1 Score: {f1}\\n\")\n\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.savefig(os.path.join(output_dir, 'training_accuracy.png'))\nplt.close()\n\ntime_horizons = np.linspace(0.1, 1.0, 10)\naccuracies = []\n\nfor horizon in time_horizons:\n    truncated_sequences = [seq[:int(len(seq) * horizon)] for seq in sequences]\n    max_len_trunc = max(len(seq) for seq in truncated_sequences)\n    padded_trunc = tf.keras.preprocessing.sequence.pad_sequences(truncated_sequences, maxlen=max_len_trunc, dtype='float32')\n    \n    X_train_trunc, X_test_trunc, _, _, y_train_trunc, y_test_trunc = train_test_split(\n        padded_trunc, static_data, outcomes, test_size=0.2, random_state=42)\n    \n    model_trunc = Model(inputs=[sequence_input, static_input], outputs=output_layer)\n    model_trunc.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    model_trunc.fit(\n        {'sequence_input': X_train_trunc, 'static_input': X_train_static},\n        y_train_trunc,\n        epochs=5,\n        batch_size=32,\n        verbose=0)\n    \n    y_pred_trunc = model_trunc.predict({'sequence_input': X_test_trunc, 'static_input': X_test_static})\n    y_pred_trunc_binary = (y_pred_trunc > 0.5).astype(int)\n    accuracies.append(accuracy_score(y_test_trunc, y_pred_trunc_binary))\n\nplt.figure(figsize=(10, 6))\nplt.plot(time_horizons, accuracies, marker='o')\nplt.xlabel('Proportion of Time-Series Used')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs Time Horizon')\nplt.savefig(os.path.join(output_dir, 'time_horizon_accuracy.png'))\nplt.close()"
    },
    {
        "task": "Predict the `Outcome` for each admission by analyzing the longitudinal patterns and summary statistics of the clinical measurements over the patient's entire stay.\n\n1.  **Data Preparation:** Group the records by `AdmissionID`. For each admission, engineer features from the time-series data (`Capillary refill rate` through `pH`) that capture not only the *summary statistics* but also *characteristics of the trajectory* for key physiological variables across the *entire* admission. Combine these engineered features with static patient/admission features (`Age`, `Sex`, `LOS`).\n2.  **Model Training:** Train a binary classification model on the admission-level dataset prepared in step 1 to predict the `Outcome` (0 or 1).\n3.  **Evaluation and Visualization:** Evaluate your model's performance. To fulfill the visualization requirement, select *at least two* engineered features that represent the *variability* or *trend* of a physiological measurement over time within the admission. Create comparative visualizations showing the distribution of these selected engineered features separately for patients with Outcome 0 and Outcome 1.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 69,
        "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndef engineer_features(group):\n    features = {}\n    ts_cols = ['Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n               'Glascow coma scale eye opening', 'Glascow coma scale motor response', \n               'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose', \n               'Heart Rate', 'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate', \n               'Systolic blood pressure', 'Temperature', 'pH']\n    for col in ts_cols:\n        if col in group.columns:\n            vals = group[col].dropna()\n            if len(vals) > 0:\n                features[f'{col}_mean'] = vals.mean()\n                features[f'{col}_std'] = vals.std()\n                features[f'{col}_min'] = vals.min()\n                features[f'{col}_max'] = vals.max()\n                features[f'{col}_slope'] = np.polyfit(range(len(vals)), vals, 1)[0] if len(vals) > 1 else 0\n    static_features = group[['Age', 'Sex', 'LOS']].iloc[0]\n    for k, v in static_features.items():\n        features[k] = v\n    features['Outcome'] = group['Outcome'].iloc[0]\n    return pd.Series(features)\n\nadmission_df = df.groupby('AdmissionID').apply(engineer_features).reset_index()\n\nX = admission_df.drop(['AdmissionID', 'Outcome'], axis=1)\ny = admission_df['Outcome']\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/69/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(f\"{output_dir}classification_report.txt\", \"w\") as f:\n    f.write(classification_report(y_test, y_pred))\n    f.write(f\"\\nROC AUC: {roc_auc_score(y_test, y_proba)}\")\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(admission_df[admission_df['Outcome'] == 0]['Heart Rate_slope'], alpha=0.5, label='Outcome 0')\nplt.hist(admission_df[admission_df['Outcome'] == 1]['Heart Rate_slope'], alpha=0.5, label='Outcome 1')\nplt.title('Heart Rate Slope Distribution')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.hist(admission_df[admission_df['Outcome'] == 0]['Systolic blood pressure_std'], alpha=0.5, label='Outcome 0')\nplt.hist(admission_df[admission_df['Outcome'] == 1]['Systolic blood pressure_std'], alpha=0.5, label='Outcome 1')\nplt.title('Systolic BP Std Distribution')\nplt.legend()\n\nplt.savefig(f\"{output_dir}feature_distributions.png\")\nplt.close()"
    },
    {
        "task": "Build a model to predict whether a patient is likely to have a 'Short Stay' or a 'Long Stay', and to visualize characteristics of patients where the model makes prediction errors.\n\n1.  **Data Preparation:**\n    *   Define 'Short Stay' and 'Long Stay' based on the `LOS`. Calculate the median `LOS` across all admissions.\n    *   For each unique `AdmissionID`, aggregate the time-series numerical clinical measurements by computing summary statistics. Include static features like 'Age', 'Sex', 'Outcome', and 'Readmission' in the aggregated dataset.\n\n2.  **Model Training and Evaluation:**\n    *   Split the aggregated admission-level dataset into training and testing sets.\n    *   Train a binary classification model on the training data to predict the 'Short Stay' vs. 'Long Stay' outcome.\n    *   Evaluate the performance of your trained model on the test set using appropriate binary classification metrics.\n\n3.  **Visualization of Prediction Errors vs. Patient Characteristics:**\n    *   Select *two* features from your aggregated dataset that represent different aspects of patient characteristics or clinical state .\n    *   Using the test set predictions, identify the instances that were misclassified by your model. Categorize these misclassified instances into False Positives (predicted 'Long Stay', actual 'Short Stay') and False Negatives (predicted 'Short Stay', actual 'Long Stay').\n    *   Create a scatter plot or a similar visualization showing the distribution or relationship between your two selected features *specifically* for the misclassified instances. Clearly distinguish between False Positives and False Negatives in the visualization. This plot should provide insights into the characteristics of patients that the model struggles to classify correctly, helping to diagnose model weaknesses.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 70,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport os\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/70/\", exist_ok=True)\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nmedian_los = data['LOS'].median()\ndata['Stay_Type'] = np.where(data['LOS'] <= median_los, 'Short Stay', 'Long Stay')\n\nnumerical_features = ['Age', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n                      'Glascow coma scale eye opening', 'Glascow coma scale motor response', \n                      'Glascow coma scale total', 'Glascow coma scale verbal response', \n                      'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', \n                      'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', \n                      'Temperature', 'Weight', 'pH']\nstatic_features = ['Age', 'Sex', 'Outcome', 'Readmission']\n\naggregated_data = data.groupby('AdmissionID').agg({\n    **{col: ['mean', 'std', 'min', 'max'] for col in numerical_features},\n    **{col: 'first' for col in static_features},\n    'Stay_Type': 'first'\n}).reset_index()\n\naggregated_data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in aggregated_data.columns]\naggregated_data.rename(columns={'Stay_Type_first': 'Stay_Type'}, inplace=True)\n\nX = aggregated_data.drop(['AdmissionID', 'Stay_Type'], axis=1)\nX = pd.get_dummies(X, columns=['Sex_first', 'Outcome_first'])\ny = aggregated_data['Stay_Type']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nreport = classification_report(y_test, y_pred, output_dict=True)\ncm = confusion_matrix(y_test, y_pred)\n\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/70/classification_report.txt\", \"w\") as f:\n    f.write(str(report))\n\nmisclassified = X_test.copy()\nmisclassified['Actual'] = y_test\nmisclassified['Predicted'] = y_pred\nmisclassified = misclassified[misclassified['Actual'] != misclassified['Predicted']]\n\nmisclassified['Error_Type'] = np.where(\n    (misclassified['Predicted'] == 'Long Stay') & (misclassified['Actual'] == 'Short Stay'), \n    'False Positive', \n    'False Negative'\n)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(\n    misclassified['Age_mean'], \n    misclassified['Heart Rate_mean'], \n    c=misclassified['Error_Type'].map({'False Positive': 'red', 'False Negative': 'blue'}), \n    alpha=0.5\n)\nplt.xlabel('Age (mean)')\nplt.ylabel('Heart Rate (mean)')\nplt.title('Misclassified Patients by Age and Heart Rate')\nplt.legend(handles=[\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='False Positive'),\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='False Negative')\n])\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/70/misclassified_patients.png\")\nplt.close()"
    },
    {
        "task": "Identify distinct groups (clusters) of patients based on their clinical state during the initial period of their admission and to visualize the distribution of the Length of Stay (`LOS`) for each identified group.\n\n1.  **Data Preparation:**\n    *   For each unique `AdmissionID`, aggregate the numerical clinical measurements (`Capillary refill rate` through `pH`) taken within the first 24 hours of the admission. Calculate summary statistics for each measurement across this time window. If a measurement is not recorded within the first 24 hours for a specific admission, handle it appropriately.\n    *   Combine these aggregated features with the static patient/admission features available early on ('Age', 'Sex').\n    *   Create a single feature vector for each `AdmissionID`.\n\n2.  **Patient Clustering:**\n    *   Apply a clustering algorithm to the prepared admission-level dataset (using the features derived from the first 24 hours). Choose a suitable number of clusters.\n    *   Assign each admission to its corresponding cluster.\n\n3.  **Visualization of Length of Stay per Cluster:**\n    *   Create a comparative visualization that displays the distribution of the `LOS` for patients in each of the identified clusters. \n    *   The visualization should clearly show the differences (or similarities) in `LOS` distributions across the patient groups defined by their early clinical profiles.",
        "task_type": "Visualization",
        "dataset": "MIMIC-IV",
        "metadata": "plotting_or_visualization_MODEL_ANALYSIS",
        "ID": 71,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/71/\", exist_ok=True)\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nnumerical_cols = [\n    'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen',\n    'Glascow coma scale eye opening', 'Glascow coma scale motor response',\n    'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose',\n    'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation',\n    'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH'\n]\nstatic_cols = ['Age', 'Sex']\n\ndef aggregate_first_24h(group):\n    first_24h = group[group['RecordTime'] <= group['RecordTime'].min() + pd.Timedelta(hours=24)]\n    agg_dict = {}\n    for col in numerical_cols:\n        if col in first_24h.columns:\n            vals = first_24h[col].dropna()\n            if len(vals) > 0:\n                agg_dict[f'{col}_mean'] = vals.mean()\n                agg_dict[f'{col}_median'] = vals.median()\n                agg_dict[f'{col}_std'] = vals.std()\n            else:\n                agg_dict[f'{col}_mean'] = np.nan\n                agg_dict[f'{col}_median'] = np.nan\n                agg_dict[f'{col}_std'] = np.nan\n    return pd.Series(agg_dict)\n\nagg_data = data.groupby('AdmissionID').apply(aggregate_first_24h).reset_index()\n\nstatic_data = data.groupby('AdmissionID')[static_cols].first().reset_index()\nmerged_data = pd.merge(agg_data, static_data, on='AdmissionID', how='left')\nmerged_data = pd.merge(merged_data, data[['AdmissionID', 'LOS']].drop_duplicates(), on='AdmissionID', how='left')\n\nmerged_data['Sex'] = merged_data['Sex'].map({'M': 1, 'F': 0})\nmerged_data.fillna(merged_data.median(), inplace=True)\n\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(merged_data.drop(['AdmissionID', 'LOS'], axis=1))\n\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(scaled_features)\nmerged_data['Cluster'] = clusters\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Cluster', y='LOS', data=merged_data)\nplt.title('Length of Stay Distribution by Cluster')\nplt.xlabel('Cluster')\nplt.ylabel('LOS (days)')\nplt.savefig('/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/71/los_distribution_by_cluster.png')\nplt.close()\n\nmerged_data.to_csv('/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/71/clustered_data.csv', index=False)"
    },
    {
        "task": "Analyze the provided patient admission data to identify which clinical measurements (such as vital signs, GCS scores, glucose, pH), demographic features (age, sex), or admission-specific details (LOS) are most significantly associated with the patient `Outcome`. Generate a report summarizing the key findings, including descriptive statistics comparing the distributions of relevant factors for different outcome groups and an analysis of which variables show the strongest statistical association or predictive power regarding the outcome.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 72,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/101/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nnumeric_cols = ['Age', 'LOS', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n                'Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', \n                'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', \n                'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\ncategorical_cols = ['Sex']\n\ndata[numeric_cols] = data[numeric_cols].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=['Outcome'] + numeric_cols + categorical_cols)\n\noutcome_groups = data.groupby('Outcome')\n\ndescriptive_stats = outcome_groups[numeric_cols].describe().transpose()\ndescriptive_stats.to_csv(os.path.join(output_dir, 'descriptive_stats.csv'))\n\np_values = {}\nfor col in numeric_cols:\n    groups = [group[col].values for name, group in outcome_groups]\n    f_val, p_val = stats.f_oneway(*groups)\n    p_values[col] = p_val\n\np_values_df = pd.DataFrame.from_dict(p_values, orient='index', columns=['p_value'])\np_values_df.to_csv(os.path.join(output_dir, 'anova_p_values.csv'))\n\nsignificant_cols = [col for col in p_values if p_values[col] < 0.05]\n\nX = data[significant_cols + ['Sex']]\nX = pd.get_dummies(X, columns=['Sex'], drop_first=True)\ny = data['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train_scaled, y_train)\n\ny_pred = rf.predict(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred, output_dict=True)\n\nwith open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n    f.write(classification_report(y_test, y_pred))\n\nfeature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})\nfeature_importances = feature_importances.sort_values(by='Importance', ascending=False)\nfeature_importances.to_csv(os.path.join(output_dir, 'feature_importances.csv'), index=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importances.head(10))\nplt.title('Top 10 Important Features')\nplt.savefig(os.path.join(output_dir, 'feature_importance_plot.png'))\nplt.close()\n\nfor col in significant_cols:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Outcome', y=col, data=data)\n    plt.title(f'Distribution of {col} by Outcome')\n    plt.savefig(os.path.join(output_dir, f'{col}_distribution.png'))\n    plt.close()"
    },
    {
        "task": "Investigate the temporal dynamics of patient health status during hospitalization using the provided dataset. The task is to analyze how key clinical measurements (e.g., Heart Rate, Blood Pressure, Temperature, Oxygen saturation, Glucose, GCS scores) evolve over the course of an admission. Generate a report that summarizes the typical patterns of these measurements over time for different patient outcomes (e.g., those with Outcome=0 vs. Outcome=1) or readmission statuses. The report should include visualizations showing variable trajectories and identify significant trends or critical points in the time series data that may distinguish patient groups.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 73,
        "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/102/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['TimeSinceAdmission'] = (df['RecordTime'] - df.groupby('AdmissionID')['RecordTime'].transform('min')).dt.total_seconds() / 3600\n\nclinical_vars = ['Heart Rate', 'Diastolic blood pressure', 'Systolic blood pressure', 'Mean blood pressure', \n                 'Temperature', 'Oxygen saturation', 'Glucose', 'Glascow coma scale total']\n\ndf_grouped = df.groupby(['AdmissionID', 'Outcome', 'TimeSinceAdmission'])[clinical_vars].mean().reset_index()\n\noutcome_0 = df_grouped[df_grouped['Outcome'] == 0]\noutcome_1 = df_grouped[df_grouped['Outcome'] == 1]\n\nfor var in clinical_vars:\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(data=outcome_0, x='TimeSinceAdmission', y=var, label='Outcome=0', ci=95)\n    sns.lineplot(data=outcome_1, x='TimeSinceAdmission', y=var, label='Outcome=1', ci=95)\n    plt.title(f'Temporal Trend of {var} by Outcome')\n    plt.xlabel('Hours Since Admission')\n    plt.ylabel(var)\n    plt.legend()\n    plt.savefig(f\"{output_dir}{var.replace(' ', '_')}_by_outcome.png\")\n    plt.close()\n\ndf_grouped_readmit = df.groupby(['AdmissionID', 'Readmission', 'TimeSinceAdmission'])[clinical_vars].mean().reset_index()\nreadmit_0 = df_grouped_readmit[df_grouped_readmit['Readmission'] == 0]\nreadmit_1 = df_grouped_readmit[df_grouped_readmit['Readmission'] == 1]\n\nfor var in clinical_vars:\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(data=readmit_0, x='TimeSinceAdmission', y=var, label='No Readmission', ci=95)\n    sns.lineplot(data=readmit_1, x='TimeSinceAdmission', y=var, label='Readmission', ci=95)\n    plt.title(f'Temporal Trend of {var} by Readmission Status')\n    plt.xlabel('Hours Since Admission')\n    plt.ylabel(var)\n    plt.legend()\n    plt.savefig(f\"{output_dir}{var.replace(' ', '_')}_by_readmission.png\")\n    plt.close()\n\nsummary_stats = df_grouped.groupby(['Outcome', 'TimeSinceAdmission'])[clinical_vars].agg(['mean', 'std']).reset_index()\nsummary_stats.to_csv(f\"{output_dir}summary_statistics_by_outcome.csv\", index=False)\n\nreadmit_stats = df_grouped_readmit.groupby(['Readmission', 'TimeSinceAdmission'])[clinical_vars].agg(['mean', 'std']).reset_index()\nreadmit_stats.to_csv(f\"{output_dir}summary_statistics_by_readmission.csv\", index=False)"
    },
    {
        "task": "Analyze the provided patient admission data to identify demographic and initial clinical characteristics associated with patient readmission. Focus on analyzing patient-level data, aggregating or selecting features from the start of each admission (identified by `AdmissionID`), such as Age, Sex, and clinical measurements from the earliest available `RecordTime` within an admission. Investigate which of these initial features or patient demographic factors are significantly correlated with or predictive of the `Readmission` flag. Generate a report summarizing the key findings, including descriptive statistics comparing initial characteristics of readmitted vs. non-readmitted patients and an analysis identifying the most influential predictors of readmission risk.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 74,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/103/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\nearliest_records = df.sort_values('RecordTime').groupby('AdmissionID').first().reset_index()\n\nfeatures = ['Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n            'Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', \n            'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', \n            'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\ntarget = 'Readmission'\n\ndata = earliest_records[features + [target]].dropna()\n\ndata['Sex'] = data['Sex'].map({'M': 1, 'F': 0})\n\nreadmitted = data[data[target] == 1]\nnot_readmitted = data[data[target] == 0]\n\ndescriptive_stats = pd.concat([\n    readmitted.describe().add_prefix('readmitted_'),\n    not_readmitted.describe().add_prefix('not_readmitted_')\n], axis=1)\ndescriptive_stats.to_csv(os.path.join(output_dir, 'descriptive_stats.csv'))\n\ncorrelations = data.corr()[target].sort_values(ascending=False)\ncorrelations.to_csv(os.path.join(output_dir, 'correlations.csv'))\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(data.corr(), annot=False, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.savefig(os.path.join(output_dir, 'correlation_matrix.png'))\nplt.close()\n\nX = data.drop(columns=[target])\ny = data[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\ny_pred = model.predict(X_test_scaled)\nreport = classification_report(y_test, y_pred, output_dict=True)\npd.DataFrame(report).transpose().to_csv(os.path.join(output_dir, 'classification_report.csv'))\n\nfeature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\nfeature_importances = feature_importances.sort_values('Importance', ascending=False)\nfeature_importances.to_csv(os.path.join(output_dir, 'feature_importances.csv'), index=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importances.head(10))\nplt.title('Top 10 Feature Importances')\nplt.savefig(os.path.join(output_dir, 'feature_importances.png'))\nplt.close()\n\nfor feature in features:\n    if feature in ['Sex']:\n        continue\n    t_stat, p_val = stats.ttest_ind(\n        readmitted[feature].dropna(),\n        not_readmitted[feature].dropna(),\n        equal_var=False\n    )\n    with open(os.path.join(output_dir, 't_test_results.txt'), 'a') as f:\n        f.write(f\"{feature}: t-statistic = {t_stat}, p-value = {p_val}\\n\")"
    },
    {
        "task": "Investigate the patterns and potential implications of missing clinical data within the provided dataset. The task is to identify which clinical measurements (`Capillary refill rate`, `Diastolic blood pressure`, ..., `pH`) have the highest percentages of missing values (`nan`). Analyze whether the missingness of specific measurements is correlated with patient demographics (`Age`, `Sex`), patient outcomes (`Outcome`), or the total length of stay (`LOS`). Furthermore, explore if there are any observable relationships between the overall level or pattern of missingness across multiple variables within an admission and the patient's `Readmission` status. Generate a report summarizing these findings on missing data distribution, including variable-specific missingness rates and any statistically significant associations found between missing data patterns and patient characteristics or outcomes.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 75,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.stats import chi2_contingency, ttest_ind\n\n# Load the dataset\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Define clinical measurements\nclinical_measurements = ['Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n                         'Glascow coma scale eye opening', 'Glascow coma scale motor response', \n                         'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose', \n                         'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', \n                         'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\n\n# Calculate missingness percentages\nmissing_percent = df[clinical_measurements].isna().mean() * 100\nmissing_percent = missing_percent.sort_values(ascending=False)\n\n# Save missingness percentages\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/104/\", exist_ok=True)\nmissing_percent.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/104/missing_percentages.csv\")\n\n# Analyze missingness vs demographics, outcomes, LOS\nresults = []\n\nfor var in clinical_measurements:\n    # Missingness vs Age\n    df['missing'] = df[var].isna()\n    age_missing = df[df['missing']]['Age'].mean()\n    age_not_missing = df[~df['missing']]['Age'].mean()\n    t_stat, p_val = ttest_ind(df[df['missing']]['Age'].dropna(), df[~df['missing']]['Age'].dropna(), equal_var=False)\n    results.append({'Variable': var, 'Comparison': 'Age', 'P-value': p_val})\n    \n    # Missingness vs Sex\n    contingency = pd.crosstab(df['missing'], df['Sex'])\n    chi2, p_val, _, _ = chi2_contingency(contingency)\n    results.append({'Variable': var, 'Comparison': 'Sex', 'P-value': p_val})\n    \n    # Missingness vs Outcome\n    contingency = pd.crosstab(df['missing'], df['Outcome'])\n    chi2, p_val, _, _ = chi2_contingency(contingency)\n    results.append({'Variable': var, 'Comparison': 'Outcome', 'P-value': p_val})\n    \n    # Missingness vs LOS\n    los_missing = df[df['missing']]['LOS'].mean()\n    los_not_missing = df[~df['missing']]['LOS'].mean()\n    t_stat, p_val = ttest_ind(df[df['missing']]['LOS'].dropna(), df[~df['missing']]['LOS'].dropna(), equal_var=False)\n    results.append({'Variable': var, 'Comparison': 'LOS', 'P-value': p_val})\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/104/missingness_associations.csv\")\n\n# Analyze overall missingness pattern vs Readmission\ndf['total_missing'] = df[clinical_measurements].isna().sum(axis=1)\nreadmission_missing = df.groupby('Readmission')['total_missing'].mean()\nt_stat, p_val = ttest_ind(df[df['Readmission'] == 1]['total_missing'], df[df['Readmission'] == 0]['total_missing'], equal_var=False)\n\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/104/readmission_missingness.txt\", \"w\") as f:\n    f.write(f\"Mean missing values for Readmission=1: {readmission_missing[1]}\\n\")\n    f.write(f\"Mean missing values for Readmission=0: {readmission_missing[0]}\\n\")\n    f.write(f\"P-value: {p_val}\\n\")"
    },
    {
        "task": "Analyze the impact of maximum physiological severity during a hospital admission on patient outcomes. For each unique `AdmissionID`, calculate the 'worst' value observed across the entire stay for key vital signs and clinical scores (e.g., minimum Oxygen saturation, maximum Heart Rate, minimum Glascow coma scale total, etc.). Investigate the correlation and association of these worst-case values with patient `Outcome` (hospital mortality) and `LOS` (Length of Stay). Generate a report summarizing the calculated peak severity metrics, their distribution relative to patient outcomes, and the statistical relationships found with hospital mortality and length of stay.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 76,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Define key vital signs and clinical scores\nkey_metrics = {\n    'Oxygen saturation': 'min',\n    'Heart Rate': 'max',\n    'Glascow coma scale total': 'min',\n    'Mean blood pressure': 'min',\n    'Respiratory rate': 'max',\n    'Systolic blood pressure': 'min',\n    'Diastolic blood pressure': 'min',\n    'Temperature': 'max',\n    'Fraction inspired oxygen': 'max',\n    'Glucose': 'max',\n    'pH': 'min'\n}\n\n# Group by AdmissionID and calculate worst-case values\nworst_case_values = df.groupby('AdmissionID').agg({\n    'Oxygen saturation': 'min',\n    'Heart Rate': 'max',\n    'Glascow coma scale total': 'min',\n    'Mean blood pressure': 'min',\n    'Respiratory rate': 'max',\n    'Systolic blood pressure': 'min',\n    'Diastolic blood pressure': 'min',\n    'Temperature': 'max',\n    'Fraction inspired oxygen': 'max',\n    'Glucose': 'max',\n    'pH': 'min',\n    'Outcome': 'first',\n    'LOS': 'first'\n}).reset_index()\n\n# Merge with original data to get other patient info\npatient_info = df[['AdmissionID', 'Age', 'Sex']].drop_duplicates(subset=['AdmissionID'])\nworst_case_values = worst_case_values.merge(patient_info, on='AdmissionID', how='left')\n\n# Analyze correlation with Outcome\noutcome_corr = worst_case_values.drop(columns=['AdmissionID', 'LOS', 'Age', 'Sex']).corrwith(worst_case_values['Outcome'])\noutcome_corr = outcome_corr.drop('Outcome')\n\n# Analyze correlation with LOS\nlos_corr = worst_case_values.drop(columns=['AdmissionID', 'Outcome', 'Age', 'Sex']).corrwith(worst_case_values['LOS'])\nlos_corr = los_corr.drop('LOS')\n\n# Statistical tests for Outcome\noutcome_stats = {}\nfor metric in key_metrics.keys():\n    alive = worst_case_values[worst_case_values['Outcome'] == 0][metric]\n    dead = worst_case_values[worst_case_values['Outcome'] == 1][metric]\n    t_stat, p_val = stats.ttest_ind(alive, dead, nan_policy='omit')\n    outcome_stats[metric] = {'t_stat': t_stat, 'p_val': p_val}\n\n# Statistical tests for LOS\nlos_stats = {}\nfor metric in key_metrics.keys():\n    r, p_val = stats.pearsonr(worst_case_values[metric].fillna(worst_case_values[metric].mean()), worst_case_values['LOS'])\n    los_stats[metric] = {'r': r, 'p_val': p_val}\n\n# Generate report\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/105/\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save worst case values\nworst_case_values.to_csv(os.path.join(output_dir, 'worst_case_values.csv'), index=False)\n\n# Save correlation results\noutcome_corr.to_csv(os.path.join(output_dir, 'outcome_correlation.csv'))\nlos_corr.to_csv(os.path.join(output_dir, 'los_correlation.csv'))\n\n# Save statistical test results\npd.DataFrame(outcome_stats).T.to_csv(os.path.join(output_dir, 'outcome_stats.csv'))\npd.DataFrame(los_stats).T.to_csv(os.path.join(output_dir, 'los_stats.csv'))\n\n# Create visualizations\nplt.figure(figsize=(12, 8))\nsns.heatmap(worst_case_values[key_metrics.keys()].corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix of Worst Case Metrics')\nplt.savefig(os.path.join(output_dir, 'correlation_matrix.png'))\nplt.close()\n\nfor metric in key_metrics.keys():\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Outcome', y=metric, data=worst_case_values)\n    plt.title(f'{metric} by Outcome')\n    plt.savefig(os.path.join(output_dir, f'{metric}_by_outcome.png'))\n    plt.close()\n\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=metric, y='LOS', data=worst_case_values)\n    plt.title(f'{metric} vs LOS')\n    plt.savefig(os.path.join(output_dir, f'{metric}_vs_los.png'))\n    plt.close()"
    },
    {
        "task": "Investigate the relationship between the variability of key physiological measurements observed during a patient's hospitalization and their Length of Stay (`LOS`). For each unique `AdmissionID`, calculate measures of variability (such as standard deviation, interquartile range, or range) for critical vital signs and clinical parameters like `Heart Rate`, `Mean blood pressure`, `Oxygen saturation`, and `Temperature`, considering all recorded values within that specific admission. Analyze the correlation between these computed variability metrics and the `LOS` for each admission. Generate a report summarizing the findings, including descriptive statistics of the variability measures across the dataset and the statistical associations found between measurement variability and the duration of hospitalization (`LOS`).",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 77,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/106/\", exist_ok=True)\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nvital_signs = ['Heart Rate', 'Mean blood pressure', 'Oxygen saturation', 'Temperature']\nvariability_metrics = ['std', 'iqr', 'range']\n\ndef calculate_iqr(x):\n    return np.percentile(x, 75) - np.percentile(x, 25)\n\ndef calculate_range(x):\n    return np.max(x) - np.min(x)\n\ngrouped = df.groupby('AdmissionID')[vital_signs + ['LOS']].agg({\n    'Heart Rate': [np.std, calculate_iqr, calculate_range],\n    'Mean blood pressure': [np.std, calculate_iqr, calculate_range],\n    'Oxygen saturation': [np.std, calculate_iqr, calculate_range],\n    'Temperature': [np.std, calculate_iqr, calculate_range],\n    'LOS': 'first'\n})\n\ngrouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\ngrouped.reset_index(inplace=True)\n\ncorrelation_results = {}\nfor vs in vital_signs:\n    for metric in variability_metrics:\n        col_name = f\"{vs}_{metric}\"\n        corr, p_value = stats.pearsonr(grouped[col_name], grouped['LOS_first'])\n        correlation_results[f\"{vs}_{metric}_corr\"] = corr\n        correlation_results[f\"{vs}_{metric}_p_value\"] = p_value\n\nsummary_stats = grouped[[f\"{vs}_{metric}\" for vs in vital_signs for metric in variability_metrics]].describe()\n\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/106/summary_stats.txt\", \"w\") as f:\n    f.write(\"Descriptive Statistics of Variability Measures:\\n\")\n    f.write(summary_stats.to_string())\n    f.write(\"\\n\\nCorrelation Results:\\n\")\n    for key, value in correlation_results.items():\n        f.write(f\"{key}: {value}\\n\")\n\ngrouped.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/106/variability_metrics.csv\", index=False)"
    },
    {
        "task": "Investigate the association between the early rate of physiological change during a patient's hospitalization and subsequent clinical outcomes. For each hospital admission (`AdmissionID`), identify the change in key physiological measurements (such as Heart Rate, Mean blood pressure, Oxygen saturation, or Glascow coma scale total) between the first recorded value and values recorded within the initial 12 to 24 hours of admission. Analyze whether the magnitude and direction of this early change are statistically associated with the patient's `Outcome` (hospital mortality) or `LOS` (Length of Stay). Generate a report summarizing the calculated early change metrics and their relationship to these patient outcomes, highlighting any significant findings.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 78,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nimport statsmodels.api as sm\n\n# Load the dataset\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Ensure output directory exists\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/107/\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert RecordTime to datetime\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\n\n# Sort by AdmissionID and RecordTime\ndf = df.sort_values(['AdmissionID', 'RecordTime'])\n\n# Group by AdmissionID\ngrouped = df.groupby('AdmissionID')\n\n# Define physiological variables of interest\nphysio_vars = ['Heart Rate', 'Mean blood pressure', 'Oxygen saturation', 'Glascow coma scale total']\n\n# Initialize results storage\nresults = []\n\nfor admission_id, group in grouped:\n    # Get the first record time\n    first_time = group['RecordTime'].min()\n    \n    # Get the first values for each physiological variable\n    first_values = group.iloc[0][physio_vars].to_dict()\n    \n    # Filter records within 12 to 24 hours after admission\n    time_mask = (group['RecordTime'] >= first_time + pd.Timedelta(hours=12)) & (group['RecordTime'] <= first_time + pd.Timedelta(hours=24))\n    later_group = group[time_mask]\n    \n    if not later_group.empty:\n        # Get mean values for each physiological variable in the 12-24 hour window\n        later_values = later_group[physio_vars].mean().to_dict()\n        \n        # Calculate change (later - first)\n        changes = {var: later_values[var] - first_values[var] for var in physio_vars if pd.notna(first_values[var]) and pd.notna(later_values[var])}\n        \n        # Get outcome and LOS for this admission\n        outcome = group['Outcome'].iloc[0]\n        los = group['LOS'].iloc[0]\n        \n        # Store results\n        result = {\n            'AdmissionID': admission_id,\n            'Outcome': outcome,\n            'LOS': los,\n            **{f'{var}_first': first_values[var] for var in physio_vars},\n            **{f'{var}_later': later_values[var] for var in physio_vars},\n            **{f'{var}_change': changes.get(var, np.nan) for var in physio_vars}\n        }\n        results.append(result)\n\n# Create results DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to CSV\nresults_df.to_csv(os.path.join(output_dir, 'physiological_changes.csv'), index=False)\n\n# Analyze association with Outcome (binary)\noutcome_report = []\nfor var in physio_vars:\n    change_col = f'{var}_change'\n    valid_rows = results_df.dropna(subset=[change_col, 'Outcome'])\n    if len(valid_rows) > 0:\n        # Mann-Whitney U test for binary outcome\n        group1 = valid_rows[valid_rows['Outcome'] == 0][change_col]\n        group2 = valid_rows[valid_rows['Outcome'] == 1][change_col]\n        if len(group1) > 0 and len(group2) > 0:\n            u_stat, p_val = stats.mannwhitneyu(group1, group2)\n            outcome_report.append({\n                'Variable': var,\n                'Test': 'Mann-Whitney U',\n                'U_statistic': u_stat,\n                'p_value': p_val\n            })\n\n# Save outcome analysis\noutcome_report_df = pd.DataFrame(outcome_report)\noutcome_report_df.to_csv(os.path.join(output_dir, 'outcome_association.csv'), index=False)\n\n# Analyze association with LOS (continuous)\nlos_report = []\nfor var in physio_vars:\n    change_col = f'{var}_change'\n    valid_rows = results_df.dropna(subset=[change_col, 'LOS'])\n    if len(valid_rows) > 0:\n        # Spearman correlation\n        corr, p_val = stats.spearmanr(valid_rows[change_col], valid_rows['LOS'])\n        los_report.append({\n            'Variable': var,\n            'Test': 'Spearman',\n            'Correlation': corr,\n            'p_value': p_val\n        })\n\n# Save LOS analysis\nlos_report_df = pd.DataFrame(los_report)\nlos_report_df.to_csv(os.path.join(output_dir, 'los_association.csv'), index=False)"
    },
    {
        "task": "Investigate the association between the Length of Stay (LOS) of a hospital admission and the patient's Readmission status. The task involves analyzing the provided data grouped by unique hospital admissions (`AdmissionID`). For each admission, the total `LOS` and the `Readmission` flag are available. Compare the distribution of `LOS` for admissions that resulted in a readmission (`Readmission` = 1) versus those that did not (`Readmission` = 0). Generate a report detailing this comparison, including descriptive statistics (mean, median, spread) for `LOS` in each readmission group and the results of statistical tests assessing whether the observed difference in `LOS` distributions is statistically significant.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 79,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Group by AdmissionID to get unique admissions\nadmission_df = df.groupby('AdmissionID').agg({\n    'LOS': 'first',\n    'Readmission': 'first'\n}).reset_index()\n\n# Split into readmission and no readmission groups\nreadmission_los = admission_df[admission_df['Readmission'] == 1]['LOS']\nno_readmission_los = admission_df[admission_df['Readmission'] == 0]['LOS']\n\n# Calculate descriptive statistics\nreadmission_stats = {\n    'mean': np.mean(readmission_los),\n    'median': np.median(readmission_los),\n    'std': np.std(readmission_los),\n    'min': np.min(readmission_los),\n    'max': np.max(readmission_los)\n}\n\nno_readmission_stats = {\n    'mean': np.mean(no_readmission_los),\n    'median': np.median(no_readmission_los),\n    'std': np.std(no_readmission_los),\n    'min': np.min(no_readmission_los),\n    'max': np.max(no_readmission_los)\n}\n\n# Perform statistical test\nt_stat, p_value = stats.ttest_ind(readmission_los, no_readmission_los, equal_var=False)\n\n# Create a report\nreport = f\"\"\"\nComparison of Length of Stay (LOS) by Readmission Status\n-------------------------------------------------------\nReadmission (1):\n- Mean: {readmission_stats['mean']:.2f}\n- Median: {readmission_stats['median']:.2f}\n- Standard Deviation: {readmission_stats['std']:.2f}\n- Range: {readmission_stats['min']:.2f} - {readmission_stats['max']:.2f}\n\nNo Readmission (0):\n- Mean: {no_readmission_stats['mean']:.2f}\n- Median: {no_readmission_stats['median']:.2f}\n- Standard Deviation: {no_readmission_stats['std']:.2f}\n- Range: {no_readmission_stats['min']:.2f} - {no_readmission_stats['max']:.2f}\n\nStatistical Test:\n- T-statistic: {t_stat:.2f}\n- P-value: {p_value:.4f}\n\"\"\"\n\n# Save the report\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/108/\"\nos.makedirs(output_dir, exist_ok=True)\nwith open(os.path.join(output_dir, \"readmission_los_report.txt\"), \"w\") as f:\n    f.write(report)\n\n# Plot the distributions\nplt.figure(figsize=(10, 6))\nplt.hist(readmission_los, bins=30, alpha=0.5, label='Readmission (1)')\nplt.hist(no_readmission_los, bins=30, alpha=0.5, label='No Readmission (0)')\nplt.xlabel('Length of Stay (LOS)')\nplt.ylabel('Frequency')\nplt.title('Distribution of LOS by Readmission Status')\nplt.legend()\nplt.savefig(os.path.join(output_dir, \"los_distribution.png\"))\nplt.close()"
    },
    {
        "task": "Investigate the association between the average physiological state during a patient's hospitalization and their clinical outcome (hospital mortality). For each unique `AdmissionID`, calculate the mean value for a selection of continuous clinical measurements (e.g., `Heart Rate`, `Mean blood pressure`, `Oxygen saturation`, `Temperature`, `Glucose`, and `pH`) across all available records for that specific admission. Analyze the relationship between these calculated average values representing the typical physiological state during the stay and the patient's `Outcome` (where 1 indicates mortality). Generate a report summarizing the distribution of these average physiological metrics for patients who survived versus those who did not, and quantify the statistical association or predictive power of these average states with hospital mortality.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 80,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy import stats\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/109/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nclinical_vars = ['Heart Rate', 'Mean blood pressure', 'Oxygen saturation', 'Temperature', 'Glucose', 'pH']\navg_metrics = df.groupby('AdmissionID')[clinical_vars].mean().reset_index()\noutcomes = df.groupby('AdmissionID')['Outcome'].first().reset_index()\nmerged_data = pd.merge(avg_metrics, outcomes, on='AdmissionID')\n\nsurvived = merged_data[merged_data['Outcome'] == 0]\ndeceased = merged_data[merged_data['Outcome'] == 1]\n\nreport = {\n    'summary_stats_survived': survived[clinical_vars].describe().to_dict(),\n    'summary_stats_deceased': deceased[clinical_vars].describe().to_dict(),\n    't_test_results': {},\n    'auc_scores': {}\n}\n\nfor var in clinical_vars:\n    t_stat, p_val = stats.ttest_ind(survived[var].dropna(), deceased[var].dropna())\n    report['t_test_results'][var] = {'t_statistic': t_stat, 'p_value': p_val}\n\nX = merged_data[clinical_vars].fillna(merged_data[clinical_vars].mean())\ny = merged_data['Outcome']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nmodel = LogisticRegression()\nmodel.fit(X_scaled, y)\ny_pred = model.predict_proba(X_scaled)[:, 1]\nauc = roc_auc_score(y, y_pred)\nreport['auc_scores']['overall'] = auc\n\nfor var in clinical_vars:\n    X_var = X[[var]]\n    X_scaled_var = scaler.fit_transform(X_var)\n    model_var = LogisticRegression()\n    model_var.fit(X_scaled_var, y)\n    y_pred_var = model_var.predict_proba(X_scaled_var)[:, 1]\n    auc_var = roc_auc_score(y, y_pred_var)\n    report['auc_scores'][var] = auc_var\n\nreport_df = pd.DataFrame.from_dict(report, orient='index')\nreport_df.to_csv(os.path.join(output_dir, 'association_report.csv'))"
    },
    {
        "task": "Investigate the internal consistency of the Glascow Coma Scale (GCS) components and their individual association with patient outcome. The task requires analyzing the relationship between the recorded Glascow coma scale eye opening, motor response, and verbal response scores and the reported Glascow coma scale total. Specifically, assess how often the sum of the individual components matches the reported total GCS score and analyze if specific individual component scores or their distributions are significantly associated with patient Outcome (0 or 1), independently or in combination, perhaps revealing insights not captured by the total score alone. Generate a report summarizing the findings, including the rate of GCS component-total inconsistency and the distribution and association analysis of individual components with patient outcome.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 81,
        "code": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/110/\"\nos.makedirs(output_dir, exist_ok=True)\n\ngcs_components = ['Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale verbal response']\ndf['Calculated_GCS'] = df[gcs_components].sum(axis=1, skipna=False)\ninconsistent = df[df['Calculated_GCS'] != df['Glascow coma scale total']]\ninconsistency_rate = len(inconsistent) / len(df) * 100\n\nresults = {\n    \"Inconsistency Rate (%)\": inconsistency_rate,\n    \"Number of Inconsistent Records\": len(inconsistent),\n    \"Total Records\": len(df)\n}\n\nwith open(f\"{output_dir}gcs_inconsistency_report.txt\", \"w\") as f:\n    f.write(\"GCS Component-Total Inconsistency Report\\n\")\n    f.write(\"----------------------------------------\\n\")\n    for key, value in results.items():\n        f.write(f\"{key}: {value}\\n\")\n\noutcome_analysis = {}\nfor component in gcs_components:\n    contingency_table = pd.crosstab(df[component], df['Outcome'])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    outcome_analysis[component] = {\n        \"Chi-square Statistic\": chi2,\n        \"P-value\": p\n    }\n\nwith open(f\"{output_dir}gcs_outcome_association_report.txt\", \"w\") as f:\n    f.write(\"GCS Component Outcome Association Report\\n\")\n    f.write(\"----------------------------------------\\n\")\n    for component, stats in outcome_analysis.items():\n        f.write(f\"Component: {component}\\n\")\n        f.write(f\"Chi-square Statistic: {stats['Chi-square Statistic']}\\n\")\n        f.write(f\"P-value: {stats['P-value']}\\n\")\n        f.write(\"\\n\")\n\ndf['GCS_Components_Sum'] = df[gcs_components].sum(axis=1)\ncontingency_table_sum = pd.crosstab(df['GCS_Components_Sum'], df['Outcome'])\nchi2_sum, p_sum, dof_sum, expected_sum = chi2_contingency(contingency_table_sum)\n\nwith open(f\"{output_dir}gcs_sum_outcome_association_report.txt\", \"w\") as f:\n    f.write(\"GCS Sum Outcome Association Report\\n\")\n    f.write(\"----------------------------------\\n\")\n    f.write(f\"Chi-square Statistic: {chi2_sum}\\n\")\n    f.write(f\"P-value: {p_sum}\\n\")"
    },
    {
        "task": "Investigate the interplay between patient age and physiological status in predicting hospital mortality. The task is to analyze how the association between a key physiological indicator reflecting patient severity during hospitalization (e.g., the lowest recorded Oxygen saturation or lowest Glascow coma scale total for each admission) and the patient's outcome (`Outcome`, hospital mortality) is influenced by the patient's `Age`. Conduct an analysis to explore if this relationship varies across different age ranges or if there is a statistical interaction effect between age and the chosen physiological indicator on the likelihood of mortality. Generate a report summarizing the findings, including descriptive statistics of the chosen physiological indicator and outcome across different age groups, and quantifying the evidence for age-dependent prognostic value of the indicator.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 82,
        "code": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport os\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/111/\", exist_ok=True)\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndf['Age_group'] = pd.cut(df['Age'], bins=[0, 40, 60, 80, 120], labels=['0-40', '41-60', '61-80', '80+'])\n\nphysiological_indicator = 'Oxygen saturation'\ndf[physiological_indicator] = pd.to_numeric(df[physiological_indicator], errors='coerce')\nlowest_indicator = df.groupby('AdmissionID')[physiological_indicator].min().reset_index()\nlowest_indicator.columns = ['AdmissionID', 'Lowest_' + physiological_indicator]\ndf = df.merge(lowest_indicator, on='AdmissionID', how='left')\n\ndf_outcome = df.drop_duplicates('AdmissionID')[['AdmissionID', 'Outcome', 'Age', 'Age_group', 'Lowest_' + physiological_indicator]]\ndf_outcome['Outcome_binary'] = (df_outcome['Outcome'] == 'Death').astype(int)\n\ndesc_stats = df_outcome.groupby('Age_group').agg({\n    'Lowest_' + physiological_indicator: ['mean', 'median', 'std', 'min', 'max'],\n    'Outcome_binary': 'mean'\n}).reset_index()\ndesc_stats.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/111/descriptive_stats.csv\", index=False)\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Age_group', y='Lowest_' + physiological_indicator, data=df_outcome)\nplt.title('Distribution of Lowest Oxygen Saturation by Age Group')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/111/oxygen_saturation_by_age.png\")\nplt.close()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Age_group', y='Outcome_binary', data=df_outcome)\nplt.title('Mortality Rate by Age Group')\nplt.savefig(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/111/mortality_by_age.png\")\nplt.close()\n\ndf_outcome['Age_x_Indicator'] = df_outcome['Age'] * df_outcome['Lowest_' + physiological_indicator]\nX = df_outcome[['Age', 'Lowest_' + physiological_indicator, 'Age_x_Indicator']]\nX = sm.add_constant(X)\ny = df_outcome['Outcome_binary']\nmodel = sm.Logit(y, X).fit()\nwith open(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/111/logistic_regression_summary.txt\", \"w\") as f:\n    f.write(model.summary().as_text())\n\nage_ranges = [(0, 40), (41, 60), (61, 80), (81, 120)]\nresults = []\nfor low, high in age_ranges:\n    subset = df_outcome[(df_outcome['Age'] >= low) & (df_outcome['Age'] <= high)]\n    X_sub = subset[['Lowest_' + physiological_indicator]]\n    X_sub = sm.add_constant(X_sub)\n    y_sub = subset['Outcome_binary']\n    if len(subset) > 0:\n        model_sub = sm.Logit(y_sub, X_sub).fit(disp=0)\n        results.append({\n            'Age_range': f\"{low}-{high}\",\n            'Coefficient': model_sub.params[1],\n            'P_value': model_sub.pvalues[1],\n            'OR': np.exp(model_sub.params[1])\n        })\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/111/age_stratified_results.csv\", index=False)"
    },
    {
        "task": "Investigate the frequency and patterns of simultaneous occurrences of clinically significant abnormal vital sign values during hospital admissions. Define clinically relevant thresholds for critical ranges for key vital signs (e.g., Heart Rate, Mean blood pressure, Respiratory rate, Oxygen saturation, Temperature, Glucose, pH). For each record (`RecordID`), identify which of these vital signs fall outside their defined normal or critical ranges. Analyze if the number of simultaneously abnormal vital signs or specific combinations of abnormal vital signs recorded at any single time point within an admission are associated with patient outcomes (`Outcome` - hospital mortality, `LOS` - length of stay, `Readmission`). Generate a report summarizing the thresholds used, the prevalence of simultaneous vital sign abnormalities across the dataset, and the statistical association between these co-occurrence patterns (e.g., number of abnormal signs, specific combinations) and patient outcomes.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 83,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.stats import chi2_contingency\nimport statsmodels.api as sm\n\n# Define thresholds\nthresholds = {\n    'Heart Rate': {'low': 60, 'high': 100},\n    'Mean blood pressure': {'low': 70, 'high': 100},\n    'Respiratory rate': {'low': 12, 'high': 20},\n    'Oxygen saturation': {'low': 95, 'high': 100},\n    'Temperature': {'low': 36, 'high': 37.5},\n    'Glucose': {'low': 70, 'high': 140},\n    'pH': {'low': 7.35, 'high': 7.45}\n}\n\n# Load data\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Identify abnormal vital signs\nvital_signs = ['Heart Rate', 'Mean blood pressure', 'Respiratory rate', 'Oxygen saturation', 'Temperature', 'Glucose', 'pH']\nfor vs in vital_signs:\n    df[f'{vs}_abnormal'] = ((df[vs] < thresholds[vs]['low']) | (df[vs] > thresholds[vs]['high'])).astype(int)\n\n# Count number of abnormal vitals per record\ndf['abnormal_count'] = df[[f'{vs}_abnormal' for vs in vital_signs]].sum(axis=1)\n\n# Analyze associations with outcomes\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/112/\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Mortality association\nmortality_df = df.groupby('abnormal_count')['Outcome'].mean().reset_index()\nmortality_df.to_csv(os.path.join(output_dir, 'mortality_by_abnormal_count.csv'), index=False)\n\n# LOS association\nlos_df = df.groupby('abnormal_count')['LOS'].mean().reset_index()\nlos_df.to_csv(os.path.join(output_dir, 'los_by_abnormal_count.csv'), index=False)\n\n# Readmission association\nreadmission_df = df.groupby('abnormal_count')['Readmission'].mean().reset_index()\nreadmission_df.to_csv(os.path.join(output_dir, 'readmission_by_abnormal_count.csv'), index=False)\n\n# Chi-square test for mortality\ncontingency = pd.crosstab(df['abnormal_count'] > 0, df['Outcome'])\nchi2, p, _, _ = chi2_contingency(contingency)\nwith open(os.path.join(output_dir, 'chi2_mortality.txt'), 'w') as f:\n    f.write(f\"Chi2: {chi2}, p-value: {p}\")\n\n# Logistic regression for mortality\ndf['intercept'] = 1\nlogit_model = sm.Logit(df['Outcome'], df[['intercept', 'abnormal_count']])\nresult = logit_model.fit()\nwith open(os.path.join(output_dir, 'logistic_mortality.txt'), 'w') as f:\n    f.write(result.summary().as_text())\n\n# Save thresholds\nthresholds_df = pd.DataFrame(thresholds).T\nthresholds_df.to_csv(os.path.join(output_dir, 'thresholds.csv'))\n\n# Save prevalence of abnormal combinations\nabnormal_combinations = df[[f'{vs}_abnormal' for vs in vital_signs]].sum().to_frame('count')\nabnormal_combinations.to_csv(os.path.join(output_dir, 'abnormal_combinations.csv'))\n\n# Save summary statistics\nsummary_stats = df[['abnormal_count'] + [f'{vs}_abnormal' for vs in vital_signs]].describe()\nsummary_stats.to_csv(os.path.join(output_dir, 'summary_stats.csv'))"
    },
    {
        "task": "Analyze the provided patient admission data to predict the Length of Stay (LOS) for each hospital admission (`AdmissionID`). The task is to build a predictive model using patient demographic information (`Age`, `Sex`) and clinical measurements recorded within the first 24 hours of each admission. For each `AdmissionID`, process the records to extract relevant features from the initial phase of the hospitalization, such as the first recorded value or the mean value within the first day for applicable clinical measurements (e.g., `Heart Rate`, `Mean blood pressure`, `Temperature`, `Glucose`, `Glascow coma scale total`). Handle missing values appropriately. Train a regression model to predict `LOS` using these early-phase features. Investigate which features or combinations of features are the most significant predictors of `LOS`. Generate a report summarizing the predictive performance of the model, including evaluation metrics (e.g., R-squared, RMSE) and an analysis highlighting the importance of the identified predictor variables for `LOS`.",
        "task_type": "Reporting",
        "dataset": "MIMIC-IV",
        "metadata": "report",
        "ID": 84,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.impute import SimpleImputer\nimport os\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/113/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata = data.sort_values(['AdmissionID', 'RecordTime'])\n\ndef process_admission(group):\n    first_24h = group[group['RecordTime'] <= group['RecordTime'].min() + pd.Timedelta(hours=24)]\n    features = {\n        'Age': group['Age'].iloc[0],\n        'Sex': 1 if group['Sex'].iloc[0] == 'M' else 0,\n        'Heart Rate_mean': first_24h['Heart Rate'].mean(),\n        'Mean blood pressure_mean': first_24h['Mean blood pressure'].mean(),\n        'Temperature_mean': first_24h['Temperature'].mean(),\n        'Glucose_mean': first_24h['Glucose'].mean(),\n        'Glascow coma scale total_mean': first_24h['Glascow coma scale total'].mean(),\n        'LOS': group['LOS'].iloc[0]\n    }\n    return pd.Series(features)\n\nadmission_features = data.groupby('AdmissionID').apply(process_admission).reset_index()\n\nimputer = SimpleImputer(strategy='mean')\nfeatures_to_impute = ['Heart Rate_mean', 'Mean blood pressure_mean', 'Temperature_mean', 'Glucose_mean', 'Glascow coma scale total_mean']\nadmission_features[features_to_impute] = imputer.fit_transform(admission_features[features_to_impute])\n\nX = admission_features.drop(['AdmissionID', 'LOS'], axis=1)\ny = admission_features['LOS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nfeature_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': model.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nresults = {\n    'R2': r2,\n    'RMSE': rmse,\n    'Feature Importance': feature_importance.to_dict()\n}\n\nwith open(os.path.join(output_dir, 'results.txt'), 'w') as f:\n    f.write(f\"R-squared: {r2}\\n\")\n    f.write(f\"RMSE: {rmse}\\n\")\n    f.write(\"\\nFeature Importance:\\n\")\n    f.write(feature_importance.to_string())\n\nfeature_importance.to_csv(os.path.join(output_dir, 'feature_importance.csv'), index=False)"
    },
    {
        "task": "Develop a predictive model to classify the hospital outcome for patients using the provided physiological measurements, demographic information, and admission characteristics.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 85,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nfeatures = ['Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure', \n            'Fraction inspired oxygen', 'Glascow coma scale eye opening', \n            'Glascow coma scale motor response', 'Glascow coma scale total', \n            'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', \n            'Height', 'Mean blood pressure', 'Oxygen saturation', \n            'Respiratory rate', 'Systolic blood pressure', 'Temperature', \n            'Weight', 'pH']\ntarget = 'Outcome'\n\ndata = data.dropna(subset=features + [target])\nX = data[features]\ny = data[target]\n\nX = pd.get_dummies(X, columns=['Sex'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/85/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"results.txt\"), \"w\") as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n    f.write(\"Classification Report:\\n\")\n    f.write(report)"
    },
    {
        "task": "Develop a predictive model using the provided dataset to classify whether a patient will be readmitted to the hospital ('Readmission' column) based on their demographic information, physiological measurements, and admission characteristics.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 86,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nfeatures = ['Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure', \n            'Fraction inspired oxygen', 'Glascow coma scale eye opening', \n            'Glascow coma scale motor response', 'Glascow coma scale total', \n            'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', \n            'Height', 'Mean blood pressure', 'Oxygen saturation', \n            'Respiratory rate', 'Systolic blood pressure', 'Temperature', \n            'Weight', 'pH', 'LOS']\ndata = data.dropna(subset=features + ['Readmission'])\nX = data[features]\ny = data['Readmission'].astype(int)\n\nX = pd.get_dummies(X, columns=['Sex'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/86/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"accuracy.txt\"), \"w\") as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n\nwith open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n    f.write(report)"
    },
    {
        "task": "Develop a regression model to predict the Length of Stay (LOS) for each patient admission. The model should utilize the provided demographic information (Age, Sex), physiological measurements (Capillary refill rate, Diastolic blood pressure, Fraction inspired oxygen, Glascow coma scale scores, Glucose, Heart Rate, Height, Mean blood pressure, Oxygen saturation, Respiratory rate, Systolic blood pressure, Temperature, Weight, pH), and admission characteristics. Consider how to aggregate or utilize the multiple records available for a single admission to make the prediction for that admission.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 87,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport os\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\naggregations = {\n    'Age': 'first',\n    'Sex': 'first',\n    'Capillary refill rate': 'mean',\n    'Diastolic blood pressure': 'mean',\n    'Fraction inspired oxygen': 'mean',\n    'Glascow coma scale eye opening': 'mean',\n    'Glascow coma scale motor response': 'mean',\n    'Glascow coma scale total': 'mean',\n    'Glascow coma scale verbal response': 'mean',\n    'Glucose': 'mean',\n    'Heart Rate': 'mean',\n    'Height': 'first',\n    'Mean blood pressure': 'mean',\n    'Oxygen saturation': 'mean',\n    'Respiratory rate': 'mean',\n    'Systolic blood pressure': 'mean',\n    'Temperature': 'mean',\n    'Weight': 'first',\n    'pH': 'mean',\n    'LOS': 'first'\n}\n\ndata_agg = data.groupby('AdmissionID').agg(aggregations).reset_index()\n\ndata_agg['Sex'] = data_agg['Sex'].astype('category').cat.codes\n\nX = data_agg.drop(['AdmissionID', 'LOS'], axis=1)\ny = data_agg['LOS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/87/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"metrics.txt\"), \"w\") as f:\n    f.write(f\"MAE: {mae}\\n\")\n    f.write(f\"MSE: {mse}\\n\")\n    f.write(f\"R2 Score: {r2}\\n\")\n\nfeature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\nfeature_importances.to_csv(os.path.join(output_dir, \"feature_importances.csv\"), index=False)"
    },
    {
        "task": "Develop a time-series forecasting model to predict the value of a specific physiological measurement, for a patient at the next recorded time point within their hospital admission. The model should utilize the sequence of available physiological measurements and demographic information up to the current time point for that patient admission.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 88,
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/88/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ntarget_variable = 'Heart Rate'\nfeatures = ['Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n            'Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', \n            'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', \n            'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\n\ndata = data[['PatientID', 'AdmissionID', 'RecordTime'] + features].sort_values(['PatientID', 'AdmissionID', 'RecordTime'])\ndata = data.groupby(['PatientID', 'AdmissionID']).apply(lambda x: x.fillna(method='ffill').fillna(method='bfill')).reset_index(drop=True)\ndata = data.dropna(subset=[target_variable])\n\nscaler = StandardScaler()\ndata[features] = scaler.fit_transform(data[features])\n\ndef create_sequences(data, sequence_length=5):\n    sequences = []\n    targets = []\n    grouped = data.groupby(['PatientID', 'AdmissionID'])\n    for _, group in grouped:\n        group = group.sort_values('RecordTime')\n        values = group[features].values\n        for i in range(len(group) - sequence_length):\n            sequences.append(values[i:i+sequence_length])\n            targets.append(values[i+sequence_length, features.index(target_variable)])\n    return np.array(sequences), np.array(targets)\n\nX, y = create_sequences(data)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = Sequential([\n    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n    Dropout(0.2),\n    LSTM(32),\n    Dropout(0.2),\n    Dense(1)\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n\nloss = model.evaluate(X_test, y_test, verbose=0)\npredictions = model.predict(X_test)\n\nresults = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': predictions.flatten()\n})\nresults.to_csv(os.path.join(output_dir, 'predictions.csv'), index=False)\n\nwith open(os.path.join(output_dir, 'performance.txt'), 'w') as f:\n    f.write(f'Test Loss: {loss}\\n')"
    },
    {
        "task": "Develop a time-series classification model to predict the short-term risk of clinical deterioration for hospitalized patients. Specifically, train a model using the provided physiological measurements and patient demographics to predict, at any given recorded time point during a patient's admission, whether a significant clinical deterioration event will occur within the subsequent 24 hours. The definition of 'significant clinical deterioration' should be established based on critical changes or thresholds in the provided physiological parameters.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 89,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ndef load_data(filepath):\n    return pd.read_parquet(filepath)\n\ndef define_deterioration(df):\n    df['Deterioration'] = 0\n    df.loc[\n        (df['Systolic blood pressure'] < 90) |\n        (df['Diastolic blood pressure'] < 60) |\n        (df['Heart Rate'] > 120) |\n        (df['Respiratory rate'] > 24) |\n        (df['Oxygen saturation'] < 90) |\n        (df['Glascow coma scale total'] < 8),\n        'Deterioration'\n    ] = 1\n    return df\n\ndef preprocess_data(df):\n    df = df.drop(['RecordID', 'PatientID', 'RecordTime', 'AdmissionID', 'Outcome', 'Text'], axis=1)\n    df = pd.get_dummies(df, columns=['Sex'])\n    df = df.fillna(df.median())\n    return df\n\ndef create_labels(df):\n    df['Future_Deterioration'] = df.groupby('AdmissionID')['Deterioration'].shift(-1).fillna(0)\n    df['Future_Deterioration'] = df['Future_Deterioration'].astype(int)\n    return df\n\ndef main():\n    data_path = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\"\n    output_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/89/\"\n    \n    os.makedirs(output_dir, exist_ok=True)\n    \n    df = load_data(data_path)\n    df = define_deterioration(df)\n    df = create_labels(df)\n    df = preprocess_data(df)\n    \n    X = df.drop(['Future_Deterioration', 'Deterioration'], axis=1)\n    y = df['Future_Deterioration']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    report = classification_report(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    with open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n        f.write(report)\n    \n    results = {\n        'accuracy': accuracy,\n        'feature_importances': model.feature_importances_.tolist()\n    }\n    \n    pd.DataFrame(results).to_json(os.path.join(output_dir, 'results.json'))\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
        "task": "Develop a regression model to predict the overall physiological instability observed during a patient's hospital admission. Physiological instability can be quantified by calculating the average variability of key vital signs over the entire duration of the admission. The model should utilize the available physiological measurements and demographic information recorded within the first 48 hours of the admission to make this prediction.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 90,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\nvital_signs = ['Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n               'Glascow coma scale eye opening', 'Glascow coma scale motor response', \n               'Glascow coma scale total', 'Glascow coma scale verbal response', \n               'Glucose', 'Heart Rate', 'Mean blood pressure', 'Oxygen saturation', \n               'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'pH']\n\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata = data.sort_values(['AdmissionID', 'RecordTime'])\n\ndef calculate_instability(group):\n    variability = group[vital_signs].std().mean()\n    return variability\n\ninstability = data.groupby('AdmissionID').apply(calculate_instability).reset_index()\ninstability.columns = ['AdmissionID', 'PhysiologicalInstability']\n\nfirst_48h_data = data[data['RecordTime'] <= data.groupby('AdmissionID')['RecordTime'].transform('first') + pd.Timedelta(hours=48)]\nfirst_48h_agg = first_48h_data.groupby('AdmissionID').agg({\n    'Age': 'first',\n    'Sex': 'first',\n    'Capillary refill rate': ['mean', 'std'],\n    'Diastolic blood pressure': ['mean', 'std'],\n    'Fraction inspired oxygen': ['mean', 'std'],\n    'Glascow coma scale eye opening': ['mean', 'std'],\n    'Glascow coma scale motor response': ['mean', 'std'],\n    'Glascow coma scale total': ['mean', 'std'],\n    'Glascow coma scale verbal response': ['mean', 'std'],\n    'Glucose': ['mean', 'std'],\n    'Heart Rate': ['mean', 'std'],\n    'Mean blood pressure': ['mean', 'std'],\n    'Oxygen saturation': ['mean', 'std'],\n    'Respiratory rate': ['mean', 'std'],\n    'Systolic blood pressure': ['mean', 'std'],\n    'Temperature': ['mean', 'std'],\n    'pH': ['mean', 'std']\n})\n\nfirst_48h_agg.columns = ['_'.join(col).strip() for col in first_48h_agg.columns.values]\nfirst_48h_agg.reset_index(inplace=True)\n\nmerged_data = pd.merge(first_48h_agg, instability, on='AdmissionID')\nmerged_data = merged_data.dropna()\n\nX = merged_data.drop(['AdmissionID', 'PhysiologicalInstability'], axis=1)\nX = pd.get_dummies(X, columns=['Sex_first'])\ny = merged_data['PhysiologicalInstability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/90/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"results.txt\"), \"w\") as f:\n    f.write(f\"MSE: {mse}\\n\")\n    f.write(f\"R2 Score: {r2}\\n\")"
    },
    {
        "task": "Develop a predictive model to estimate the patient's Glasgow Coma Scale Total score approximately 24 hours after their hospital admission begins. The model should utilize patient demographic information (Age, Sex) and physiological measurements recorded within the first few hours of the admission. Consider appropriate data processing to handle multiple records per patient admission and define the target variable as the GCS Total score recorded closest to the 24-hour mark, or a suitable aggregate if multiple records exist around that time.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 91,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport os\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata = data.sort_values(['AdmissionID', 'RecordTime'])\n\ndef get_target_gcs(group):\n    group = group.copy()\n    group['TimeSinceAdmission'] = (group['RecordTime'] - group['RecordTime'].min()).dt.total_seconds() / 3600\n    mask = (group['TimeSinceAdmission'] >= 20) & (group['TimeSinceAdmission'] <= 28)\n    if mask.any():\n        closest_idx = (group.loc[mask, 'TimeSinceAdmission'] - 24).abs().idxmin()\n        return group.loc[closest_idx, 'Glascow coma scale total']\n    return np.nan\n\ntarget_gcs = data.groupby('AdmissionID').apply(get_target_gcs).reset_index()\ntarget_gcs.columns = ['AdmissionID', 'TargetGCS']\ndata = data.merge(target_gcs, on='AdmissionID', how='left')\n\nfeatures = ['Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen',\n            'Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale verbal response',\n            'Glucose', 'Heart Rate', 'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate',\n            'Systolic blood pressure', 'Temperature', 'pH']\n\ndata = data.dropna(subset=['TargetGCS'] + features)\ndata = data.groupby('AdmissionID').first().reset_index()\n\nX = data[features]\nX = pd.get_dummies(X, columns=['Sex'])\ny = data['TargetGCS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/91/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, 'metrics.txt'), 'w') as f:\n    f.write(f\"MAE: {mae}\\nMSE: {mse}\\n\")\n\nfeature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\nfeature_importance.to_csv(os.path.join(output_dir, 'feature_importance.csv'), index=False)"
    },
    {
        "task": "Develop a predictive model to classify whether a patient's Respiratory Rate ('Respiratory rate') will exceed a threshold of 30 at any point *after* the first 24 hours of their admission, using only data recorded within the first 24 hours of that admission. The model should process the time-series data available in the initial period and predict this future binary outcome.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 92,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ndata = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndata['RecordTime'] = pd.to_datetime(data['RecordTime'])\ndata['AdmissionTime'] = data.groupby('AdmissionID')['RecordTime'].transform('min')\ndata['TimeSinceAdmission'] = (data['RecordTime'] - data['AdmissionTime']).dt.total_seconds() / 3600\n\nfirst_24h = data[data['TimeSinceAdmission'] <= 24]\nafter_24h = data[data['TimeSinceAdmission'] > 24]\n\noutcome = after_24h.groupby('AdmissionID')['Respiratory rate'].max() > 30\noutcome = outcome.astype(int).reindex(first_24h['AdmissionID'].unique(), fill_value=0)\n\nfeatures = first_24h.groupby('AdmissionID').agg({\n    'Age': 'first',\n    'Sex': 'first',\n    'Capillary refill rate': 'mean',\n    'Diastolic blood pressure': 'mean',\n    'Fraction inspired oxygen': 'mean',\n    'Glascow coma scale eye opening': 'mean',\n    'Glascow coma scale motor response': 'mean',\n    'Glascow coma scale total': 'mean',\n    'Glascow coma scale verbal response': 'mean',\n    'Glucose': 'mean',\n    'Heart Rate': 'mean',\n    'Height': 'first',\n    'Mean blood pressure': 'mean',\n    'Oxygen saturation': 'mean',\n    'Respiratory rate': 'mean',\n    'Systolic blood pressure': 'mean',\n    'Temperature': 'mean',\n    'Weight': 'first',\n    'pH': 'mean'\n})\n\nX_train, X_test, y_train, y_test = train_test_split(features, outcome, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/92/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"accuracy.txt\"), \"w\") as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n\nwith open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n    f.write(report)"
    },
    {
        "task": "Develop a regression model to predict the average value of the 'Heart Rate' physiological measurement over the entire duration of a patient's hospital admission. The model should use patient demographic information (Age, Sex) and physiological measurements recorded *only within the first 12 hours* of that specific admission as input features. You will need to calculate the target variable (average Heart Rate) for each admission by taking the mean of all 'Heart Rate' records available for that AdmissionID, and extract/aggregate features from the records associated with that AdmissionID that fall within the initial 12-hour window.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 93,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\n\n# Load the dataset\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Convert RecordTime to datetime\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\n\n# Calculate the average Heart Rate per admission (target variable)\ntarget_df = df.groupby('AdmissionID')['Heart Rate'].mean().reset_index()\ntarget_df.columns = ['AdmissionID', 'AvgHeartRate']\n\n# Filter records within the first 12 hours of each admission\ndf['AdmissionStartTime'] = df.groupby('AdmissionID')['RecordTime'].transform('min')\ndf['HoursFromAdmission'] = (df['RecordTime'] - df['AdmissionStartTime']).dt.total_seconds() / 3600\nfirst_12_hours = df[df['HoursFromAdmission'] <= 12]\n\n# Aggregate features from the first 12 hours\nfeatures = ['Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure', \n            'Fraction inspired oxygen', 'Glascow coma scale eye opening', \n            'Glascow coma scale motor response', 'Glascow coma scale total', \n            'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', \n            'Height', 'Mean blood pressure', 'Oxygen saturation', \n            'Respiratory rate', 'Systolic blood pressure', 'Temperature', \n            'Weight', 'pH']\n\n# Group by AdmissionID and compute mean for each feature\nfeature_df = first_12_hours.groupby('AdmissionID')[features].mean().reset_index()\n\n# Merge target and features\nmerged_df = pd.merge(target_df, feature_df, on='AdmissionID', how='inner')\n\n# Handle missing values\nmerged_df.fillna(merged_df.mean(), inplace=True)\n\n# Convert Sex to numerical (assuming binary for simplicity)\nmerged_df['Sex'] = merged_df['Sex'].map({'M': 1, 'F': 0})\n\n# Prepare data for modeling\nX = merged_df.drop(['AdmissionID', 'AvgHeartRate'], axis=1)\ny = merged_df['AvgHeartRate']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Save results\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/93/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"results.txt\"), \"w\") as f:\n    f.write(f\"MSE: {mse}\\nR2: {r2}\\n\")\n\n# Save model\nimport joblib\njoblib.dump(model, os.path.join(output_dir, \"model.joblib\"))"
    },
    {
        "task": "Develop a binary classification model to predict, for a given record representing a time point within a patient's hospital admission, whether the 'pH' physiological measurement will be missing ('nan') in the *next* recorded time point for that same admission. The model should utilize the physiological measurements and demographic information available at the current time point as input features.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 94,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\n# Load the dataset\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\n# Sort by PatientID, AdmissionID, and RecordTime to ensure chronological order\ndf = df.sort_values(['PatientID', 'AdmissionID', 'RecordTime'])\n\n# Create the target variable: whether the next pH is NaN\ndf['next_pH_missing'] = df.groupby(['PatientID', 'AdmissionID'])['pH'].shift(-1).isna().astype(int)\n\n# Drop the last record of each admission since there is no 'next' record\ndf = df.groupby(['PatientID', 'AdmissionID']).apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n\n# Select features and target\nfeatures = ['Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n            'Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', \n            'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', \n            'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\ntarget = 'next_pH_missing'\n\n# Handle missing values in features (impute with median for numerical, mode for categorical)\nfor col in features:\n    if df[col].dtype == 'object':\n        df[col].fillna(df[col].mode()[0], inplace=True)\n    else:\n        df[col].fillna(df[col].median(), inplace=True)\n\n# Convert categorical variables to dummy variables\ndf = pd.get_dummies(df, columns=['Sex'])\n\n# Update features list after one-hot encoding\nfeatures = [col for col in df.columns if col in features or col.startswith('Sex_')]\nfeatures.remove('pH')  # Remove pH if it's not to be included\n\n# Split into train and test sets\nX = df[features]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\n# Save results\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/94/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, 'accuracy.txt'), 'w') as f:\n    f.write(f\"Accuracy: {accuracy}\\n\")\n\nwith open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n    f.write(report)"
    },
    {
        "task": "Develop a binary classification model to predict for each hospital admission (identified by `AdmissionID`), whether the physiological measurements recorded in the *final* record for that admission indicate a state of \"Respiratory Concern\". Define \"Respiratory Concern\" as meeting *at least one* of the following criteria in the *last* record for that admission: 1. Respiratory Rate > 25 breaths/min, 2. Oxygen saturation < 90%. The model should be trained using the demographic information (`Age`, `Sex`) and *all physiological measurements recorded up to the penultimate (second to last) record* for each admission.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 95,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf_sorted = df.sort_values(['AdmissionID', 'RecordTime'])\n\nlast_records = df_sorted.groupby('AdmissionID').last().reset_index()\nlast_records['Respiratory Concern'] = ((last_records['Respiratory rate'] > 25) | (last_records['Oxygen saturation'] < 90)).astype(int)\n\npenultimate_records = df_sorted.groupby('AdmissionID').apply(lambda x: x.iloc[-2] if len(x) > 1 else None).reset_index(drop=True)\npenultimate_records = penultimate_records.dropna(subset=['AdmissionID'])\n\nmerged_data = pd.merge(\n    penultimate_records,\n    last_records[['AdmissionID', 'Respiratory Concern']],\n    on='AdmissionID',\n    how='inner'\n)\n\nfeatures = [\n    'Age', 'Sex', 'Capillary refill rate', 'Diastolic blood pressure',\n    'Fraction inspired oxygen', 'Glascow coma scale eye opening',\n    'Glascow coma scale motor response', 'Glascow coma scale total',\n    'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', 'Height',\n    'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate',\n    'Systolic blood pressure', 'Temperature', 'Weight', 'pH'\n]\n\nX = merged_data[features]\nX['Sex'] = X['Sex'].map({'M': 1, 'F': 0})\nX = X.apply(pd.to_numeric, errors='coerce')\nX = X.fillna(X.mean())\n\ny = merged_data['Respiratory Concern']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/95/\"\nos.makedirs(output_dir, exist_ok=True)\n\nreport = classification_report(y_test, y_pred, output_dict=True)\naccuracy = accuracy_score(y_test, y_pred)\n\nreport_df = pd.DataFrame(report).transpose()\nreport_df.to_csv(os.path.join(output_dir, 'classification_report.csv'))\n\nwith open(os.path.join(output_dir, 'accuracy.txt'), 'w') as f:\n    f.write(str(accuracy))"
    },
    {
        "task": "Develop a binary classification model to predict, for each hospital admission (identified by `AdmissionID`), whether the patient's Systolic Blood Pressure ('Systolic blood pressure') will fall below 90 mmHg at *any point* during that entire admission. The model should utilize patient demographic information (`Age`, `Sex`) and physiological measurements recorded *only within the first 24 hours* of that specific admission as input features.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 96,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf = df.sort_values(['AdmissionID', 'RecordTime'])\n\nfirst_24h_data = df.groupby('AdmissionID').apply(lambda x: x[x['RecordTime'] <= x['RecordTime'].min() + pd.Timedelta(hours=24)]).reset_index(drop=True)\n\nhas_low_sbp = df.groupby('AdmissionID')['Systolic blood pressure'].apply(lambda x: (x < 90).any()).reset_index()\nhas_low_sbp.columns = ['AdmissionID', 'LowSBP']\n\nfeatures = first_24h_data.groupby('AdmissionID').agg({\n    'Age': 'first',\n    'Sex': 'first',\n    'Capillary refill rate': 'mean',\n    'Diastolic blood pressure': 'mean',\n    'Fraction inspired oxygen': 'mean',\n    'Glascow coma scale eye opening': 'mean',\n    'Glascow coma scale motor response': 'mean',\n    'Glascow coma scale total': 'mean',\n    'Glascow coma scale verbal response': 'mean',\n    'Glucose': 'mean',\n    'Heart Rate': 'mean',\n    'Height': 'mean',\n    'Mean blood pressure': 'mean',\n    'Oxygen saturation': 'mean',\n    'Respiratory rate': 'mean',\n    'Systolic blood pressure': 'mean',\n    'Temperature': 'mean',\n    'Weight': 'mean',\n    'pH': 'mean'\n}).reset_index()\n\nfeatures = features.merge(has_low_sbp, on='AdmissionID', how='left')\nfeatures['Sex'] = features['Sex'].map({'M': 1, 'F': 0})\n\nX = features.drop(['AdmissionID', 'LowSBP'], axis=1)\ny = features['LowSBP']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/96/\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith open(os.path.join(output_dir, \"accuracy.txt\"), \"w\") as f:\n    f.write(f\"Accuracy: {accuracy}\")\n\nwith open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n    f.write(report)"
    },
    {
        "task": "Develop a binary classification model to predict, for each hospital admission (identified by `AdmissionID`), whether the patient will experience at least one record exhibiting \"Severe Hypotension and Tachycardia\" at any point during their entire hospital stay. Define \"Severe Hypotension and Tachycardia\" as any record where the Mean Blood Pressure (`Mean blood pressure`) is less than 60 mmHg AND the Heart Rate (`Heart Rate`) is greater than 100 beats per minute. The model should be trained using patient demographic information (`Age`, `Sex`) and all physiological measurements recorded within the first 24 hours of that specific admission as input features.",
        "task_type": "Modeling",
        "dataset": "MIMIC-IV",
        "metadata": "modeling",
        "ID": 97,
        "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport os\n\ndf = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n\ndf['RecordTime'] = pd.to_datetime(df['RecordTime'])\ndf['Severe_Hypotension_Tachycardia'] = ((df['Mean blood pressure'] < 60) & (df['Heart Rate'] > 100)).astype(int)\n\nfirst_24h = df.groupby('AdmissionID').apply(lambda x: x[x['RecordTime'] <= x['RecordTime'].min() + pd.Timedelta(hours=24)]).reset_index(drop=True)\nagg_funcs = {\n    'Age': 'first',\n    'Sex': 'first',\n    'Capillary refill rate': ['mean', 'min', 'max'],\n    'Diastolic blood pressure': ['mean', 'min', 'max'],\n    'Fraction inspired oxygen': ['mean', 'min', 'max'],\n    'Glascow coma scale eye opening': ['mean', 'min', 'max'],\n    'Glascow coma scale motor response': ['mean', 'min', 'max'],\n    'Glascow coma scale total': ['mean', 'min', 'max'],\n    'Glascow coma scale verbal response': ['mean', 'min', 'max'],\n    'Glucose': ['mean', 'min', 'max'],\n    'Heart Rate': ['mean', 'min', 'max'],\n    'Height': ['mean', 'min', 'max'],\n    'Mean blood pressure': ['mean', 'min', 'max'],\n    'Oxygen saturation': ['mean', 'min', 'max'],\n    'Respiratory rate': ['mean', 'min', 'max'],\n    'Systolic blood pressure': ['mean', 'min', 'max'],\n    'Temperature': ['mean', 'min', 'max'],\n    'Weight': ['mean', 'min', 'max'],\n    'pH': ['mean', 'min', 'max']\n}\nfeatures = first_24h.groupby('AdmissionID').agg(agg_funcs)\nfeatures.columns = ['_'.join(col).strip() for col in features.columns.values]\n\nlabels = df.groupby('AdmissionID')['Severe_Hypotension_Tachycardia'].max()\n\nfeatures = features.loc[labels.index]\ndata = pd.merge(features, labels, left_index=True, right_index=True)\ndata['Sex'] = data['Sex_first'].map({'M': 1, 'F': 0})\ndata.drop(columns=['Sex_first'], inplace=True)\n\nX = data.drop(columns=['Severe_Hypotension_Tachycardia'])\ny = data['Severe_Hypotension_Tachycardia']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\noutput_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/97/\"\nos.makedirs(output_dir, exist_ok=True)\n\nreport = classification_report(y_test, y_pred)\nwith open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n    f.write(report)\n\naccuracy = accuracy_score(y_test, y_pred)\nwith open(os.path.join(output_dir, 'accuracy.txt'), 'w') as f:\n    f.write(str(accuracy))"
    },
    {
        "task": "Preprocess the dataset by first imputing missing values in numerical columns using the column mean across the entire dataset. Then, aggregate the records for each `AdmissionID` into a single row. For the imputed numerical columns, calculate the mean value across all records belonging to that `AdmissionID`. For columns that are constant within an `AdmissionID`, retain their value from any one record for that `AdmissionID`. The final output should have one row per unique `AdmissionID`.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_preprocessing",
        "ID": 98,
        "code": "import pandas as pd\nimport os\n\ndef preprocess_dataset():\n    df = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n    \n    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    for col in numerical_cols:\n        df[col] = df[col].fillna(df[col].mean())\n    \n    constant_cols = ['PatientID', 'Outcome', 'Age', 'Sex', 'Height', 'Weight']\n    aggregated_data = []\n    \n    for admission_id, group in df.groupby('AdmissionID'):\n        aggregated_row = {}\n        for col in numerical_cols:\n            if col not in constant_cols:\n                aggregated_row[col] = group[col].mean()\n        \n        for col in constant_cols:\n            if col in group.columns:\n                aggregated_row[col] = group[col].iloc[0]\n        \n        aggregated_row['AdmissionID'] = admission_id\n        aggregated_data.append(aggregated_row)\n    \n    result_df = pd.DataFrame(aggregated_data)\n    \n    output_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/98/\"\n    os.makedirs(output_dir, exist_ok=True)\n    result_df.to_parquet(os.path.join(output_dir, \"preprocessed_data.parquet\"))\n\npreprocess_dataset()"
    },
    {
        "task": "Preprocess the dataset by first sorting records by `AdmissionID` and `RecordTime`. Then, for each `AdmissionID`, impute missing values in numerical columns using forward fill, followed by backward fill for any remaining NaNs at the start of a sequence. If a column is entirely NaN for an `AdmissionID`, impute with 0. After imputation, aggregate the records for each `AdmissionID` into a single row. For the imputed numerical columns, retain the value from the last record within that `AdmissionID`. For columns that are constant within an `AdmissionID`, retain their value from any one record for that `AdmissionID`. The final output should have one row per unique `AdmissionID`.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_preprocessing",
        "ID": 99,
        "code": "import pandas as pd\nimport os\n\ndef preprocess_dataset():\n    df = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n    \n    df = df.sort_values(by=['AdmissionID', 'RecordTime'])\n    \n    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n    numerical_cols = [col for col in numerical_cols if col not in ['RecordID', 'PatientID', 'AdmissionID', 'Outcome', 'LOS', 'Readmission']]\n    \n    grouped = df.groupby('AdmissionID')\n    \n    imputed_dfs = []\n    for name, group in grouped:\n        group = group.copy()\n        for col in numerical_cols:\n            if group[col].isna().all():\n                group[col] = 0\n            else:\n                group[col] = group[col].ffill().bfill()\n        imputed_dfs.append(group)\n    \n    imputed_df = pd.concat(imputed_dfs)\n    \n    constant_cols = ['PatientID', 'Outcome', 'LOS', 'Readmission', 'Age', 'Sex', 'Height', 'Weight']\n    last_record_cols = [col for col in numerical_cols if col not in constant_cols]\n    \n    agg_dict = {col: 'last' for col in last_record_cols}\n    agg_dict.update({col: 'first' for col in constant_cols})\n    \n    result_df = imputed_df.groupby('AdmissionID').agg(agg_dict).reset_index()\n    \n    output_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/99/\"\n    os.makedirs(output_dir, exist_ok=True)\n    result_df.to_parquet(os.path.join(output_dir, \"preprocessed_data.parquet\"))\n\npreprocess_dataset()"
    },
    {
        "task": "Preprocess the dataset by first handling missing values in numerical columns within each `AdmissionID`. For each numerical column, impute missing values using the mean of the non-missing values within that specific `AdmissionID`. If a numerical column is entirely missing for a given `AdmissionID`, impute with 0. After imputation, aggregate the records for each `AdmissionID` into a single row. For each imputed numerical column, calculate the mean, standard deviation, minimum, and maximum values across all records belonging to that `AdmissionID`, creating new features. For columns that are constant within an `AdmissionID`, retain their value from any one record for that `AdmissionID`. The final output should have one row per unique `AdmissionID`.",
        "task_type": "Data",
        "dataset": "MIMIC-IV",
        "metadata": "data_preprocessing",
        "ID": 100,
        "code": "import pandas as pd\nimport numpy as np\nimport os\n\ndef preprocess_dataset():\n    df = pd.read_parquet(\"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/dataset/mimic-iv-timeseries-note.parquet\")\n    \n    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numerical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n    \n    grouped = df.groupby('AdmissionID')\n    \n    def impute_and_aggregate(group):\n        imputed_group = group.copy()\n        for col in numerical_cols:\n            if col in group.columns:\n                if group[col].isna().all():\n                    imputed_group[col] = 0\n                else:\n                    mean_val = group[col].mean()\n                    imputed_group[col] = group[col].fillna(mean_val)\n        \n        aggregated_data = {}\n        for col in numerical_cols:\n            if col in imputed_group.columns:\n                aggregated_data[f'{col}_mean'] = imputed_group[col].mean()\n                aggregated_data[f'{col}_std'] = imputed_group[col].std()\n                aggregated_data[f'{col}_min'] = imputed_group[col].min()\n                aggregated_data[f'{col}_max'] = imputed_group[col].max()\n        \n        for col in non_numerical_cols:\n            if col in imputed_group.columns:\n                unique_vals = imputed_group[col].unique()\n                if len(unique_vals) == 1:\n                    aggregated_data[col] = unique_vals[0]\n                else:\n                    aggregated_data[col] = imputed_group[col].iloc[0]\n        \n        return pd.Series(aggregated_data)\n    \n    result = grouped.apply(impute_and_aggregate).reset_index()\n    \n    output_dir = \"/home/kisara/RESEARCH/MedAgentBoard/MedAgentBoard/single_llm_2/output/100/\"\n    os.makedirs(output_dir, exist_ok=True)\n    result.to_parquet(os.path.join(output_dir, \"preprocessed_data.parquet\"))\n\npreprocess_dataset()"
    }
]